{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b504c1e4-a928-4d7d-92d5-f87ec7688469",
   "metadata": {},
   "source": [
    "### Week 13 Day 2: TensorFlow Outline\n",
    "\n",
    "### WEEK 14 day 1 part one : Business Case (9 Lessons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddebc42-2cfa-4caf-90ff-f05b3e4b05c6",
   "metadata": {},
   "source": [
    "#### Introduction to Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c48af-149d-4233-b20e-1a47668524fa",
   "metadata": {},
   "source": [
    "There are python libraries tailored for machine learning.\n",
    "\n",
    "Two are Tensorflow and sklearn\n",
    "\n",
    "We are discussing on Tensorflow\n",
    "\n",
    "The mathematical concept of a Tensor could be broadly explaine din this way.\n",
    "\n",
    "If a scalar have the lowest dimensionality and is followed by a vector and then by a matrix, a tensor will be the next object in line\n",
    "\n",
    "Scalars, Vectors and Matrices are all tensors of rank 0 , 1 and 2 respectively.\n",
    "\n",
    "Tensors are simply a generalization of the concpt we have seen so far.\n",
    "\n",
    "+ Why is tensorflow a good choice?\n",
    "\n",
    "We will be comparing Tensorflow to sklearn,as they are two of the most popular libraries and most of the  machine learning courses are based on sklearn.\n",
    "\n",
    "Google is a leader in machine learning , it is also and it is also one of the great innovators in the field because of the google rating, as machine learning was developing , google needed better programmong methods to suit their needs.\n",
    "\n",
    "That is why they developed the tensorflow package for internal use.\n",
    "\n",
    "At the end of 2015, google realed Tensorflow to the public.\n",
    "\n",
    "Currently , it is probably yhre leading library for neural networks including deep neural network, convolution neural networks and recurring neural network>\n",
    "\n",
    "One of the biggest advantage of Tensorflow is that it uses not only the CPU of the computer but also its GPU.\n",
    "\n",
    "+ This is crucial for the speed of the algorithm as in this way Tensorflow utilizes much more computing power.\n",
    "The bset part is that this is done automatically.\n",
    "\n",
    "Recently, furthere its strength by introducing TPU or Tensor Proceesing  unit which imprves performance even further.\n",
    "\n",
    "Tensorflow is one of the cutting edge technology available rigth now and it is likely here to stay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1836e8b4-d6d2-4913-b1fb-e4f8f06764e5",
   "metadata": {},
   "source": [
    "An alternative of tensorflow is sklearn,if you have done any machine learning and have not used Tensorflow , you are probably familiar with the scikit learn or sklearn library.It is very powerful and widely adopted .\n",
    "\n",
    "+ However, sklearn deos not offer theame functionality like Tensorflor regarding nueral networks.\n",
    "\n",
    "\n",
    "We cn now make the opposite pint for other fields of machine learning,\n",
    "\n",
    "In the presence of problems such as :\n",
    "\n",
    "+ K-Means\n",
    "\n",
    "+ Clutering \n",
    "+ Random forest \n",
    "\n",
    "Sklearn could be a better feet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44329f1-b2dd-4cdf-a1a6-f89aff6f5001",
   "metadata": {},
   "source": [
    "+ It is important to note that the theory is thesame for Tensor flow , sklearn or whatever package you are using .\n",
    "The only difference is the underlying code you write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c46e65-f206-4fcc-a611-fe4a8be0ee71",
   "metadata": {},
   "source": [
    "#### TensorFlow 2 Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60ec8c7-88d1-44b9-a6ca-8f7237743f9b",
   "metadata": {},
   "source": [
    "\n",
    "#### Tensorflow 1 Vs Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbaebd3-b24b-4363-848c-c0c5313bf047",
   "metadata": {},
   "source": [
    "###### History of Development(2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92828b31-683a-4f9b-b42c-6b3941760aee",
   "metadata": {},
   "source": [
    "Tensorflow 1 is one of the most widely used deep learning packages.it is built with many varsatility which makes it a great choice of many Practitioners . Unfortunately , it has one major drawback, it is very hard to learn and use.\n",
    "That's why many people are dsheartened tha after seeing few lines of tensorflow code .\n",
    "Not only is the matter strange but a whole logic of coding is unlike most library out there. \n",
    "\n",
    "This lead to the development and popularization of higher level packages such as pytouch and keras in 2017.\n",
    "\n",
    "Keras is particularly interesting. In 2017 , it was integrated in the core tensorflow. Both tensorflow and keras are open source, So ti sholdnt be suprizing that such things happen in the programming world.\n",
    "\n",
    "Francois Chollet:  claims that keras is \"an interface for tensorflow rather than a different library\" making this integration easier to digest and implement.\n",
    "\n",
    "Even though Keras is a part of keras , Tensorflow was still loosing popularity bwtween 2017 - 2018.\n",
    "\n",
    "Tensorflow 2.0 came on the horizon and this was tensorflow effort to catch up with the current demand for higher level programming.\n",
    "\n",
    "+ Instead of developing their high level syntax, the Tf developers choose to borrow that of keras.( Higher level than TF 1)\n",
    "\n",
    "+ Thia decision made sense as keras was already widely adopted and \n",
    "\n",
    "+ people generally love it.\n",
    "\n",
    "And you can hear people saying that Tensorflow is basically keras,\n",
    "\n",
    "Tf 2 has the best of both words. Most of the varsatilty of Tf - 1 and the high level simplicity of keras.\n",
    "\n",
    "Other advantages of TF 2 over TF 1 are as follows:\n",
    "\n",
    "+ Simplified API\n",
    "\n",
    "+ Removed duplicate or deprecated functions\n",
    "\n",
    "+ Added new features in the core Tensorflow\n",
    "\n",
    "> Most importantly, TensorFlow 2 boost eager execution. In other word s allowing standard  python rule the physics to apply to it.\n",
    "Rather than complex computational graph you dont only want to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c6956-ee71-4295-8127-e5eb0c364339",
   "metadata": {},
   "source": [
    "#### A note on coding in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876fe31d-bf60-4eac-aab7-04b924633fe6",
   "metadata": {},
   "source": [
    "##### Actual Tensorflow Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9a2e8-02cc-489b-99d1-9821c434d8e0",
   "metadata": {},
   "source": [
    "Tensorflow is a deep learning library developed by google. It allows us to construct fairly complicated model with little coding.\n",
    "\n",
    "To give you a perspective, our practical example requires 20 lines of code.\n",
    "With tensowflow it will still be 20 lines of code.\n",
    "No differece whatsoever.\n",
    "\n",
    "However the last exercise will require a few 100 lines of code using numpy.\n",
    "\n",
    "With Tensor flow still around 20.\n",
    "Moreover, most of those will almost be thesame in our minimal example.\n",
    "\n",
    "Tensorflow was an amazing frame work and you will be convinced by that at the end of the course. The only issue is that it is yet another libary to learn.\n",
    "\n",
    "Once you start working with it ,it is going to be super easy but you must make an extra effort to understand it properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0191b9-15b3-4409-a8f6-e76ed9967d85",
   "metadata": {},
   "source": [
    "#### Types of file formats in Tensorflow and data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c7462-eaea-44bd-9df8-dda8f897f29a",
   "metadata": {},
   "source": [
    "#### Minimal example with TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a449728-5950-47fb-a6a4-684ea7d180db",
   "metadata": {},
   "source": [
    "##### import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9780d34c-a498-401c-b63a-e91a913cdc7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "814fb7be-e7b6-446b-a9e8-dd1150d3722c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__init' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43m__init\u001b[49m\u001b[38;5;241m.\u001b[39mpy__\n",
      "\u001b[1;31mNameError\u001b[0m: name '__init' is not defined"
     ]
    }
   ],
   "source": [
    "__init.py__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b764be-27a7-4889-95eb-e56f7c970bd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3-TF2.0\\Lib\\site-packages\\opt_einsum\\backends\\tensorflow.py:7\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdd17da-2e35-4dda-a238-d58f0b43ab91",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc9ef14-0bb6-482d-9ed0-9517e7943717",
   "metadata": {},
   "source": [
    "For each project you work on you have a dataset or a csv file, however Tensor flow does not work with all of them, it is tensor based so it likes tensors so we want  a format that can store the information in tensors \n",
    "\n",
    "One solution to this problem is .npz files, that is basically Numpy's file type, it allows you to save ND-Arrays or N-Dimensional arrays.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b07cc12-756b-464e-b16f-c60d606ed687",
   "metadata": {},
   "source": [
    "Thinking like computer scientist, we can say that tensors can be represented as multi-dimentional arrays.\n",
    "\n",
    "When we read an NPZ file , the data is already organized in the desired way.\n",
    "\n",
    "Often, this is an important part of deep learning pre-procesing.\n",
    "\n",
    "You are given data in a specific file format, then you open it, pre-process it and finally save it in an npz.\n",
    "\n",
    "Later you build your algorithm using the npz instead of the original file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c8e82e-d9e9-4c1e-853c-b9259ae745bc",
   "metadata": {},
   "source": [
    "Looking at the code, we have named the inputs and targets we generated( generated_inputs, targets) \n",
    "Next, we can easily save them into a  Tensor friendly file.\n",
    "\n",
    "The proper way to do that is to use the np.save method.\n",
    "\n",
    "It involves several arguement:\n",
    "\n",
    "+ The first one is the file name\n",
    "It is written in quatation marks, lets call it 'tf_intro'\n",
    "\n",
    "+ Then ,we must indicate the object we want to save into the file.\n",
    "The syntax is as follows\n",
    "The label we want to assign the nd-array = to the array we want to save under that label \n",
    "The label is input and it is equal to the generated input array\n",
    "\n",
    "Similarly, the targets are equal to the generated targets.\n",
    "\n",
    "Note,it is not required to call them input and targets.\n",
    "\n",
    "If we would like to we could call them with arbitrary names such as rad 1 and rad 2\n",
    "\n",
    "Executing the code will save the Tf-intro file in thesame directory as the jupiter notebook we are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74ce7971-afc6-4eab-a79a-c705fb4ba469",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = 1000\n",
    "\n",
    "xs = np.random.uniform(low=-10, high=10, size=(observations,1))\n",
    "zs = np.random.uniform(-10, 10, (observations,1))\n",
    "\n",
    "generated_inputs = np.column_stack((xs,zs))\n",
    "\n",
    "noise = np.random.uniform(-1, 1, (observations,1))\n",
    "\n",
    "generated_targets = 2*xs - 3*zs + 5 + noise \n",
    "\n",
    "#np.savez('TF_intro', Rad1=generated_inputs, Rad2= generated_targets)\n",
    "np.savez('TF_intro', inputs=generated_inputs, targets= generated_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6288a287-9544-4698-b210-27414673e693",
   "metadata": {},
   "source": [
    "#### Model layout â€“ inputs, outputs, target, weights, bias, optimizer, and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b9f8e8-f75f-4717-94bf-bdf977eeaedc",
   "metadata": {},
   "source": [
    "We will start by load the data from the .npz file Is good to get use to loading your data from npz as that is how you usually will be provided with it.\n",
    "\n",
    "We will create two variables that will measure the size of our input and output.The input size is two as there are two input variables. the xs and zs we saw earlier and the output size is 1 as there is only one output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6367ff-844d-4daa-a0e6-32ef73f89730",
   "metadata": {},
   "source": [
    "Unlike other packages ew have built in models , when we are employing tensorflow,we must actually build the model.\n",
    "\n",
    "S let's build the model,  i will store it in a variable called model.\n",
    "\n",
    "Model = tf.keras.sequential\n",
    "tf stands for tensorflow\n",
    "keras as we have already discussed tf2 is based on keras, so that is the model needed.\n",
    "\n",
    "Sequential is the function that indicates that we are laying down the model.it takes as an arguement the different layers we will like to include in  our algorithm\n",
    "\n",
    "tf.keras.sequential() function that specifies how the model will be laid down('Stacks layers')\n",
    "\n",
    "Why we have not spoken about layers just yet is because the algorithm we are building have a simple stucture.It takes inputs , applies a single linear transformation and provides output.\n",
    "\n",
    "This linear combination together with the output constitutes the so called output layer.\n",
    "\n",
    "With the minimal example with the numpy ,the outputs are equal to the dot product to be input and weight plus the bias.\n",
    "\n",
    "There is another useful method called Dense from tf.keras.layers\n",
    "\n",
    "tf.keras.layers.Dense(output size) takes the inputs provided to the model and calculates the dot product of the inputs and the weights and add the bias * also applies activation function(optional)\n",
    "\n",
    "\n",
    "The Dens method takes a provided input and calculates the dot product to the inputs and weights and adds the bias.\n",
    "It is precisely what we wanted to achieve, therefore in bracket we must simply specify the output size.\n",
    "\n",
    "We have already stored it in the variable, so we can primetarize our code by placing that variable as an arguement.\n",
    "\n",
    "This alone is completely enough for our model specification.\n",
    "\n",
    "Now according to our theological frame work,we need data, a model , an objective function and an optimization algorithm.\n",
    "\n",
    "We have taken care of the dat and the model, we are left witht the later two.\n",
    "\n",
    "The mothod which allows us to specify them is called compile.\n",
    "\n",
    "model.compile(optimizer, loss) cofigures the model for training.\n",
    "\n",
    "model.compile() we include several different arguement in the bracket, the optimizer or optimization algorithm.\n",
    "What we will use is abbreviated a sgd which stands for sochastic gradient descent . It is a generalization of the gradient descent concept we have already learnt.  we will learn the differences later.\n",
    "\n",
    "To ad as an arguement we write, optimizer equals and the string name of the optimizer we awnt to use.\n",
    "\n",
    "When using high level packages that require a string , if you want to check what you can actually include as a string. we go online and check (tf.keras.optimizer) we see a list of names of different optimizers .\n",
    "\n",
    "Under classes ; you can check the exact name of the optimizer e want to use.\n",
    "For this example , that will be sgd. we will explore other optimizers later in the course.\n",
    "\n",
    "The second arguement we will include is the loss function. We wantt o make this example as close as possible to the  numpy minimal example, so we have to us the L2-norm loss scaled by the number of observations.\n",
    "\n",
    "In such cases , Good theorogical prepration comes in handy.\n",
    "\n",
    "+ The L2-norm loss is also known as the least sum of squares.\n",
    "\n",
    "+ Moreover scaling by the number of observations  = average (mean)\n",
    "\n",
    "Looking to the posible lossess from the theoritical , we discovered mean squared error.\n",
    "\n",
    "Mean squared error is pricesily the L2-norm loss scaled by the the total number of observations.\n",
    "\n",
    "Let's include the arguement  having that in mind. los = mean_squared_error.\n",
    "\n",
    "we have lodwd the data , outlined the model amd configured the learning process by selecting an objective function and an optimization algorithm.\n",
    "\n",
    "What we have got left is o indicate to the model which data to fit.\n",
    "\n",
    "Similar to many other library, tensorflow 2.0 employs a fit method with two mandatory arguement: the inputs and the targets\n",
    "\n",
    "model.fit(inputs, targets)fits (trains) the model\n",
    "\n",
    "In the bracket we must specify the input variable containd in the training data and the target which are contained in the target tensor from training data.\n",
    "\n",
    "This same method is also the same place where we set the number of iteration.\n",
    "\n",
    "Each iteration over the full dataset in machine learning is called an epochs \n",
    "From now on we use this term to describe iterations and number of iterations .\n",
    "\n",
    "Now let's set the number of epochs to 100\n",
    "Finally, we set verbose=0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ac637-b2d1-49e1-a4ba-fbadd8bdbea1",
   "metadata": {},
   "source": [
    "We have got all the code needed to train our first algorithm with Tensorflow 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1818af3c-97ca-4319-a969-0d70c161f2a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Solving with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b434e7-d859-480b-aece-9dd77990713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load('TF_intro.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a80ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load('TF_intro.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ca46a49-e8ef-4e7c-b660-c0a4344baa0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m output_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# when we introduced kernel and bias iniializers the output is still same\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      5\u001b[0m                            tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(output_size,\n\u001b[0;32m      6\u001b[0m                                                  kernel_initializer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mrandom_uniform_initializer(minval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m, maxval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m),\n\u001b[0;32m      7\u001b[0m                                                  bias_initializer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mrandom_uniform_initializer(minval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m, maxval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m      8\u001b[0m                                                 )\n\u001b[0;32m      9\u001b[0m                            ])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# lets create a custom optimizer and set learning rate to 0.02 as we did in numpy e.g\u001b[39;00m\n\u001b[0;32m     12\u001b[0m custom_optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "inputs_size = 2\n",
    "output_size = 1\n",
    "# when we introduced kernel and bias iniializers the output is still same\n",
    "model = tf.keras.Sequential([\n",
    "                           tf.keras.layers.Dense(output_size,\n",
    "                                                 kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n",
    "                                                 bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "                                                )\n",
    "                           ])\n",
    "\n",
    "# lets create a custom optimizer and set learning rate to 0.02 as we did in numpy e.g\n",
    "custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)\n",
    "#model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "# Replace sgd with new custom optimizer\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error')\n",
    "# We will use built-in losses without customizing them.\n",
    "# in future you may get hooked without customizing them\n",
    "# The new result will still be the same , with the diff. being we need to set learning rate ourself\n",
    "# Verbose = 0 stands for silence or no output available when the training is displayed\n",
    "# If we set verbose to 1 ,we will get a progress bar\n",
    "# verbose = 2 stands for 1 line per epoch\n",
    "#model.fit(training_data['inputs'], training_data['targets'], epochs=100, verbose=0)\n",
    "model.fit(training_data['inputs'], training_data['targets'], epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cbb315-214c-4516-a015-f82ef4b4557e",
   "metadata": {},
   "source": [
    "#### Interpreting the result and extracting the weights and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd873bb-fc00-4273-94d3-d8ebe2caaae7",
   "metadata": {},
   "source": [
    "We have got all the code needed to train our first algorithm with Tensorflow 2.\n",
    "\n",
    "So, it is time to run it.\n",
    "The result is a bit underwelming\n",
    "We get nothing more than an output signifying that the model have been trained in object with no information about the training with no information about the training,the reason we said verbose is 0, which stands for silence or no output about the training is displayed, if we set verbose to one , we should get a progress bar.  \n",
    "\n",
    "Unfortunately, for the current version on tensorflow and its integreation in jupiter we get the whole output in text form.\n",
    "We can still clearly see all the information needed. For other coding in other environment , it may be a more difficult experience . Because of this experience,this cleaner form of information can be found when verbose is two. This indicates we will get one line per epoch which will allow us to follow the development of the loss function over the training.\n",
    "\n",
    "The first piece is a timer tracking the time it took in seconds to complete each epoch.\n",
    "For all epoch in each example, it took 0 seconds per epoch.\n",
    "The second output is the current value of the loss function.\n",
    "\n",
    "As we scroll down. we comfirm that the loss is decreasing so our algorithm have worked as intended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773327aa-ec70-4755-815a-f27472889850",
   "metadata": {
    "tags": []
   },
   "source": [
    "`inputs_size = 2\n",
    "output_size = 1\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(output_size)\n",
    "                            ])\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "model.fit(training_data['inputs'], training_data['targets'], epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f499b3-3dd7-48ec-a569-e23f02a49ccc",
   "metadata": {},
   "source": [
    "As we discussed already , we generated the function 2X - 3z + 5 + noise in other to be able t access how our model did. You should be aware that in a real life situation, you never know the exact relationship. So it wouldn't be possible to comfirm how well your model how faired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c8f37c-d7c5-4979-9049-a1408f8f2a68",
   "metadata": {},
   "source": [
    "##### Extract the weights and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01765ec-cae4-4ffb-bad8-47fbce639308",
   "metadata": {},
   "source": [
    "If we check the weight and biases . they shoild be 2 and -3.\n",
    "There is a convenient built-in method called get weight that could be applied  to each layer for this purpose.\n",
    "\n",
    "Model is the model that we created.we have to specify the layer we are interested in.\n",
    "\n",
    "In that case the only layer ie position o, finally ,we have to apply the method get weight, the output is a tensor with 2 rays. one for the weight and 1 for the biases.In this case ,bias.\n",
    "As anticipated , the weight are approximately 2 and -3 while the bias is 5. This is pricisely the information which comfirms that our algorithm has indeed learnt the underlying relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8b6616-ad18-4684-91d6-3762e28abfd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 2.0957038],\n",
       "        [-3.02566  ]], dtype=float32),\n",
       " array([4.9747496], dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9a75353-b4d7-487e-9f2c-f2fb4ad6aa07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0957038],\n",
       "       [-3.02566  ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.layers[0].get_weights()[0]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dacab660-355a-4200-9290-a5784a5ff9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.9747496], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias = model.layers[0].get_weights()[1]\n",
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c8bc5d-1759-4e9d-9a84-750e4a962b91",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Extract the outputs (make predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7fbe9a-dc28-4255-9028-6743d15b7ece",
   "metadata": {},
   "source": [
    "+ what if we wanted to predict values using our model?\n",
    "\n",
    "To predict values with our model, we use the method, predict on batch. The batcg here is the data we are provided with\n",
    "\n",
    "model.predict_on_batch(data) calculates the outputs given inputs\n",
    "\n",
    "so the code is model.predict _no batch then you feed it with the training inputs.\n",
    "\n",
    "The results consists of an array with corresponding outputs for each of the inputs.\n",
    "\n",
    "Infact, these are the values that were compared to the target to evaluate the loss function.\n",
    "To be precise , these are the outputs based on the train model or in our case the outputs after 100 epochs of training.\n",
    "\n",
    "Since tthe outputs are compared to the targets at each epoch, it may be interesting to compare them manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5b26e95-c34e-42a9-bba7-38312163ff53",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-32. ],\n",
       "       [-17.5],\n",
       "       [ 35.3],\n",
       "       [ 10.7],\n",
       "       [-28.5],\n",
       "       [ 16.4],\n",
       "       [ 47.8],\n",
       "       [ 25.2],\n",
       "       [ 28.5],\n",
       "       [ 15.3],\n",
       "       [  9.2],\n",
       "       [-24.3],\n",
       "       [-12.5],\n",
       "       [ -0.5],\n",
       "       [ 12.9],\n",
       "       [ 22.8],\n",
       "       [ -9.1],\n",
       "       [ -0.5],\n",
       "       [ -0.3],\n",
       "       [ 11.3],\n",
       "       [-14.5],\n",
       "       [ -3.1],\n",
       "       [ -8.8],\n",
       "       [  2.5],\n",
       "       [-17.8],\n",
       "       [  8.7],\n",
       "       [ 16.2],\n",
       "       [ -8.6],\n",
       "       [ 32.2],\n",
       "       [  8.1],\n",
       "       [ -7.8],\n",
       "       [ -5.6],\n",
       "       [-28.5],\n",
       "       [ 30.1],\n",
       "       [ 41.6],\n",
       "       [ -3.4],\n",
       "       [-27.7],\n",
       "       [ 33.8],\n",
       "       [  1.3],\n",
       "       [ 28.2],\n",
       "       [ 30.9],\n",
       "       [ 27. ],\n",
       "       [ 22.9],\n",
       "       [-28.2],\n",
       "       [ 46.8],\n",
       "       [-13.6],\n",
       "       [ 44.3],\n",
       "       [ -9.3],\n",
       "       [ 48.3],\n",
       "       [-15.7],\n",
       "       [ 24.4],\n",
       "       [ 19.5],\n",
       "       [-32.6],\n",
       "       [  6.2],\n",
       "       [ 52. ],\n",
       "       [-33.9],\n",
       "       [-15.2],\n",
       "       [ 21.4],\n",
       "       [ 28.7],\n",
       "       [ 11.2],\n",
       "       [ -8.9],\n",
       "       [ 36.8],\n",
       "       [ -6.7],\n",
       "       [  5.7],\n",
       "       [-12.3],\n",
       "       [ 21.7],\n",
       "       [ 37.3],\n",
       "       [ 25.8],\n",
       "       [-40.4],\n",
       "       [ 13.3],\n",
       "       [  1.1],\n",
       "       [ 23.9],\n",
       "       [ 14. ],\n",
       "       [-21.8],\n",
       "       [-14.2],\n",
       "       [-25. ],\n",
       "       [ -1.3],\n",
       "       [ 12.4],\n",
       "       [-19.4],\n",
       "       [-29.2],\n",
       "       [ 17.7],\n",
       "       [  2.4],\n",
       "       [  9.2],\n",
       "       [ -1.1],\n",
       "       [ 34.2],\n",
       "       [-11.7],\n",
       "       [ 34.6],\n",
       "       [ -4.4],\n",
       "       [-21.1],\n",
       "       [ 22.9],\n",
       "       [ 17.9],\n",
       "       [ 10.6],\n",
       "       [ -3.8],\n",
       "       [ 15. ],\n",
       "       [ 19.6],\n",
       "       [ 11.8],\n",
       "       [-17.4],\n",
       "       [ 26. ],\n",
       "       [ 23.5],\n",
       "       [-11.9],\n",
       "       [ -5.7],\n",
       "       [ -7.6],\n",
       "       [ 31.2],\n",
       "       [ 27.7],\n",
       "       [ 17.6],\n",
       "       [ 23.5],\n",
       "       [ 52.3],\n",
       "       [  4.5],\n",
       "       [ 18.7],\n",
       "       [-15.9],\n",
       "       [ -3.4],\n",
       "       [  1.4],\n",
       "       [-18.8],\n",
       "       [ 10.4],\n",
       "       [-31.7],\n",
       "       [-29.5],\n",
       "       [-11.7],\n",
       "       [ -4.9],\n",
       "       [-37.1],\n",
       "       [-24.5],\n",
       "       [ 13. ],\n",
       "       [-33.7],\n",
       "       [-29.9],\n",
       "       [-14.8],\n",
       "       [ 36.8],\n",
       "       [  6.2],\n",
       "       [ 27.9],\n",
       "       [ -6.3],\n",
       "       [ 11.4],\n",
       "       [ -0.9],\n",
       "       [ 16.3],\n",
       "       [ 22. ],\n",
       "       [-34.3],\n",
       "       [  2.5],\n",
       "       [-14.8],\n",
       "       [ 20.6],\n",
       "       [-17.4],\n",
       "       [  4.5],\n",
       "       [ -2. ],\n",
       "       [  2.8],\n",
       "       [ 29.3],\n",
       "       [ -1.3],\n",
       "       [ 29.9],\n",
       "       [  0.4],\n",
       "       [  4.2],\n",
       "       [ -2.9],\n",
       "       [  7.4],\n",
       "       [ 21.9],\n",
       "       [ 13.7],\n",
       "       [ 33.7],\n",
       "       [ -1.4],\n",
       "       [-22.5],\n",
       "       [ -5.9],\n",
       "       [ -2. ],\n",
       "       [-13. ],\n",
       "       [ 29.5],\n",
       "       [ 12.1],\n",
       "       [ 23.4],\n",
       "       [ 25.3],\n",
       "       [ 23.1],\n",
       "       [ 13.1],\n",
       "       [-27.8],\n",
       "       [-24.2],\n",
       "       [  4.7],\n",
       "       [-10.9],\n",
       "       [-14.2],\n",
       "       [-36.6],\n",
       "       [  6.6],\n",
       "       [ 48.4],\n",
       "       [ 11.4],\n",
       "       [-23.2],\n",
       "       [ 24.7],\n",
       "       [ 20.5],\n",
       "       [ -8.1],\n",
       "       [ 25.8],\n",
       "       [ 10.1],\n",
       "       [ 34.7],\n",
       "       [ -3.1],\n",
       "       [-32.6],\n",
       "       [ -5.8],\n",
       "       [ -8.3],\n",
       "       [  3.6],\n",
       "       [-12.9],\n",
       "       [ 31.5],\n",
       "       [  2.7],\n",
       "       [ -3.4],\n",
       "       [  6.5],\n",
       "       [ 16.6],\n",
       "       [ 19.1],\n",
       "       [ -4.8],\n",
       "       [ 37.4],\n",
       "       [ 26.1],\n",
       "       [ -3.2],\n",
       "       [ 32.4],\n",
       "       [ -2.2],\n",
       "       [ -2.2],\n",
       "       [ -5.3],\n",
       "       [-25.4],\n",
       "       [ 41. ],\n",
       "       [ 21. ],\n",
       "       [  7.1],\n",
       "       [ 47.6],\n",
       "       [-39. ],\n",
       "       [-15.7],\n",
       "       [ 27.4],\n",
       "       [ 11.5],\n",
       "       [  2.3],\n",
       "       [ 21.8],\n",
       "       [ 34. ],\n",
       "       [ 26.3],\n",
       "       [ -5.7],\n",
       "       [ 21.4],\n",
       "       [ -5.4],\n",
       "       [ 14.7],\n",
       "       [ 21.8],\n",
       "       [  1.8],\n",
       "       [ 20.5],\n",
       "       [  2.7],\n",
       "       [ -7. ],\n",
       "       [ 29.6],\n",
       "       [  9.7],\n",
       "       [ -5.8],\n",
       "       [ 23. ],\n",
       "       [ 13.8],\n",
       "       [ 46.1],\n",
       "       [ 33.8],\n",
       "       [ 15.3],\n",
       "       [  9. ],\n",
       "       [-20.4],\n",
       "       [-13. ],\n",
       "       [ 30.3],\n",
       "       [ 14.9],\n",
       "       [-19.7],\n",
       "       [-11.5],\n",
       "       [ 29.8],\n",
       "       [-20. ],\n",
       "       [ 12.1],\n",
       "       [ 14.3],\n",
       "       [ 29.3],\n",
       "       [  5.9],\n",
       "       [ 22.2],\n",
       "       [-27.7],\n",
       "       [  5.3],\n",
       "       [-39.6],\n",
       "       [ 18. ],\n",
       "       [ 17.8],\n",
       "       [-13.5],\n",
       "       [ 12.8],\n",
       "       [  5.3],\n",
       "       [-10.5],\n",
       "       [ -9.7],\n",
       "       [ 16.7],\n",
       "       [-13.7],\n",
       "       [ -0.5],\n",
       "       [ 37.3],\n",
       "       [ -1.2],\n",
       "       [ 52.9],\n",
       "       [ 25.5],\n",
       "       [ 22.9],\n",
       "       [ 33.3],\n",
       "       [ 50.6],\n",
       "       [-21.9],\n",
       "       [ -1.9],\n",
       "       [  2.5],\n",
       "       [ 23.2],\n",
       "       [-19.6],\n",
       "       [ 26.8],\n",
       "       [  9.9],\n",
       "       [ 20.2],\n",
       "       [ 21.5],\n",
       "       [-21.7],\n",
       "       [ 33.8],\n",
       "       [ 21.9],\n",
       "       [  8.7],\n",
       "       [-10.3],\n",
       "       [ 18.1],\n",
       "       [ 44.2],\n",
       "       [ 12.2],\n",
       "       [ 21.6],\n",
       "       [  3.1],\n",
       "       [-20.4],\n",
       "       [ 26.3],\n",
       "       [-18.7],\n",
       "       [ 34.3],\n",
       "       [ 35.9],\n",
       "       [ -5. ],\n",
       "       [ 32.7],\n",
       "       [ -0.2],\n",
       "       [-14.2],\n",
       "       [-23.3],\n",
       "       [ -0.6],\n",
       "       [ 23.7],\n",
       "       [ -7.8],\n",
       "       [ 19.3],\n",
       "       [-19.4],\n",
       "       [-30.8],\n",
       "       [ 20.9],\n",
       "       [ 25.9],\n",
       "       [-14.2],\n",
       "       [ 17.2],\n",
       "       [ -6.3],\n",
       "       [ 17. ],\n",
       "       [ 21.6],\n",
       "       [ -4.9],\n",
       "       [  4.1],\n",
       "       [ 38.9],\n",
       "       [-26.3],\n",
       "       [ 10.4],\n",
       "       [-40.8],\n",
       "       [  6.9],\n",
       "       [ 33.4],\n",
       "       [-27.6],\n",
       "       [ 10.6],\n",
       "       [ 20.6],\n",
       "       [ 21.7],\n",
       "       [  2.7],\n",
       "       [ 29.3],\n",
       "       [ 18.3],\n",
       "       [ 11.1],\n",
       "       [  9.1],\n",
       "       [ -7.6],\n",
       "       [  8.5],\n",
       "       [-24.9],\n",
       "       [-17.1],\n",
       "       [ 32. ],\n",
       "       [ -3.4],\n",
       "       [ 21.1],\n",
       "       [ 25.4],\n",
       "       [ -9. ],\n",
       "       [  4.5],\n",
       "       [ 38.7],\n",
       "       [ -3.9],\n",
       "       [ -2.2],\n",
       "       [ 16. ],\n",
       "       [-18.1],\n",
       "       [ 28.3],\n",
       "       [ 12.3],\n",
       "       [  0.8],\n",
       "       [-12.9],\n",
       "       [-22.6],\n",
       "       [  8. ],\n",
       "       [ 49.5],\n",
       "       [ -0.9],\n",
       "       [-12.9],\n",
       "       [ 23.2],\n",
       "       [ 12.7],\n",
       "       [-34.7],\n",
       "       [ 42.9],\n",
       "       [ 27.6],\n",
       "       [-34.5],\n",
       "       [-25.1],\n",
       "       [ 31.3],\n",
       "       [ 35.3],\n",
       "       [ 33.8],\n",
       "       [ 12.3],\n",
       "       [ 15.9],\n",
       "       [-13.4],\n",
       "       [ 24.6],\n",
       "       [ 11.1],\n",
       "       [-22.7],\n",
       "       [ 18.3],\n",
       "       [ 45.7],\n",
       "       [ -5.7],\n",
       "       [-21.9],\n",
       "       [ 38.6],\n",
       "       [-26.9],\n",
       "       [ 51.9],\n",
       "       [ 15.9],\n",
       "       [ 44.6],\n",
       "       [-10.1],\n",
       "       [-17.4],\n",
       "       [-22.6],\n",
       "       [-16.7],\n",
       "       [  1.4],\n",
       "       [ -1. ],\n",
       "       [  9.5],\n",
       "       [ 12.2],\n",
       "       [  1.5],\n",
       "       [-26.8],\n",
       "       [ 31.2],\n",
       "       [  2.8],\n",
       "       [-19.7],\n",
       "       [ 14.4],\n",
       "       [ 27.8],\n",
       "       [ -3.1],\n",
       "       [ 13.8],\n",
       "       [ -4.1],\n",
       "       [  4.9],\n",
       "       [ 10.1],\n",
       "       [-20.4],\n",
       "       [-23.7],\n",
       "       [ 21.4],\n",
       "       [-10.6],\n",
       "       [-29.4],\n",
       "       [-26.9],\n",
       "       [  3.1],\n",
       "       [ 24.3],\n",
       "       [ 17.8],\n",
       "       [ 18.8],\n",
       "       [ -5. ],\n",
       "       [  1.6],\n",
       "       [-33.6],\n",
       "       [ 13.7],\n",
       "       [  7.8],\n",
       "       [ -6.1],\n",
       "       [ 39.7],\n",
       "       [  8.1],\n",
       "       [ 11. ],\n",
       "       [-28.1],\n",
       "       [-18.7],\n",
       "       [ 28.9],\n",
       "       [ 31.5],\n",
       "       [  5.4],\n",
       "       [ 16.7],\n",
       "       [-28.7],\n",
       "       [ -7.8],\n",
       "       [-10.4],\n",
       "       [ 24.1],\n",
       "       [ 50.1],\n",
       "       [ 44.3],\n",
       "       [-10.9],\n",
       "       [ 43. ],\n",
       "       [-12.5],\n",
       "       [  2.3],\n",
       "       [ 32.5],\n",
       "       [ 23.2],\n",
       "       [-29.8],\n",
       "       [  7.5],\n",
       "       [-19.9],\n",
       "       [  6.8],\n",
       "       [ -5.9],\n",
       "       [-18.5],\n",
       "       [ 20.4],\n",
       "       [ 11.8],\n",
       "       [ 23.2],\n",
       "       [ -2. ],\n",
       "       [  5. ],\n",
       "       [ 36.6],\n",
       "       [-21.8],\n",
       "       [ 16.5],\n",
       "       [ 27.3],\n",
       "       [ -8.8],\n",
       "       [  4.2],\n",
       "       [ 21.4],\n",
       "       [ -2.6],\n",
       "       [-11.8],\n",
       "       [ 31.3],\n",
       "       [  4.2],\n",
       "       [ 24. ],\n",
       "       [ 23.6],\n",
       "       [ 33. ],\n",
       "       [ 22.6],\n",
       "       [ 22.8],\n",
       "       [ -8.7],\n",
       "       [ 19. ],\n",
       "       [ 24.7],\n",
       "       [-19.7],\n",
       "       [-17.5],\n",
       "       [ 20.4],\n",
       "       [ 28.8],\n",
       "       [ 20.7],\n",
       "       [-34.3],\n",
       "       [ -1.7],\n",
       "       [ -3.5],\n",
       "       [  6.1],\n",
       "       [ 13.7],\n",
       "       [ 27.3],\n",
       "       [ -9. ],\n",
       "       [ -1.6],\n",
       "       [ 28.2],\n",
       "       [-44.1],\n",
       "       [ 23.6],\n",
       "       [-36.8],\n",
       "       [ 13. ],\n",
       "       [ -7.6],\n",
       "       [-25. ],\n",
       "       [-34.2],\n",
       "       [ 11.8],\n",
       "       [-12.4],\n",
       "       [ 31. ],\n",
       "       [ 16.9],\n",
       "       [  6.4],\n",
       "       [-21.4],\n",
       "       [ 31.1],\n",
       "       [ 23.4],\n",
       "       [ 36. ],\n",
       "       [-13.1],\n",
       "       [ -2.8],\n",
       "       [ -7.4],\n",
       "       [ 16.5],\n",
       "       [ 12.8],\n",
       "       [ 27.1],\n",
       "       [-13.5],\n",
       "       [  3.6],\n",
       "       [  7.2],\n",
       "       [-23.5],\n",
       "       [ 17.1],\n",
       "       [-10.1],\n",
       "       [-14.9],\n",
       "       [ 26.8],\n",
       "       [-19.3],\n",
       "       [-43.8],\n",
       "       [  1.3],\n",
       "       [-23. ],\n",
       "       [-29.3],\n",
       "       [ -8. ],\n",
       "       [ 19.4],\n",
       "       [ 35.2],\n",
       "       [-29.3],\n",
       "       [  0.6],\n",
       "       [ 41.1],\n",
       "       [ 25.9],\n",
       "       [ 22.6],\n",
       "       [ 18.9],\n",
       "       [-13.5],\n",
       "       [ -7.5],\n",
       "       [  1.2],\n",
       "       [ 17.9],\n",
       "       [ 15.9],\n",
       "       [-39. ],\n",
       "       [-16.7],\n",
       "       [ 13.7],\n",
       "       [ -6.8],\n",
       "       [ 31.7],\n",
       "       [-25.7],\n",
       "       [ -5.8],\n",
       "       [  1. ],\n",
       "       [ 41.6],\n",
       "       [ 30.2],\n",
       "       [-17.9],\n",
       "       [ -6.7],\n",
       "       [ -2.7],\n",
       "       [  0.2],\n",
       "       [-25.8],\n",
       "       [ 16.8],\n",
       "       [-32.3],\n",
       "       [ 16. ],\n",
       "       [ 28.9],\n",
       "       [  7.7],\n",
       "       [ 53.4],\n",
       "       [ -0. ],\n",
       "       [ 15.4],\n",
       "       [ -2.9],\n",
       "       [-18.8],\n",
       "       [ 13.3],\n",
       "       [  6.2],\n",
       "       [-21.5],\n",
       "       [-12.2],\n",
       "       [-18.6],\n",
       "       [-17.1],\n",
       "       [ -5.3],\n",
       "       [-18.4],\n",
       "       [ 37.7],\n",
       "       [ -7.4],\n",
       "       [ -1.4],\n",
       "       [ 50.1],\n",
       "       [ 40.7],\n",
       "       [ 30.7],\n",
       "       [-20.4],\n",
       "       [ 21.4],\n",
       "       [ -0.3],\n",
       "       [-21.8],\n",
       "       [ 14. ],\n",
       "       [  4.8],\n",
       "       [ -5.4],\n",
       "       [ 41.5],\n",
       "       [  9.2],\n",
       "       [  4.9],\n",
       "       [ -6.4],\n",
       "       [-33.4],\n",
       "       [-34.3],\n",
       "       [  7.1],\n",
       "       [  3.8],\n",
       "       [ 42.9],\n",
       "       [  1.3],\n",
       "       [-22.2],\n",
       "       [ 16.6],\n",
       "       [  1.5],\n",
       "       [ -4.9],\n",
       "       [ 31.8],\n",
       "       [-25. ],\n",
       "       [ -0.1],\n",
       "       [-17.2],\n",
       "       [ 29.7],\n",
       "       [  4.1],\n",
       "       [ 15.3],\n",
       "       [-24.4],\n",
       "       [  2.8],\n",
       "       [-22.3],\n",
       "       [ 18. ],\n",
       "       [-31.8],\n",
       "       [ 12.8],\n",
       "       [ 25.7],\n",
       "       [ -3.6],\n",
       "       [-13.3],\n",
       "       [ 11.6],\n",
       "       [ 22.5],\n",
       "       [ 17.6],\n",
       "       [ 17.1],\n",
       "       [ -8.6],\n",
       "       [ 21.8],\n",
       "       [  2.6],\n",
       "       [  2.2],\n",
       "       [-33.3],\n",
       "       [-18.1],\n",
       "       [ 23.9],\n",
       "       [-17.4],\n",
       "       [ 22.1],\n",
       "       [ 36.3],\n",
       "       [ 10.6],\n",
       "       [ -9.7],\n",
       "       [-12.1],\n",
       "       [ -6.7],\n",
       "       [-21.2],\n",
       "       [ -0.1],\n",
       "       [  7.4],\n",
       "       [ 19.9],\n",
       "       [ 22.3],\n",
       "       [  4.8],\n",
       "       [-27.9],\n",
       "       [-13.3],\n",
       "       [ 26.1],\n",
       "       [-13.8],\n",
       "       [ 38.6],\n",
       "       [ 16.8],\n",
       "       [  2.3],\n",
       "       [ 38. ],\n",
       "       [-27.6],\n",
       "       [ 20.7],\n",
       "       [ 32.2],\n",
       "       [ 11.1],\n",
       "       [-16.8],\n",
       "       [-20.2],\n",
       "       [ 22.3],\n",
       "       [  9.4],\n",
       "       [ 39.7],\n",
       "       [ -5. ],\n",
       "       [ 14.8],\n",
       "       [ 40. ],\n",
       "       [ 17.5],\n",
       "       [ 17.8],\n",
       "       [ -5.4],\n",
       "       [-21.9],\n",
       "       [ 35.6],\n",
       "       [ 36.8],\n",
       "       [ 46.6],\n",
       "       [ 17.2],\n",
       "       [  6.9],\n",
       "       [ -0.1],\n",
       "       [-12.5],\n",
       "       [ -5.5],\n",
       "       [ 15. ],\n",
       "       [ 14.6],\n",
       "       [-28.7],\n",
       "       [  0.3],\n",
       "       [-37.7],\n",
       "       [  8.8],\n",
       "       [ 10.6],\n",
       "       [-23.9],\n",
       "       [  4.1],\n",
       "       [ -5.7],\n",
       "       [ 33.2],\n",
       "       [ 23.9],\n",
       "       [ -1.3],\n",
       "       [ 23.2],\n",
       "       [ -4.5],\n",
       "       [-18.7],\n",
       "       [-18.3],\n",
       "       [ 30. ],\n",
       "       [-10.9],\n",
       "       [ 23.4],\n",
       "       [-10.8],\n",
       "       [ -1.9],\n",
       "       [-13.5],\n",
       "       [  8.6],\n",
       "       [-21.9],\n",
       "       [ 23. ],\n",
       "       [-26.7],\n",
       "       [-30. ],\n",
       "       [ -7.4],\n",
       "       [ -0.8],\n",
       "       [-16.5],\n",
       "       [ 16.5],\n",
       "       [-33.5],\n",
       "       [-15.9],\n",
       "       [ 14.1],\n",
       "       [ 11. ],\n",
       "       [-27.6],\n",
       "       [ 22.4],\n",
       "       [ 31.5],\n",
       "       [ 15.3],\n",
       "       [-23.7],\n",
       "       [-12. ],\n",
       "       [  2.4],\n",
       "       [ 21.6],\n",
       "       [ 49.3],\n",
       "       [  9.8],\n",
       "       [ 44.3],\n",
       "       [ 47. ],\n",
       "       [ -0.1],\n",
       "       [-11.5],\n",
       "       [ 13. ],\n",
       "       [ -1. ],\n",
       "       [  3.9],\n",
       "       [-13.5],\n",
       "       [-44.9],\n",
       "       [ 11.9],\n",
       "       [ 28.9],\n",
       "       [ 42.6],\n",
       "       [ 29.6],\n",
       "       [-14.4],\n",
       "       [ 15. ],\n",
       "       [ 42.4],\n",
       "       [-14.1],\n",
       "       [-12.2],\n",
       "       [ 16.9],\n",
       "       [ 10. ],\n",
       "       [ 18.9],\n",
       "       [ -7.7],\n",
       "       [  9.3],\n",
       "       [  2.6],\n",
       "       [ 16.5],\n",
       "       [ 26.8],\n",
       "       [-32.1],\n",
       "       [-13.2],\n",
       "       [-16. ],\n",
       "       [ -8. ],\n",
       "       [-21.5],\n",
       "       [ 17.5],\n",
       "       [-38.4],\n",
       "       [ 31.4],\n",
       "       [ -4.2],\n",
       "       [  9.9],\n",
       "       [ 12.9],\n",
       "       [ -4.9],\n",
       "       [  0.6],\n",
       "       [-15.7],\n",
       "       [ 11.9],\n",
       "       [-36.2],\n",
       "       [ 18.3],\n",
       "       [  0.4],\n",
       "       [-36.7],\n",
       "       [ 22.9],\n",
       "       [-21.9],\n",
       "       [  1.5],\n",
       "       [ -1.9],\n",
       "       [-19.6],\n",
       "       [ 18.9],\n",
       "       [ 34. ],\n",
       "       [ 10.5],\n",
       "       [ 47.1],\n",
       "       [ 19.1],\n",
       "       [ 28. ],\n",
       "       [  5. ],\n",
       "       [ -1. ],\n",
       "       [  2.1],\n",
       "       [ -2.5],\n",
       "       [-14.3],\n",
       "       [ 21.8],\n",
       "       [ 29.8],\n",
       "       [ 29.6],\n",
       "       [  6.4],\n",
       "       [-15. ],\n",
       "       [ 16. ],\n",
       "       [ 43.6],\n",
       "       [-18.3],\n",
       "       [ -3.4],\n",
       "       [ -1.9],\n",
       "       [ 12.1],\n",
       "       [-16.8],\n",
       "       [ 16.1],\n",
       "       [ 23.5],\n",
       "       [ -6.1],\n",
       "       [-17.7],\n",
       "       [ 12.2],\n",
       "       [ 24.7],\n",
       "       [ 38.8],\n",
       "       [  4.4],\n",
       "       [-33.8],\n",
       "       [ -4.6],\n",
       "       [ 37. ],\n",
       "       [ -8.7],\n",
       "       [ -7.1],\n",
       "       [ 13. ],\n",
       "       [  3.7],\n",
       "       [-13.8],\n",
       "       [ 11.5],\n",
       "       [ 10.6],\n",
       "       [ 32.3],\n",
       "       [  8.8],\n",
       "       [-42.4],\n",
       "       [ 12.5],\n",
       "       [ -4.3],\n",
       "       [ 29.2],\n",
       "       [ 27.3],\n",
       "       [-20.6],\n",
       "       [-40.3],\n",
       "       [ 25.9],\n",
       "       [ 40.6],\n",
       "       [ 35.6],\n",
       "       [-33.3],\n",
       "       [-27.7],\n",
       "       [  6. ],\n",
       "       [ -3.6],\n",
       "       [ 20.4],\n",
       "       [ 22.3],\n",
       "       [ 43.5],\n",
       "       [ 31. ],\n",
       "       [  7.8],\n",
       "       [ 15.2],\n",
       "       [ 38.2],\n",
       "       [  7.5],\n",
       "       [ 33.1],\n",
       "       [ 20.9],\n",
       "       [-20.7],\n",
       "       [ 25.1],\n",
       "       [ -6.5],\n",
       "       [ -9.7],\n",
       "       [ 34.9],\n",
       "       [ 33.2],\n",
       "       [ 21.4],\n",
       "       [ -7. ],\n",
       "       [ 35.6],\n",
       "       [  6. ],\n",
       "       [ -4.2],\n",
       "       [  1. ],\n",
       "       [ 23.5],\n",
       "       [ 20.2],\n",
       "       [ 37.3],\n",
       "       [ -0. ],\n",
       "       [ 54. ],\n",
       "       [ -1.6],\n",
       "       [ 27.1],\n",
       "       [ 16.1],\n",
       "       [-21.7],\n",
       "       [ 23.1],\n",
       "       [  1. ],\n",
       "       [ 31.5],\n",
       "       [-42.9],\n",
       "       [-12.6],\n",
       "       [  5.7],\n",
       "       [-42.6],\n",
       "       [ -4.9],\n",
       "       [-16.9],\n",
       "       [-21.9],\n",
       "       [  7.7],\n",
       "       [ 19.1],\n",
       "       [ 27.1],\n",
       "       [-38. ],\n",
       "       [ 12.8],\n",
       "       [-24. ],\n",
       "       [-29.3],\n",
       "       [-10.8],\n",
       "       [ 22.3],\n",
       "       [ 51.5],\n",
       "       [ 22.8],\n",
       "       [-21.8],\n",
       "       [ 18.1],\n",
       "       [-15.7],\n",
       "       [  3.2],\n",
       "       [ -9.4],\n",
       "       [-20.4],\n",
       "       [ 16.4],\n",
       "       [  8.9],\n",
       "       [  6.1],\n",
       "       [ 28.7],\n",
       "       [  1. ],\n",
       "       [-35.6],\n",
       "       [ 33.9],\n",
       "       [  1.9],\n",
       "       [  2.2],\n",
       "       [  4.4],\n",
       "       [  4.1],\n",
       "       [ -4.4],\n",
       "       [ 26.9],\n",
       "       [  7.2],\n",
       "       [ 18.6],\n",
       "       [ 25.5],\n",
       "       [ 33. ],\n",
       "       [ -0.5],\n",
       "       [-17.9],\n",
       "       [ -6.1],\n",
       "       [ 40.8],\n",
       "       [ 13.6],\n",
       "       [ -8.4],\n",
       "       [-31.1],\n",
       "       [-27.6],\n",
       "       [ 21.3],\n",
       "       [-32.6],\n",
       "       [  3. ],\n",
       "       [-25. ],\n",
       "       [ 43. ],\n",
       "       [-17.8],\n",
       "       [ -1.8],\n",
       "       [ 44.6],\n",
       "       [ 15.2],\n",
       "       [ 25.3],\n",
       "       [-11.4],\n",
       "       [-39.2],\n",
       "       [ 24.9],\n",
       "       [ 48.2],\n",
       "       [ 20.8],\n",
       "       [  8.8],\n",
       "       [ 35.6],\n",
       "       [ 37. ],\n",
       "       [ -4.3],\n",
       "       [ -1.8],\n",
       "       [ -3.3],\n",
       "       [ 40.9],\n",
       "       [ -8.7],\n",
       "       [ -8.6],\n",
       "       [ 10.8],\n",
       "       [ 36.3],\n",
       "       [ 11.7],\n",
       "       [-29. ],\n",
       "       [ 33.1],\n",
       "       [ 41.2],\n",
       "       [ 24.9],\n",
       "       [ -5.7],\n",
       "       [ 13.2],\n",
       "       [  3.2],\n",
       "       [ 14.2],\n",
       "       [ 12.3],\n",
       "       [ 32.5],\n",
       "       [ 22.2],\n",
       "       [ 34.6],\n",
       "       [ 10.7],\n",
       "       [ -8.2],\n",
       "       [ 31.2],\n",
       "       [-14.7],\n",
       "       [ 18.2],\n",
       "       [ 11.6],\n",
       "       [ 11.8],\n",
       "       [ 30.8],\n",
       "       [ 13.7],\n",
       "       [ 14.1],\n",
       "       [  9.5],\n",
       "       [ 44.1],\n",
       "       [ -3.1],\n",
       "       [-31.6],\n",
       "       [ 15.4],\n",
       "       [ 36.5],\n",
       "       [ 21.4],\n",
       "       [-15.7],\n",
       "       [ 44.3],\n",
       "       [ 11.8],\n",
       "       [  9.9],\n",
       "       [ 25.5],\n",
       "       [  0.8],\n",
       "       [-12.5],\n",
       "       [ 37.2],\n",
       "       [ 25.1],\n",
       "       [-14.2],\n",
       "       [-35.7],\n",
       "       [  3.6],\n",
       "       [ -4. ],\n",
       "       [ 22.6],\n",
       "       [  1.9],\n",
       "       [-39.4],\n",
       "       [ 41.9],\n",
       "       [ -3.6],\n",
       "       [ 24.3],\n",
       "       [-37.6],\n",
       "       [  0.9],\n",
       "       [ -2.7],\n",
       "       [-14.6],\n",
       "       [ -1.8],\n",
       "       [ 17.9],\n",
       "       [ -1.8],\n",
       "       [-39.6],\n",
       "       [-14. ],\n",
       "       [ 15.3],\n",
       "       [ 15.3],\n",
       "       [ -5.8],\n",
       "       [  1.8],\n",
       "       [ 47. ],\n",
       "       [ -8.7],\n",
       "       [ 14.2],\n",
       "       [  5.7],\n",
       "       [ 15.9],\n",
       "       [ -9.2],\n",
       "       [  3.8],\n",
       "       [  0.9],\n",
       "       [ -7.6],\n",
       "       [ 31.6],\n",
       "       [ -9.6],\n",
       "       [-18.2],\n",
       "       [ 10. ],\n",
       "       [ -5.4],\n",
       "       [ 14. ],\n",
       "       [ 25.9],\n",
       "       [  5.6],\n",
       "       [ 24.7],\n",
       "       [ 18.3],\n",
       "       [ 29.6],\n",
       "       [  4.9],\n",
       "       [ 35.9],\n",
       "       [ 19.7],\n",
       "       [-22.4],\n",
       "       [-14.1]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_on_batch(training_data['inputs']).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56896e-1763-45c2-b39c-62bea750cca6",
   "metadata": {},
   "source": [
    "Since tthe outputs are compared to the targets at each epoch, it may be interesting to compare them manually. \n",
    "To achieve that explain the training target and round all digits to  one digit after the dot,so that they are easily readeable. What we see is that the output and the target are very close to each other but notexactily thesame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bdec364-73df-4639-8927-090bda8dbef9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-31.4],\n",
       "       [-17. ],\n",
       "       [ 34.9],\n",
       "       [  9.3],\n",
       "       [-27.4],\n",
       "       [ 15.5],\n",
       "       [ 47.3],\n",
       "       [ 25.5],\n",
       "       [ 26.9],\n",
       "       [ 14.3],\n",
       "       [  9.7],\n",
       "       [-23.6],\n",
       "       [-12.3],\n",
       "       [ -1.1],\n",
       "       [ 11.5],\n",
       "       [ 23.4],\n",
       "       [ -9.6],\n",
       "       [ -0.2],\n",
       "       [ -0.8],\n",
       "       [ 12.2],\n",
       "       [-13.5],\n",
       "       [ -2.5],\n",
       "       [ -9.9],\n",
       "       [  1.5],\n",
       "       [-17.7],\n",
       "       [  8.1],\n",
       "       [ 15.5],\n",
       "       [ -7.9],\n",
       "       [ 32.6],\n",
       "       [  9.4],\n",
       "       [ -7.2],\n",
       "       [ -6.2],\n",
       "       [-27.4],\n",
       "       [ 29.6],\n",
       "       [ 40.7],\n",
       "       [ -4.6],\n",
       "       [-27. ],\n",
       "       [ 33.2],\n",
       "       [  0.1],\n",
       "       [ 27. ],\n",
       "       [ 29.5],\n",
       "       [ 26.3],\n",
       "       [ 23.4],\n",
       "       [-26.6],\n",
       "       [ 45.1],\n",
       "       [-13.5],\n",
       "       [ 44.1],\n",
       "       [ -9.7],\n",
       "       [ 46.9],\n",
       "       [-16.1],\n",
       "       [ 22.7],\n",
       "       [ 19.2],\n",
       "       [-31. ],\n",
       "       [  4.8],\n",
       "       [ 50.9],\n",
       "       [-33.9],\n",
       "       [-14.5],\n",
       "       [ 20.7],\n",
       "       [ 29. ],\n",
       "       [ 12.3],\n",
       "       [ -8.5],\n",
       "       [ 36. ],\n",
       "       [ -7.3],\n",
       "       [  5.8],\n",
       "       [-12.3],\n",
       "       [ 20.8],\n",
       "       [ 36.7],\n",
       "       [ 26.6],\n",
       "       [-39.2],\n",
       "       [ 12.8],\n",
       "       [  2.1],\n",
       "       [ 23.3],\n",
       "       [ 14.5],\n",
       "       [-20.4],\n",
       "       [-14.8],\n",
       "       [-25.1],\n",
       "       [ -1.6],\n",
       "       [ 12.3],\n",
       "       [-18.1],\n",
       "       [-28.5],\n",
       "       [ 18.3],\n",
       "       [  1.6],\n",
       "       [  9.4],\n",
       "       [ -0.3],\n",
       "       [ 34.2],\n",
       "       [-10.9],\n",
       "       [ 34.1],\n",
       "       [ -4. ],\n",
       "       [-20.2],\n",
       "       [ 22.6],\n",
       "       [ 16.7],\n",
       "       [ 11.5],\n",
       "       [ -4.5],\n",
       "       [ 14.1],\n",
       "       [ 18.6],\n",
       "       [ 12. ],\n",
       "       [-18.2],\n",
       "       [ 24.3],\n",
       "       [ 23.2],\n",
       "       [-10.9],\n",
       "       [ -6.1],\n",
       "       [ -8.2],\n",
       "       [ 30.6],\n",
       "       [ 28.2],\n",
       "       [ 16.9],\n",
       "       [ 22.3],\n",
       "       [ 51.5],\n",
       "       [  4.8],\n",
       "       [ 18.8],\n",
       "       [-15.9],\n",
       "       [ -3.7],\n",
       "       [  2.1],\n",
       "       [-17.6],\n",
       "       [  9.7],\n",
       "       [-30.1],\n",
       "       [-28.9],\n",
       "       [-11.3],\n",
       "       [ -5.6],\n",
       "       [-36.7],\n",
       "       [-24.2],\n",
       "       [ 12.4],\n",
       "       [-32.7],\n",
       "       [-29.1],\n",
       "       [-14.2],\n",
       "       [ 35.9],\n",
       "       [  5.7],\n",
       "       [ 27.6],\n",
       "       [ -6.4],\n",
       "       [ 10.9],\n",
       "       [ -0.7],\n",
       "       [ 16.8],\n",
       "       [ 22.3],\n",
       "       [-32.9],\n",
       "       [  1.6],\n",
       "       [-15.3],\n",
       "       [ 21. ],\n",
       "       [-17.2],\n",
       "       [  4.2],\n",
       "       [ -2.1],\n",
       "       [  3.3],\n",
       "       [ 27.7],\n",
       "       [ -1.4],\n",
       "       [ 29.4],\n",
       "       [  0.3],\n",
       "       [  4.2],\n",
       "       [ -3.2],\n",
       "       [  5.9],\n",
       "       [ 21. ],\n",
       "       [ 14.5],\n",
       "       [ 32.6],\n",
       "       [ -1.8],\n",
       "       [-22.3],\n",
       "       [ -6.4],\n",
       "       [ -2.3],\n",
       "       [-12. ],\n",
       "       [ 28.9],\n",
       "       [ 11.5],\n",
       "       [ 22.7],\n",
       "       [ 24.2],\n",
       "       [ 22.1],\n",
       "       [ 11.8],\n",
       "       [-26.4],\n",
       "       [-24.5],\n",
       "       [  4.8],\n",
       "       [-10.8],\n",
       "       [-14.9],\n",
       "       [-36.3],\n",
       "       [  6.5],\n",
       "       [ 46.6],\n",
       "       [ 11.6],\n",
       "       [-22.6],\n",
       "       [ 24.1],\n",
       "       [ 21.5],\n",
       "       [ -8.2],\n",
       "       [ 24.7],\n",
       "       [ 10.2],\n",
       "       [ 35.1],\n",
       "       [ -3.2],\n",
       "       [-31.8],\n",
       "       [ -5.9],\n",
       "       [ -9. ],\n",
       "       [  3. ],\n",
       "       [-13.3],\n",
       "       [ 31.7],\n",
       "       [  2.1],\n",
       "       [ -2. ],\n",
       "       [  6.7],\n",
       "       [ 16.3],\n",
       "       [ 19.4],\n",
       "       [ -5.1],\n",
       "       [ 36.2],\n",
       "       [ 24.8],\n",
       "       [ -3.7],\n",
       "       [ 31.2],\n",
       "       [ -1.9],\n",
       "       [ -1.9],\n",
       "       [ -6. ],\n",
       "       [-25. ],\n",
       "       [ 40.3],\n",
       "       [ 21.1],\n",
       "       [  6. ],\n",
       "       [ 46.9],\n",
       "       [-38.8],\n",
       "       [-15.9],\n",
       "       [ 27.2],\n",
       "       [ 11.7],\n",
       "       [  2.4],\n",
       "       [ 21.2],\n",
       "       [ 33.5],\n",
       "       [ 25.5],\n",
       "       [ -5.1],\n",
       "       [ 20.5],\n",
       "       [ -5.2],\n",
       "       [ 13.9],\n",
       "       [ 22.6],\n",
       "       [  2.8],\n",
       "       [ 21.3],\n",
       "       [  2.4],\n",
       "       [ -6.6],\n",
       "       [ 29.8],\n",
       "       [ 10.1],\n",
       "       [ -6.2],\n",
       "       [ 21.2],\n",
       "       [ 13. ],\n",
       "       [ 44.9],\n",
       "       [ 33.2],\n",
       "       [ 15.5],\n",
       "       [  9.3],\n",
       "       [-19.2],\n",
       "       [-12.2],\n",
       "       [ 29.4],\n",
       "       [ 15.1],\n",
       "       [-19.3],\n",
       "       [-11.4],\n",
       "       [ 29.2],\n",
       "       [-20.2],\n",
       "       [ 11.6],\n",
       "       [ 12.7],\n",
       "       [ 28.4],\n",
       "       [  5. ],\n",
       "       [ 21.2],\n",
       "       [-27.7],\n",
       "       [  5.5],\n",
       "       [-39.4],\n",
       "       [ 18.1],\n",
       "       [ 17.7],\n",
       "       [-13.1],\n",
       "       [ 13.9],\n",
       "       [  4.6],\n",
       "       [ -9.4],\n",
       "       [ -9.1],\n",
       "       [ 15.2],\n",
       "       [-13.2],\n",
       "       [ -1.7],\n",
       "       [ 35.7],\n",
       "       [ -0.1],\n",
       "       [ 50.8],\n",
       "       [ 24.7],\n",
       "       [ 22.2],\n",
       "       [ 32.8],\n",
       "       [ 49.6],\n",
       "       [-21.3],\n",
       "       [ -2.6],\n",
       "       [  3.5],\n",
       "       [ 22.1],\n",
       "       [-19.4],\n",
       "       [ 27.2],\n",
       "       [ 10.5],\n",
       "       [ 20.9],\n",
       "       [ 21.1],\n",
       "       [-21.9],\n",
       "       [ 32.8],\n",
       "       [ 21.8],\n",
       "       [  8.6],\n",
       "       [ -9.2],\n",
       "       [ 17.3],\n",
       "       [ 43.6],\n",
       "       [ 13.8],\n",
       "       [ 21.8],\n",
       "       [  3.6],\n",
       "       [-20.1],\n",
       "       [ 25.9],\n",
       "       [-18.3],\n",
       "       [ 34.1],\n",
       "       [ 34.8],\n",
       "       [ -5.9],\n",
       "       [ 32.9],\n",
       "       [  0.7],\n",
       "       [-14.6],\n",
       "       [-22.1],\n",
       "       [  1.1],\n",
       "       [ 24.1],\n",
       "       [ -8.1],\n",
       "       [ 18.4],\n",
       "       [-18.9],\n",
       "       [-30.3],\n",
       "       [ 20.1],\n",
       "       [ 26.3],\n",
       "       [-13.1],\n",
       "       [ 17.4],\n",
       "       [ -5.7],\n",
       "       [ 16.2],\n",
       "       [ 21.5],\n",
       "       [ -4.8],\n",
       "       [  4.2],\n",
       "       [ 38.2],\n",
       "       [-25.7],\n",
       "       [  9.3],\n",
       "       [-38.9],\n",
       "       [  6.7],\n",
       "       [ 32.5],\n",
       "       [-27.4],\n",
       "       [ 10.2],\n",
       "       [ 20.2],\n",
       "       [ 21.5],\n",
       "       [  2.7],\n",
       "       [ 29.4],\n",
       "       [ 18.8],\n",
       "       [ 11.2],\n",
       "       [  9.3],\n",
       "       [ -7. ],\n",
       "       [  8.5],\n",
       "       [-24.3],\n",
       "       [-15.8],\n",
       "       [ 31.4],\n",
       "       [ -3.6],\n",
       "       [ 21.2],\n",
       "       [ 25.4],\n",
       "       [ -8.1],\n",
       "       [  4.3],\n",
       "       [ 39.1],\n",
       "       [ -4.8],\n",
       "       [ -1.8],\n",
       "       [ 15.4],\n",
       "       [-17.9],\n",
       "       [ 28.7],\n",
       "       [ 13. ],\n",
       "       [  0.8],\n",
       "       [-12.2],\n",
       "       [-22.4],\n",
       "       [  7.5],\n",
       "       [ 48.3],\n",
       "       [ -2. ],\n",
       "       [-11.9],\n",
       "       [ 22.5],\n",
       "       [ 12.4],\n",
       "       [-34.6],\n",
       "       [ 41.9],\n",
       "       [ 27. ],\n",
       "       [-34.6],\n",
       "       [-24.1],\n",
       "       [ 30.9],\n",
       "       [ 34.9],\n",
       "       [ 32. ],\n",
       "       [ 12.1],\n",
       "       [ 16.8],\n",
       "       [-13. ],\n",
       "       [ 23.9],\n",
       "       [ 11.3],\n",
       "       [-21.9],\n",
       "       [ 18.1],\n",
       "       [ 45.7],\n",
       "       [ -5.5],\n",
       "       [-21. ],\n",
       "       [ 38.5],\n",
       "       [-26.8],\n",
       "       [ 49.9],\n",
       "       [ 16.5],\n",
       "       [ 43.2],\n",
       "       [-10. ],\n",
       "       [-16.2],\n",
       "       [-22.6],\n",
       "       [-14.9],\n",
       "       [  1.5],\n",
       "       [ -1. ],\n",
       "       [ 10.3],\n",
       "       [ 13. ],\n",
       "       [  2.1],\n",
       "       [-27. ],\n",
       "       [ 30.3],\n",
       "       [  2.1],\n",
       "       [-20.3],\n",
       "       [ 13.9],\n",
       "       [ 26.5],\n",
       "       [ -3.8],\n",
       "       [ 14.4],\n",
       "       [ -4.5],\n",
       "       [  5.3],\n",
       "       [ 10.3],\n",
       "       [-20.2],\n",
       "       [-23.9],\n",
       "       [ 21.9],\n",
       "       [-11.9],\n",
       "       [-29. ],\n",
       "       [-25.1],\n",
       "       [  3. ],\n",
       "       [ 23.7],\n",
       "       [ 18.1],\n",
       "       [ 18.2],\n",
       "       [ -4.8],\n",
       "       [  1.6],\n",
       "       [-33.3],\n",
       "       [ 14.1],\n",
       "       [  8.7],\n",
       "       [ -5.9],\n",
       "       [ 38.8],\n",
       "       [  8.8],\n",
       "       [ 11.1],\n",
       "       [-27.1],\n",
       "       [-18.7],\n",
       "       [ 28.3],\n",
       "       [ 31.7],\n",
       "       [  6.4],\n",
       "       [ 16.8],\n",
       "       [-28.6],\n",
       "       [ -6.2],\n",
       "       [ -9.6],\n",
       "       [ 24.3],\n",
       "       [ 48.1],\n",
       "       [ 43.4],\n",
       "       [ -9.7],\n",
       "       [ 42.6],\n",
       "       [-11.2],\n",
       "       [  2. ],\n",
       "       [ 31.8],\n",
       "       [ 22.2],\n",
       "       [-28.5],\n",
       "       [  7.5],\n",
       "       [-19. ],\n",
       "       [  7.6],\n",
       "       [ -5.7],\n",
       "       [-17.6],\n",
       "       [ 21.3],\n",
       "       [ 10.8],\n",
       "       [ 22.2],\n",
       "       [ -2.1],\n",
       "       [  5.9],\n",
       "       [ 35.4],\n",
       "       [-20.2],\n",
       "       [ 15.9],\n",
       "       [ 27.2],\n",
       "       [ -7.7],\n",
       "       [  4.3],\n",
       "       [ 22.2],\n",
       "       [ -2.4],\n",
       "       [-10.9],\n",
       "       [ 30.4],\n",
       "       [  3.4],\n",
       "       [ 24.8],\n",
       "       [ 24.4],\n",
       "       [ 33. ],\n",
       "       [ 21.7],\n",
       "       [ 22.2],\n",
       "       [ -8.9],\n",
       "       [ 20.4],\n",
       "       [ 24. ],\n",
       "       [-20.6],\n",
       "       [-16.9],\n",
       "       [ 19.9],\n",
       "       [ 28.5],\n",
       "       [ 20.1],\n",
       "       [-33.7],\n",
       "       [ -2. ],\n",
       "       [ -3.6],\n",
       "       [  4.5],\n",
       "       [ 15.1],\n",
       "       [ 28.1],\n",
       "       [ -8.7],\n",
       "       [ -2.2],\n",
       "       [ 27.6],\n",
       "       [-42.9],\n",
       "       [ 23.3],\n",
       "       [-35.3],\n",
       "       [ 12.8],\n",
       "       [ -7.8],\n",
       "       [-24.9],\n",
       "       [-33.6],\n",
       "       [ 11.8],\n",
       "       [-12.5],\n",
       "       [ 31. ],\n",
       "       [ 16.6],\n",
       "       [  6. ],\n",
       "       [-20.6],\n",
       "       [ 30.8],\n",
       "       [ 23.1],\n",
       "       [ 35.4],\n",
       "       [-13.2],\n",
       "       [ -1.5],\n",
       "       [ -8. ],\n",
       "       [ 17.1],\n",
       "       [ 12.9],\n",
       "       [ 26.5],\n",
       "       [-13. ],\n",
       "       [  3.6],\n",
       "       [  7.9],\n",
       "       [-22.4],\n",
       "       [ 16.5],\n",
       "       [ -9.9],\n",
       "       [-13.4],\n",
       "       [ 26.7],\n",
       "       [-19. ],\n",
       "       [-42.5],\n",
       "       [  0.1],\n",
       "       [-21.7],\n",
       "       [-28.5],\n",
       "       [ -6.8],\n",
       "       [ 19.8],\n",
       "       [ 34.6],\n",
       "       [-29.3],\n",
       "       [  0.2],\n",
       "       [ 40.1],\n",
       "       [ 24.9],\n",
       "       [ 21.7],\n",
       "       [ 18. ],\n",
       "       [-13.7],\n",
       "       [ -7.8],\n",
       "       [  1.4],\n",
       "       [ 18.3],\n",
       "       [ 15.6],\n",
       "       [-37.6],\n",
       "       [-17.1],\n",
       "       [ 13.3],\n",
       "       [ -6.3],\n",
       "       [ 31.3],\n",
       "       [-25. ],\n",
       "       [ -5.4],\n",
       "       [  2.3],\n",
       "       [ 40.9],\n",
       "       [ 29.6],\n",
       "       [-16.4],\n",
       "       [ -7.8],\n",
       "       [ -1.7],\n",
       "       [  0.5],\n",
       "       [-24. ],\n",
       "       [ 17.1],\n",
       "       [-31.6],\n",
       "       [ 16.2],\n",
       "       [ 28.9],\n",
       "       [  8.1],\n",
       "       [ 51.5],\n",
       "       [  0.3],\n",
       "       [ 15.3],\n",
       "       [ -2.4],\n",
       "       [-18.6],\n",
       "       [ 13.8],\n",
       "       [  6.8],\n",
       "       [-21.3],\n",
       "       [-11.5],\n",
       "       [-17.6],\n",
       "       [-15.7],\n",
       "       [ -3.6],\n",
       "       [-18.7],\n",
       "       [ 36.6],\n",
       "       [ -7.8],\n",
       "       [ -1.1],\n",
       "       [ 48.4],\n",
       "       [ 38.7],\n",
       "       [ 31.4],\n",
       "       [-18.5],\n",
       "       [ 21.3],\n",
       "       [  0.5],\n",
       "       [-20.8],\n",
       "       [ 13.6],\n",
       "       [  4.9],\n",
       "       [ -5.6],\n",
       "       [ 41.1],\n",
       "       [  9.4],\n",
       "       [  3.4],\n",
       "       [ -6.9],\n",
       "       [-33. ],\n",
       "       [-33.9],\n",
       "       [  7.7],\n",
       "       [  3.2],\n",
       "       [ 41.9],\n",
       "       [  2.8],\n",
       "       [-22.1],\n",
       "       [ 15.9],\n",
       "       [  2. ],\n",
       "       [ -6. ],\n",
       "       [ 31.8],\n",
       "       [-24.3],\n",
       "       [ -0.1],\n",
       "       [-17.3],\n",
       "       [ 29.4],\n",
       "       [  3.8],\n",
       "       [ 14.5],\n",
       "       [-23.4],\n",
       "       [  1.9],\n",
       "       [-22.4],\n",
       "       [ 19.3],\n",
       "       [-30.1],\n",
       "       [ 12.1],\n",
       "       [ 25. ],\n",
       "       [ -4.2],\n",
       "       [-12.1],\n",
       "       [ 12.4],\n",
       "       [ 23. ],\n",
       "       [ 18.3],\n",
       "       [ 18.1],\n",
       "       [ -8.8],\n",
       "       [ 21.1],\n",
       "       [  4. ],\n",
       "       [  3.7],\n",
       "       [-32.6],\n",
       "       [-17.9],\n",
       "       [ 23.7],\n",
       "       [-17.7],\n",
       "       [ 21.6],\n",
       "       [ 35. ],\n",
       "       [ 10.1],\n",
       "       [ -9.7],\n",
       "       [-12.2],\n",
       "       [ -6.7],\n",
       "       [-21.2],\n",
       "       [ -0.4],\n",
       "       [  6.2],\n",
       "       [ 20.3],\n",
       "       [ 22.5],\n",
       "       [  4.4],\n",
       "       [-27.4],\n",
       "       [-13.8],\n",
       "       [ 25.2],\n",
       "       [-14.6],\n",
       "       [ 37.4],\n",
       "       [ 16.8],\n",
       "       [  2.3],\n",
       "       [ 37.2],\n",
       "       [-27. ],\n",
       "       [ 20.5],\n",
       "       [ 33. ],\n",
       "       [ 11.3],\n",
       "       [-15.4],\n",
       "       [-19.6],\n",
       "       [ 21.9],\n",
       "       [ 10.5],\n",
       "       [ 38.6],\n",
       "       [ -3.3],\n",
       "       [ 13.7],\n",
       "       [ 38.8],\n",
       "       [ 16.9],\n",
       "       [ 17.2],\n",
       "       [ -3.7],\n",
       "       [-21.1],\n",
       "       [ 34.3],\n",
       "       [ 36. ],\n",
       "       [ 45.2],\n",
       "       [ 18.1],\n",
       "       [  6.6],\n",
       "       [ -0.3],\n",
       "       [-11. ],\n",
       "       [ -5.9],\n",
       "       [ 14.1],\n",
       "       [ 14.3],\n",
       "       [-27.6],\n",
       "       [  0.2],\n",
       "       [-37.6],\n",
       "       [  7.2],\n",
       "       [  9.1],\n",
       "       [-24.1],\n",
       "       [  4. ],\n",
       "       [ -6.8],\n",
       "       [ 32. ],\n",
       "       [ 24.8],\n",
       "       [ -2.2],\n",
       "       [ 24. ],\n",
       "       [ -3. ],\n",
       "       [-18.2],\n",
       "       [-16.9],\n",
       "       [ 30.1],\n",
       "       [-10.1],\n",
       "       [ 22.9],\n",
       "       [-10. ],\n",
       "       [ -1.6],\n",
       "       [-13.8],\n",
       "       [  7. ],\n",
       "       [-20.9],\n",
       "       [ 21.6],\n",
       "       [-25.7],\n",
       "       [-28.8],\n",
       "       [ -6.5],\n",
       "       [ -1.2],\n",
       "       [-16.5],\n",
       "       [ 16.2],\n",
       "       [-33.3],\n",
       "       [-16. ],\n",
       "       [ 14.3],\n",
       "       [ 12. ],\n",
       "       [-26.5],\n",
       "       [ 21.6],\n",
       "       [ 30.8],\n",
       "       [ 14.9],\n",
       "       [-22.9],\n",
       "       [-10.7],\n",
       "       [  3.3],\n",
       "       [ 21.1],\n",
       "       [ 48.1],\n",
       "       [  8.8],\n",
       "       [ 43.3],\n",
       "       [ 46.8],\n",
       "       [  0.9],\n",
       "       [-11.8],\n",
       "       [ 11.8],\n",
       "       [ -0.2],\n",
       "       [  4.7],\n",
       "       [-14. ],\n",
       "       [-44.2],\n",
       "       [ 11.1],\n",
       "       [ 29.8],\n",
       "       [ 42.3],\n",
       "       [ 29.1],\n",
       "       [-15. ],\n",
       "       [ 14.6],\n",
       "       [ 42.4],\n",
       "       [-14. ],\n",
       "       [-11.2],\n",
       "       [ 17.4],\n",
       "       [  9.2],\n",
       "       [ 19.3],\n",
       "       [ -8.7],\n",
       "       [  9.8],\n",
       "       [  2.4],\n",
       "       [ 16.6],\n",
       "       [ 27.1],\n",
       "       [-31.6],\n",
       "       [-12.8],\n",
       "       [-15.6],\n",
       "       [ -7.4],\n",
       "       [-20.6],\n",
       "       [ 17.2],\n",
       "       [-37.2],\n",
       "       [ 31.4],\n",
       "       [ -3.4],\n",
       "       [ 10.4],\n",
       "       [ 13.1],\n",
       "       [ -5.1],\n",
       "       [  1.1],\n",
       "       [-16.4],\n",
       "       [ 12.1],\n",
       "       [-35.9],\n",
       "       [ 18.4],\n",
       "       [  0.3],\n",
       "       [-36.5],\n",
       "       [ 21.7],\n",
       "       [-20.5],\n",
       "       [  1.9],\n",
       "       [ -1.2],\n",
       "       [-19.3],\n",
       "       [ 19.3],\n",
       "       [ 33.6],\n",
       "       [ 10.2],\n",
       "       [ 46.9],\n",
       "       [ 19.1],\n",
       "       [ 26.7],\n",
       "       [  4.6],\n",
       "       [  0.5],\n",
       "       [  1.4],\n",
       "       [ -2.5],\n",
       "       [-14.2],\n",
       "       [ 20.3],\n",
       "       [ 30.4],\n",
       "       [ 29.4],\n",
       "       [  6. ],\n",
       "       [-14.3],\n",
       "       [ 15.4],\n",
       "       [ 43. ],\n",
       "       [-16.9],\n",
       "       [ -2.7],\n",
       "       [ -0.9],\n",
       "       [ 13.1],\n",
       "       [-15.8],\n",
       "       [ 17.1],\n",
       "       [ 23.1],\n",
       "       [ -5.4],\n",
       "       [-18.4],\n",
       "       [ 12.9],\n",
       "       [ 23.4],\n",
       "       [ 37.7],\n",
       "       [  4.7],\n",
       "       [-32. ],\n",
       "       [ -4.5],\n",
       "       [ 37. ],\n",
       "       [ -9.8],\n",
       "       [ -6.8],\n",
       "       [ 13.3],\n",
       "       [  4.4],\n",
       "       [-13.5],\n",
       "       [ 12.1],\n",
       "       [  9.9],\n",
       "       [ 32.1],\n",
       "       [  9.2],\n",
       "       [-41.9],\n",
       "       [ 12.7],\n",
       "       [ -4.5],\n",
       "       [ 28.4],\n",
       "       [ 27.4],\n",
       "       [-19.5],\n",
       "       [-38.3],\n",
       "       [ 24.7],\n",
       "       [ 40.6],\n",
       "       [ 34.5],\n",
       "       [-33.2],\n",
       "       [-27.4],\n",
       "       [  7.3],\n",
       "       [ -3.4],\n",
       "       [ 20.2],\n",
       "       [ 21.9],\n",
       "       [ 41.6],\n",
       "       [ 29.8],\n",
       "       [  8.1],\n",
       "       [ 13.9],\n",
       "       [ 38.4],\n",
       "       [  7.2],\n",
       "       [ 32.3],\n",
       "       [ 21.5],\n",
       "       [-19.3],\n",
       "       [ 24.2],\n",
       "       [ -6.8],\n",
       "       [ -9.9],\n",
       "       [ 34.1],\n",
       "       [ 31.7],\n",
       "       [ 20.7],\n",
       "       [ -6.7],\n",
       "       [ 34.4],\n",
       "       [  6. ],\n",
       "       [ -4.8],\n",
       "       [  0.6],\n",
       "       [ 24.5],\n",
       "       [ 18.7],\n",
       "       [ 37.5],\n",
       "       [ -0.6],\n",
       "       [ 53.6],\n",
       "       [ -0.9],\n",
       "       [ 27.7],\n",
       "       [ 15.2],\n",
       "       [-21.3],\n",
       "       [ 22.7],\n",
       "       [  2.2],\n",
       "       [ 31. ],\n",
       "       [-41.5],\n",
       "       [-11.5],\n",
       "       [  5. ],\n",
       "       [-42.1],\n",
       "       [ -4.9],\n",
       "       [-15.8],\n",
       "       [-20.8],\n",
       "       [  7.6],\n",
       "       [ 19.8],\n",
       "       [ 27.4],\n",
       "       [-36.8],\n",
       "       [ 13.1],\n",
       "       [-23.2],\n",
       "       [-29.1],\n",
       "       [-10. ],\n",
       "       [ 22. ],\n",
       "       [ 50.9],\n",
       "       [ 23.6],\n",
       "       [-21.1],\n",
       "       [ 17.8],\n",
       "       [-15.3],\n",
       "       [  3.1],\n",
       "       [ -9.5],\n",
       "       [-20.7],\n",
       "       [ 14.9],\n",
       "       [  8.8],\n",
       "       [  6.5],\n",
       "       [ 27.8],\n",
       "       [  1.5],\n",
       "       [-34.8],\n",
       "       [ 34. ],\n",
       "       [  2.7],\n",
       "       [  1.5],\n",
       "       [  4.3],\n",
       "       [  4.3],\n",
       "       [ -4.4],\n",
       "       [ 27.1],\n",
       "       [  6.9],\n",
       "       [ 18.5],\n",
       "       [ 25.9],\n",
       "       [ 31.7],\n",
       "       [  0.4],\n",
       "       [-17.1],\n",
       "       [ -6.2],\n",
       "       [ 39.8],\n",
       "       [ 14.9],\n",
       "       [ -8.6],\n",
       "       [-30.3],\n",
       "       [-28.2],\n",
       "       [ 22. ],\n",
       "       [-31.9],\n",
       "       [  3.2],\n",
       "       [-23.6],\n",
       "       [ 43.2],\n",
       "       [-18.6],\n",
       "       [ -1.5],\n",
       "       [ 44. ],\n",
       "       [ 14.5],\n",
       "       [ 25.6],\n",
       "       [-12. ],\n",
       "       [-38.4],\n",
       "       [ 24.5],\n",
       "       [ 46.5],\n",
       "       [ 19.9],\n",
       "       [  9.4],\n",
       "       [ 34.7],\n",
       "       [ 35.5],\n",
       "       [ -4.4],\n",
       "       [ -0.6],\n",
       "       [ -2.8],\n",
       "       [ 40.5],\n",
       "       [ -7.5],\n",
       "       [ -9.1],\n",
       "       [ 11.8],\n",
       "       [ 35.2],\n",
       "       [ 11.3],\n",
       "       [-28. ],\n",
       "       [ 31.3],\n",
       "       [ 40.5],\n",
       "       [ 24.1],\n",
       "       [ -4.7],\n",
       "       [ 12.7],\n",
       "       [  3.5],\n",
       "       [ 13.3],\n",
       "       [ 13.1],\n",
       "       [ 31.4],\n",
       "       [ 22.3],\n",
       "       [ 33.3],\n",
       "       [ 10.8],\n",
       "       [ -7.7],\n",
       "       [ 31.5],\n",
       "       [-14.4],\n",
       "       [ 16.8],\n",
       "       [ 12.6],\n",
       "       [ 11.6],\n",
       "       [ 29.6],\n",
       "       [ 13. ],\n",
       "       [ 13.9],\n",
       "       [  9.5],\n",
       "       [ 42.5],\n",
       "       [ -2.8],\n",
       "       [-31.6],\n",
       "       [ 15.6],\n",
       "       [ 35.6],\n",
       "       [ 20.4],\n",
       "       [-15.5],\n",
       "       [ 42.5],\n",
       "       [ 12.1],\n",
       "       [ 10.1],\n",
       "       [ 24.8],\n",
       "       [  1.7],\n",
       "       [-12.3],\n",
       "       [ 36.1],\n",
       "       [ 25.4],\n",
       "       [-14.3],\n",
       "       [-34.1],\n",
       "       [  2.5],\n",
       "       [ -4.3],\n",
       "       [ 21.3],\n",
       "       [  2.4],\n",
       "       [-38.2],\n",
       "       [ 40.4],\n",
       "       [ -2.3],\n",
       "       [ 23.9],\n",
       "       [-35.8],\n",
       "       [  1.9],\n",
       "       [ -3.5],\n",
       "       [-13.3],\n",
       "       [ -1.6],\n",
       "       [ 17.6],\n",
       "       [ -1.6],\n",
       "       [-39.2],\n",
       "       [-13. ],\n",
       "       [ 16.5],\n",
       "       [ 16. ],\n",
       "       [ -5.4],\n",
       "       [  2.2],\n",
       "       [ 45.2],\n",
       "       [ -8. ],\n",
       "       [ 14.2],\n",
       "       [  6. ],\n",
       "       [ 15.7],\n",
       "       [ -9.9],\n",
       "       [  3.9],\n",
       "       [  2. ],\n",
       "       [ -6.8],\n",
       "       [ 31.3],\n",
       "       [ -9.1],\n",
       "       [-18.7],\n",
       "       [ 10.7],\n",
       "       [ -5.5],\n",
       "       [ 13.7],\n",
       "       [ 24.3],\n",
       "       [  5.6],\n",
       "       [ 25.2],\n",
       "       [ 18.3],\n",
       "       [ 28.7],\n",
       "       [  6. ],\n",
       "       [ 34.4],\n",
       "       [ 18.6],\n",
       "       [-21.5],\n",
       "       [-14.8]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['targets'].round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc707407-341c-450d-aa54-9e3f7e53efc9",
   "metadata": {},
   "source": [
    "##### Plotting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcf89c7-2690-473a-b2d4-b77775c1dfe5",
   "metadata": {},
   "source": [
    "Finally , we can us the same techniques as in our previous minimal example. We can plot the output against the target. Since, we expect them to be very close to each other, the lines should be as close to 45 deegree  as possible. and that is precisely, what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3914000d-2115-4764-94b4-3d7e7df0544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.squeeze(model.predict_on_batch(training_data['inputs'])), np.squeeze(training_data['targets']))\n",
    "plt.xlabel('outputs')\n",
    "plt.ylabel('targets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48181a9f-39d7-4b3e-8d23-810afc13af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THis is not running, it tells me to restart kernnel !!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eafe06-1fa6-4835-98fb-cdd4588569d5",
   "metadata": {},
   "source": [
    "#### Customizing your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bdcdb2-e1a2-4142-8435-c6189b691c40",
   "metadata": {},
   "source": [
    "##### Numpy Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30b1419-4686-44f8-bcfe-946f0778d3fc",
   "metadata": {},
   "source": [
    "In our numpy neural network, we had several decision to make, First ,we had to select the bset way to initialize the weight.\n",
    "\n",
    "Back then, we choose the starting point of the weight and biases to be random numbers between -0.1 to 0.1. Here, we left the default tensorflow settings to do the magic.\n",
    "Infact, if we want o make this example as close to the original as possible , we can set the random  uniform initializer where we definre the layer.\n",
    "\n",
    "Instead of having a single arguement, output size , we can also have a kernel initializer and a bias initializer.\n",
    "\n",
    "kernel here is a broader term for weight\n",
    "\n",
    "tf.keras.layers.Dense(output)_size,kernel_initializer,bias_initializer)\n",
    "function that is laying dowm the model(used to 'stack layers') and initialize weights \n",
    "\n",
    "Let kernel  -initialer = tf.random_uniform _initializer from -0.1 to 0.1, imilarly , bias initializer will be equal to the same expression.\n",
    "\n",
    "Naturally, you can specify other ways you want your weights  and bias to be initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a3f3f5-9d65-4346-b8ac-3b2141c6cc5b",
   "metadata": {},
   "source": [
    "##### Set a learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3560763-1e53-480e-8b62-8a88838c7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rate = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224bb503-03e1-4504-bf3f-eefff3fba506",
   "metadata": {},
   "source": [
    "The learning rate is an integral part of the optimizer. Here,we took the default Stochastic gradient descent or sgd . we can create a variable called custom optimizer = tf.keras.optimizer.SGD and specify several arguement. The only one we know is the learning rate\n",
    "o let's set it to 0.02 as we did in the numpy example\n",
    "tf.keras.optimizers.SGD(leaning_rate)Stochastic gradient descent optimizers including support for learning rate momentum, decay, etc.\n",
    "Next we should replace the string as agda model and compile it with a new custom optimizer.\n",
    "\n",
    "The new result will be pracically thesame but the small difference is that we have to set the learning rate ourselves.\n",
    "\n",
    "Is a good idea for you to know that you can always refer to the tensor flow documentation and figure out how to customize your model\n",
    "\n",
    "Now the only thing we have left is the loss.\n",
    "\n",
    "We will use the built in losses without customizing them since matters are much more complicated\n",
    "\n",
    "In the future , you may want to try out new loss function.\n",
    "Let's train our model with this tweet , surprisngly , the results are not different, our weight and biases are still the same as well as the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f494c340-856b-4f47-bf5a-82efcff4527e",
   "metadata": {},
   "source": [
    "#### Neural Networks (Tensor Flow): Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60109bb-ad84-44f9-bc44-879d919f1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "What two types of Neural Networks are there\n",
    "Biological Neural network and Artificial Neural Network\n",
    "\n",
    "What are ANNs used for?\n",
    "Reproduce Human Brain Functions\n",
    "\n",
    "What are the Three Parts of a Neuron?\n",
    "Dendrite, Soma, Axon\n",
    "\n",
    "ANNs are efficient data-driven modelling tools widely used for nonlinear systems dynamic modelling and identification, due to their universal approximation capabilities and flexible structure that allow to captu\n",
    "\n",
    "What Problems do ANNs not solve?\n",
    "Geometric problems\n",
    "\n",
    "What futuristic actions can not be performed by ANNs?\n",
    "Function Approximation\n",
    "\n",
    "The best type of ANN is one that is built for a specific purpose and not a general purpose. true\n",
    "What 2 subjects are neural networks usually associated with?\n",
    "Biology & Artificial-engineering\n",
    "\n",
    "Which of the following is/are true for neural networks\n",
    "The training time depends on the size of the network.\n",
    "\n",
    "Neural networks can be simulated on a conventional computer.\n",
    "\n",
    "What is/are the advantages of neural networks over conventional computers?\n",
    "They have the ability to learn by example They are more fault tolerant They are more suited for real time operation due to their high 'computational' rates\n",
    "\n",
    "What is an activation value?\n",
    "weighted sum of inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0cd7b-81f5-4fa1-b26e-3b25c2e9600d",
   "metadata": {},
   "source": [
    "### Week 13: Day 3 â€“ Deep Nets Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3744450c-c4db-41f6-b044-933ac5126ac5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fe1312-4dfc-4af9-90d4-aa64fdb1a3d6",
   "metadata": {},
   "source": [
    "In one of the earlier sessions ,we say how to train a simple linear regression model. It has two inputs and a single output. that is the type of neural network but it has no depth. we use the linear model that learned the function\n",
    ",This function was the best fit of the dta according to the L2- Norm Loss with a couple of 100 iterations. Most real life dependencies cannot be modelled\n",
    "with a simple linear combination and because we want to be better forecasters we need better models. Most of the time ,this means working with a model that is more sophisticated than a linear model.\n",
    "\n",
    "Such complexity is usually achieved by both linear and non linear operations\n",
    "mixing linear and non- linearity allows us to model arbitrary functions or in other words functions with strange conventional shapes. Basically ,our models changes from inputs that are linearly combined  resulting in outputs to inputs that are lineraly combined and then go through some non linear transformation\n",
    "resulting in output. Probably, you will bre having the same example of non linearity.\n",
    " \n",
    "Input -> Linear Combination xw + b -> Non- linearity (sigmoid function) -> output\n",
    "\n",
    "A nomally used non linearity is the sigmoid function, it defines a sigma x = \n",
    "1 divide by 1 + e to the power minus x.\n",
    "the initial linear combination in the added non linearity forms a layer. the layer is the building block of neural network. \n",
    "When we have more than one layer then we are talking about deep neural networ\n",
    "k. Hence forth, layers will be an important topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8299f531-355c-43ef-a0c1-362cb2d8ae70",
   "metadata": {},
   "source": [
    "sigmoid = Ïƒ (x) = 1 / 1 + e ^-x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef09f6-e0cc-4316-b841-de4d41b781c9",
   "metadata": {},
   "source": [
    "#### What is a deep net ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bde317-ecfe-4119-aa8f-eb0d3704e2bd",
   "metadata": {},
   "source": [
    "This is our first layer, it is called the input layer, that is basically the data that we have.We take in the inputs and get outputs. the main thing now is that we can now use this outut as input fo another layer,and another one and it continues until we decide to stop.The kast layer we built is the output layer\n",
    "That is basically what we compare our targets to.\n",
    "\n",
    "The first layer is the input layer and the last layer is the output layer, all the layer between are called hidden layers.We call tham hidden as we know the inputs and we get the outputs but we dont know what happens between as these operations are hidden.\n",
    "Stacking layer one after the other produces a deep network, and this is what we call a deep net. the building block of the hidden layer are called hidden\n",
    "units or nodes.Here is a hidden unit. In mathematical terms , if Age is the tensor related to the hidden layer, each hidden unit is an element of that tensor. The number of hidden units in a hidden layer is often referred to as\n",
    "the width of the layer.Usually but not always , we stack layers wuth the same width so that the layer width is equal to the width of the entire network. we say how wide the deep network is , let,s examine how deep it can be . Depth is an important ingredient as it refers to the number of hidden layers in a network . When we createa machine learning algorithm, we choose its width and depth. we refer to this values as hyperparameters. Hyperparameters should not be mistaken by pararmeters.\n",
    "\n",
    "Parameters are the weights(w) and the biases(b) while Hyperparameters are the width ,depth ,learning rate() and some other variables that we will see later\n",
    ".The main difference between the two is that the value of the parameters through optimization while hyperparameters are set by us before we start optimizing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb41683-f25d-4550-9e1d-0ab9f14ecacb",
   "metadata": {},
   "source": [
    "#### Really understand deep nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee5f4d1-6524-4ec8-8928-787445481a12",
   "metadata": {},
   "source": [
    "+ An illustrastion of deep nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf940c2a-5892-42c8-84a0-26fb49d3208d",
   "metadata": {},
   "source": [
    "The first layer you see is the input layer,Each circle represents a seperate input. In the graph we have 8 inputs.These inputs are the data we feed to train the model. In the Tensor flow frame work,this is he place for input. Remember the weather forecast example,imagine it has 8 inputs, For instance, average temperature, max. temperature,min. temperature, humidity,Precipitation,Atmospheric pressure, cloud cover , Visibility and distance. Remember,we combined this input linearity and then add non-linearity, how can we do that? well, linearity is easy.We will just use the good old linear model.\n",
    "\n",
    "Its input are the X and to combine the linearity , we need weight. in this example, the weight are in 8 X 9 metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b48f72-46ee-40a9-9eeb-996455fdbdf4",
   "metadata": {},
   "source": [
    "1 X 8 * 8 X 9 = 1 X 9\n",
    "\n",
    "Will give us an object with a shape of 1 x 9\n",
    "\n",
    "Therefore following this operation, we wil get a vector of weight 9 or a 1 X 9 matrix.\n",
    "\n",
    "This is exactly ,the number of hidden units we have in the first hidden layer\n",
    "All these errors will get us from the input to  the first hidden layer.\n",
    "Each error represents the mathematical transformation of a certain value. Though a certain weight is applied , then a non-linearity is added. Note, that a non-linearity doesn't change the shape of the expression. It only changes its linearity.\n",
    "\n",
    "Errors represents weight and non-linearity but how many weights are there?\n",
    "The weight matrix is 8 X 9 , so there are 72 weights.\n",
    "We also have 72 errors.\n",
    "\n",
    "8 X 9 = 72\n",
    "\n",
    "9 errors goes out of each input unit and into a hidden unit. Since we have 8 inputs units, this gives us 72 errors.\n",
    "\n",
    "To be even more specific, each weight has 2 numbers, the first one indicates the input it is referring to \n",
    "W36(input)and the second one indicates the (hidden )unit it is referring to . For example, weight 36 is applied to the third input. It is involved in calculating the 6 hidden unit.\n",
    "\n",
    "In thesame way, W16,W26,W46,W56,W66,W76,W86 all participate in computing the 6th hidden unit.\n",
    "\n",
    "They are linearly compbined and then non linearity is added in other to produce the 6th hidden unit.\n",
    "\n",
    "In thesame way, we gave each of the other hidden unit.\n",
    "Then we have the first hidden layer.Using the same logic, we can linearly combine the hidden units and apply a non - linearity.\n",
    "\n",
    "Indeed ,this time though there are 9 input units and 9 output units. Therefore ,the weight will be contained in a 9 X 9 matrix and there will be 81 errors.\n",
    "\n",
    "Finally , we apply non -linearity and we reached the second hidden layer, we can go on and on and on like this and we can ad a 100 hidden layers if we want.\n",
    "\n",
    "That is a question of how deep we want our deep net to be. We will discuss further on this later.\n",
    "\n",
    "Finally, we have the last hidden layer.When we apply the operation once again ,we will reach the output layer. The output units depends on the number of output we will like to have. In this picture, we have 4 , but they may be humidity, precipitation, temperature and pressure for the next day.\n",
    "\n",
    "To reach this point we will have a 9 X 4 ways matrix.Whcih refers to 36 errors or 36 weights. Exactly what we expected.\n",
    "As before our optimization goal is finding values for weight matrices that will allows to convert inputs into correct outputs as best as we can.This time though we are not using a single linear model but a complex infracture with a much higher probability of delivering a meaning for result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435e7583-440f-48bf-aa1a-bd794893b230",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Why do we need non-linearities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bef823-490a-4b60-b178-5d8459722227",
   "metadata": {},
   "source": [
    "Non-linearities are needed so that we can represent more complecated relationships.\n",
    "\n",
    "Why that is true, it is not the full picture. An important consequence of including non linearity is the ability to stack layers.\n",
    "Stacking layer is a process of placing one layer after the other in a meaningful way. Remember that it is fundamental. The point we will make is that we cannot stack layers when we have only linear relationships.\n",
    "\n",
    "Imagine we have a single hidden layer and there are no non linearities(just linear combinations)\n",
    "\n",
    "We have 8 input nodes,9 hidden layers and 4 output nodes. Therefore we have an 8 x 9 matrix for the linear relationship between the input layer and the hidden layer.Let's call this matrix W1 and the hidden unit h\n",
    "\n",
    "According to the linear model '\n",
    "h = X * w1\n",
    "\n",
    "let's ignore the biases for a while .So our hidden unit are civilised in a matrix h and a shape of 1  X 9.\n",
    "\n",
    "Now let's get to the output layer from the hidden layer\n",
    "According to yhe linear model, \n",
    "y = h * W2\n",
    "\n",
    "We have w2 as these weights are diferent, we already know that h matrix is equal to X x w1\n",
    "\n",
    "Let's replace h in this equation\n",
    "y = x * w1 * w2\n",
    "\n",
    "But w1(8x9)and w2(9x4) can be multiplied, what we get is  acombined matrix\n",
    "\n",
    "X * w* with dimensions 8 x 4. Our deepnet can be simplified into a linear model that loos like this.\n",
    "\n",
    "y = x * W*\n",
    "\n",
    "with this we find out that the hidden layers is completely useless in this\n",
    "case. We can just  train the simple linear model and we can get thesame \n",
    "result. In mathematics this seems like an obvious fact but in machine learning ,it is not so clear from the beginning.\n",
    "\n",
    "Two consecutive linear transformations are equivalent to a single one. Even if we add 100 layers , the problem will be simplified to a single transformation\n",
    "That is th reason we need non linearities. without them ,stack in layers one after the other will be meaningless. Without stacking layers we will have no depth, with out depth each and every problem will be equal to the simple linear example we did earlier.\n",
    "\n",
    "Non-Linaerities  ->  Stacking layers ->  Depth -> Deep Learning.\n",
    "\n",
    "And many machine learning will tell you it was border line machine learning.\n",
    "\n",
    "In summary , to have deep nets and find complex relationships through arbitrary functions, we need non-linearities.\n",
    "\n",
    "Input -> Linear Combination  -> Non - Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e051d2de-0001-4310-b6cc-318e47fbedb7",
   "metadata": {},
   "source": [
    "In other to have deep nets and find complete relationships through arbitrary function we need non linearity.\n",
    "\n",
    "It is used to represent complicated relationship.\n",
    "we cant stack layer if we have just linear relationship.\n",
    "\n",
    "h = x *W1\n",
    "\n",
    "y = x * W1 * W2 =\n",
    "\n",
    "X *W*\n",
    "\n",
    "we can train the simple linear model and get same result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16722b2c-30a8-411b-be5c-8f890a55e177",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64081bd9-5807-4cd6-aae9-20a0b75ff63d",
   "metadata": {},
   "source": [
    "In a machine learning context, non linaerity are also called activation functions\n",
    "Activation functions transform input into output of a differnt kind. Think about the temperature outside assuming you wake up and the sun is shinning so you put on some light cloths.You go out and feel warm and comfortable.You carry your jacket in your hands, in the afternoon, the temperature starts decreasing.\n",
    "\n",
    "Initially, you dont feel the difference, at some points your brain says it is getting cold, you listen to your brain and put on your jacket.The input you got was the change in the temperature,the activation function transforms this inpput into an action(put on the jacket or continue carying it).\n",
    "\n",
    "This is also the output after the transformation, it is a binary variable(Jacket or no Jacket).\n",
    "\n",
    "This is the basic logic behind non -linearity.The change in the temperature was following a linear model as it was steadily decreasing(e.g.falling 2-3 degrees each hour).\n",
    "The activation function transforms this relationship into an output linked to the temperature or was of a different kind.\n",
    "\n",
    "In machine learning, we have differnt activation functions but there are few used much more frequently than others.\n",
    "\n",
    "Table showing four of the most commonly used activation functions\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ea6c183-02c2-4cb8-a81a-4579efaa8905",
   "metadata": {},
   "source": [
    "Name       Formula         Derivative      Graph      Range\n",
    " \n",
    "Sigmoid    Ïƒ (x) =         aÏƒ (Ïƒ ) / aÏƒ   func.     (0,1)\n",
    "           1 / 1 + e ^-x   a(Ïƒ )(1-Ïƒ(Ïƒ ))  graph\n",
    "(logistic                   \n",
    "function)\n",
    "\n",
    "\n",
    "TanH       tanh(Ïƒ)=e^Ïƒ -e^Ïƒ /   atanh(Ïƒ)/eÏƒ =      (-1,1)  \n",
    "(hyperbolic       e^Ïƒ + e^Ïƒ     4/ (e^Ïƒ + e^-Ïƒ)2\n",
    "tangent)\n",
    "\n",
    "\n",
    "ReLu               relu(a)=       a relu(Ïƒ)/aÏƒ       (0,âˆž)\n",
    "(rectified         max(0,a)       = {0,if Ïƒ â‰¤ 0\n",
    "linear unit)                       {1,if  Ïƒ  0\n",
    "\n",
    "\n",
    "Softmax       Ïƒi(Î´)=         aÎ´i(Ïƒ)/aÏƒj=            (0,1)            eÏƒ^i/Î£je^Ïƒj      Î´i(Ïƒ)(Î´ij-Ïƒj(Ïƒ)       graph \n",
    "                            where Î´ij is 1 if    is diff.\n",
    "                            i=j,0 otherwise     every time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9426071-e7e0-464a-ab1f-de4dad20b088",
   "metadata": {},
   "source": [
    "The derivative is an essential part of the gradient descent, naturallly when we work with TensorFlow,it's calculated automatically.\n",
    "The purpose of this lesson is understanding these functions, the graph and ranges in a way tha will allows to acquire intuitions about the way they behave.\n",
    "\n",
    "Once, we have applied the sigmoid as an activator, all the outputs will be contained in the range from 0 to 1.\n",
    "the output is somewhat standardized.\n",
    "\n",
    "All these functions are activators, what makes them similar is the fact that all the grapg are monotonic, continuous and differentiable. These are important properties needed for the optimization process.\n",
    "\n",
    "Finally, Activation functions are also called Transfer functions because of their Transformation properties.\n",
    "\n",
    "The two terms are used interchangeably in ML context but have differences in other fields.To avoid confusions,we will stick to the term activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40658b1-8dbf-4873-a7ec-b01a14245bc2",
   "metadata": {},
   "source": [
    "#### Softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adcd354-458a-49f3-a5d5-99e4da421d5f",
   "metadata": {},
   "source": [
    "We said the softmax function have no definite graph\n",
    "This is because this function is differnt.\n",
    "\n",
    "If we look at the formala carefully , we will notice the difference between the softmax and the others.\n",
    "It takes an auguement the whole vector a instead of an individual element.\n",
    "\n",
    "The softmax function is equal to the exponential at position i divided by the sum of exponential of all element of the vector.\n",
    "\n",
    "So while the other activation functions get an input value and transform it regardless of all the other elements, the softmax considers the information about the whole set of numbers we have\n",
    "\n",
    "For example;\n",
    "let a be equal to w plus b which is our well known model, then we can say that the output of y will be the softmax of a\n",
    "a = xw + b\n",
    "y = softmax(a)\n",
    "\n",
    "\n",
    "Let's look at the hidden layer with 3 units, so a here is equal to hw + b\n",
    "\n",
    "ah = hw + b\n",
    "a = [-0.21,0.47,1.72]\n",
    "\n",
    "After transforming it to a linear combination, we obtain a vector a with 3 elements.\n",
    "\n",
    "\n",
    "Now, if we use the different activation such as the sigmoid\n",
    "it will simply apply the formala for  each of the 3 numbers and we will obtain a new vector containing the 3 new numbers\n",
    "\n",
    "sigmoid activation:\n",
    "\n",
    "Sigmoid(a)= [sigmoid(-0.21),sigmoid(0.47,sigmoid(1.72)]\n",
    "\n",
    "\n",
    "Softmax is special, each elment in the output depends on the entire set of element of the input\n",
    "\n",
    "softmax(a) = eÏƒ^i/Î£je^Ïƒj\n",
    "\n",
    "Let's find the softmax of a\n",
    "\n",
    "First, we calculate the denominator, it is given by e to the power of -0.21 plus e to the power of 0.47 plus e to the power of 1.72 which is equal to 8.\n",
    "\n",
    "Î£je^Ïƒj = e^0.21 + e^0.47 +  e ^1.72 = 8\n",
    "\n",
    "Then we must divide each exponential by this denominator to get the new vector.The result is 0.1, 0.2 and 0.7\n",
    "\n",
    "softmax(a) = [ e^0.21 /8 , e^0.47 /8 ,   e ^1.72 /8 ]\n",
    "\n",
    "y =[0.1, 0.2, 0.7] = 1\n",
    "\n",
    "This is our output layer\n",
    "\n",
    "A key aspect in the softmax transformation  is that the values and output are in the range from zero to one. The sum is exactly 1.\n",
    "\n",
    "What else has such a property ?\n",
    "\n",
    "Probabilities\n",
    "\n",
    "The point of the softmax transformation is that it transforms a bunch of arbitrary large or small numbers that comes out of previous layers and fit them into a valid probability distribution nd convert them into probabilities.\n",
    "This is extremely important and useful.\n",
    "\n",
    "Remember our example with cats, dogs we saw earlier, one phot was described by a vector containing 0.1 ,0.2 and 0.7. This can be achieved through a soft max transformation\n",
    "\n",
    "Now that we know we are talking about probabilities, we can confidently say that we are 70% sure the image is a picture of  a horse. This makes everything so intuitive and useful that the soft max activation is often used as the output laye in classification problems. So no matter what happens before,the final output of the algorithm is a probability distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5241294e-4423-473d-8edf-b6ee8c55fc7a",
   "metadata": {},
   "source": [
    "#### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf8593d-5aeb-4754-9e9c-6906724678b8",
   "metadata": {},
   "source": [
    "Backpropagation is the most intuitive lesson yet the hardest to grasp in mathematical terms.We will firt explore the intuition behind it then look into the mathematics. \n",
    "Maths is optional but is good to look at it for better understanding.\n",
    "\n",
    "Backpropagation of output layer\n",
    "\n",
    "aL / auij  =   aE/ayj ayj/aÏƒj  aÏƒj /auij  = \n",
    "\n",
    "(yi - tj)yj(1 - yi)xi = Î´jxi\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bd9834-e544-4f3c-8efd-3660df8d0139",
   "metadata": {},
   "source": [
    "Backpropagation of hidden layer\n",
    "\n",
    "aL / awij = Î´jxi,\n",
    "\n",
    "where Î´j = Î£k Î´kwjkyj(1-yj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403127a6-f613-4805-8162-6b5b87f2bef3",
   "metadata": {},
   "source": [
    "We have seen and understood how layers are stacked\n",
    "\n",
    "\n",
    "Stacking layers\n",
    "\n",
    "input layer -> Hidden layers 1 to 3 -> Output layer\n",
    "\n",
    "We have also explored  few activation functions such as the softmax, Relu TanH , sigmoid etc .\n",
    "We also said that thetraining process  consists of updating parameters to the gradient descent for optimmizing the objective function.\n",
    "\n",
    "In supervised ML, the process of uptimization consists of minimizing the loss. Our update was directly related to the patial derivative of the loss and indiretly related to the errors or deltas as wrs call them. The deltas where the difference between the output and the target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b353e153-73fe-4de2-90a6-fcca9689f803",
   "metadata": {},
   "source": [
    " Î´i,output = yi - ti\n",
    "    \n",
    "Deltas  with the hidden layers are difficult to define\n",
    "\n",
    " Î´i,hidden = ?\n",
    "    \n",
    "Still they have a similar meaning   \n",
    "\n",
    "The procedure for calculating that is called Backpropagation of errors.\n",
    "\n",
    "Having these deltas allows us to vary parametters using the familiar update rule.\n",
    "\n",
    "wi+1 = wi- n Î£1xiÎ´i\n",
    "\n",
    "bi+1 = bi - n Î£1Î´i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32a70ae-3bb8-40d5-94f1-14aab97fa63b",
   "metadata": {},
   "source": [
    "Let's start rom the other side of the coin,\n",
    "\n",
    "Forward Propagation\n",
    "\n",
    "This is the process of pushing inputs to the net. At the end of each epoch , the obtained output  are compared to the target that formed the errors. Then we back Propagate the derivatives and change each parameters so that the errors at the next epoch are minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d906e-6925-480f-8734-b35f2915ded0",
   "metadata": {},
   "source": [
    "For the minimal example, the back propagation consists of a single step linning the weight given the errors we obtained\n",
    "\n",
    "wi+1 = wi- n Î£1xiei\n",
    "\n",
    "bi+1 = bi - n Î£1ei\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c74a00b-650f-4796-a13f-138d2567a1b2",
   "metadata": {},
   "source": [
    "This is where it gets a little tricky, when we have a deep net,We must update all the weihgt related to the input layer and the hidden layers.\n",
    "\n",
    "In the example , we have 270 weights, this means we have to manually draw all 270 errors you see in the diagram.\n",
    "\n",
    "So updating all 270 weights is a big deal but we also introduced activation functions, this means we have to update the weights accordingly considering the use non -linearities and their derivatives.\n",
    "\n",
    "Finally to update the weights we must compare the output to the targets. This is done for each layer.But we have no target for the hidden units. We dont know the errors, so how do we update the weights?\n",
    "\n",
    "That's what BackPropagation  is all about , we must arrive the appropriate update as if we have targets.\n",
    "\n",
    "Î´i,hidden = ? Î´i,hidden = ? Î´i,hidden = ?\n",
    "\n",
    "                             Î´i,hidden = yi - ti\n",
    "                              ei -> t1\n",
    "                              e2 -> t2\n",
    "                              e3 -> t3\n",
    "                              e4  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22aad69-dd9d-464c-9a75-6d26ac3f9698",
   "metadata": {},
   "source": [
    "The way academics solve this issue is through errors, the main point is that we can trace the contribution of each unit, hidden or not to the error of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc173a2-627f-402f-a1dc-93031bf098da",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Backpropagation â€“ intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd2ce9-4ef5-4725-ba10-94e8255d8fd4",
   "metadata": {},
   "source": [
    "\n",
    "Using example of deep net to further explain backpropagation'\n",
    "\n",
    "Our net is quite simple, Each node is labelled so we have input x1 and X2\n",
    "\n",
    "hidden layer units h1,h2,h3\n",
    "\n",
    "Output layer units y1 and y2\n",
    "\n",
    "Finally, the targets, t1 and t2\n",
    " \n",
    " \n",
    " For the first part of the net, the weight are as follows:\n",
    " \n",
    " w11,w12,w13,w21,w22,w23 etc\n",
    " \n",
    " For the second part we named them:\n",
    " \n",
    " u11,u12,u21,u22,u31,u32 \n",
    " \n",
    " so that we can differentiate the two types of weights and this is vital.\n",
    " \n",
    " We know the errors associated with y1 and y2 as it depends on known targets.\n",
    " So let's call the two errors e1 and e2\n",
    " \n",
    " Base on them, we can adjust the weight label with u. Each u contributes to a single error.\n",
    " \n",
    " For exampl:\n",
    " U11 contributes to a single error e1\n",
    " \n",
    " Then we find its derivative(gradient) and update the \n",
    " \n",
    " coefficient  . (Î· -eta) \n",
    " \n",
    " Ui + 1 = Ui - Î·â–½uL(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a126a2-4d88-417b-be56-c1b1744dd6b9",
   "metadata": {},
   "source": [
    "Looking at w11, it helped us to predict h1, but we need a h1 to calculate y1 and y2, thus it played a roll in determinig both errors,e1 and e2.\n",
    "why u11 contributes to a single error, w11 contributes to both errors, therefore its adjustment rule must be different. The solution to this problem is to take the errors and backpropagate them through the net using the weight we can measure the contribution of each hidden unit to the respective errors.\n",
    "\n",
    "Then once,we found out the contribution of each hidden unit to the respective errors we can update the wu weight.\n",
    "\n",
    "So essentially,thrrough backpropagation, the algorithm identifies which weight leads to which errors ,then it adjusts the weight that has a bigger contribution to the errors by more:\n",
    "than the weights with a smaller contribiution to the errors by less\n",
    "\n",
    "\n",
    "A big problem arises when we also consider the activation functions, they introduce additional complexity to the process.\n",
    "Linear contribution are easy but non linear ones are tougher.\n",
    "\n",
    "Imagine backpropagation  in our introductory net, once you understand it it seems very simple and totally straight forward. Mathematically , it is rough tosay the least, that is why backpropagation is the biggest challenge to the speed of an algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d77e4e9-347a-40d2-8046-d297f5e613b6",
   "metadata": {},
   "source": [
    "aL /awij  = Î´jxi, where Î´j = Î£k Î´k wjk yi(1 -yj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f9c53e-aaeb-4c68-a85f-7b03db85df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ïƒ (x) = 1 / 1 + e ^-x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6247e3-0063-43fa-ba41-f96c182136f8",
   "metadata": {},
   "source": [
    "### Week 13: Day 3 â€“ Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ec0e4-823e-46cf-bc2d-d626d502e422",
   "metadata": {},
   "source": [
    "#### Underfitting and overfitting. A regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca867519-6c16-49bd-913e-5702eda8214a",
   "metadata": {},
   "source": [
    "+  One of the most commonly asked questions at interviews is about overfitting. A Recruiter will probably bring up tht topic, what is overfitting and how do we deal with it ?\n",
    "\n",
    "There are two concept that are interrelated, underfitting and overfitting. They go together and understanding one helps us to understand the other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146cfd6-a24c-4843-a970-a317bd82f4a8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "+ Overfitting means our training has focusing on a particular trining set so much ,it has missed the point. They capture the random noise.\n",
    "Over fitted model are models that are so good at modelling the training data that they fair or come very near each observation, the proble is that the random\n",
    "noise is captured in each observation.\n",
    "\n",
    "\n",
    "For example: imagine you are trying to predict the Euro dollar exchange rate based on 50 common FX indicators\n",
    "\n",
    "+ Bolliger Bands\n",
    "\n",
    "+ Macd\n",
    "\n",
    "+ Parabolic SAR\n",
    "\n",
    "+ Stochastic  etc..\n",
    "\n",
    "You train your models and get low cost and high accuracy.Infact, you believe that with 99.99% accuracy you can predict the exchange rate.\n",
    "\n",
    "Comfident with your machine learning skills you start training with real money.Unfortunately, most orders you placed failed miseareable.\n",
    "\n",
    "In the end ,you loose all your money because you trusted the amazing model so much. What happened with your model is that it probably overfit the data. It was trained to explaining the training data so well that it has 'missed the point'.\n",
    "\n",
    "Instead of finding the dependence between the Euro and and the dollar. You modelled the noise.\n",
    "The noise in this case is the random decisions of the investors participating i n the market at that time\n",
    "\n",
    "Should't be computer be more smarter than that you may ask?\n",
    "We explained the training data well so our output were close to the target. the loss function was low and the training process was like a charm in mathematical terms.\n",
    "\n",
    "However, once we go out of the training set and meet the real life situation, we see our model is actually bad.\n",
    "\n",
    "\n",
    "First role of programming states that the computer is never wrong, it is us who made the mistake.\n",
    "\n",
    "We must keep issues like overfitting in mind and take care of them with the appropriate remedies.\n",
    "\n",
    "As  whole , overfitting can be quite tricky, after seeing the graph you may believe you can spot an over fitting problem, Remember that in the forest example , there were 50 indicators which means you need a 51 dimentional graph.\n",
    "\n",
    "> An over-fitted model\n",
    "\n",
    "i. Captures all the noise , thus 'missed the point'\n",
    "\n",
    "11. Low loss\n",
    "\n",
    "iii. Low accuracy\n",
    "\n",
    "+ Underfitting :the model has not captured the underlying logic of the data.\n",
    "It doesn't know what to do and therefore provides an answer that is far from correct.\n",
    "\n",
    "> They are clumsy and have high class in terms of high loss functions( targets, or points ,are far away from the predictions, y, or the orange line)\n",
    "\n",
    "> Low accurracy:\n",
    "The accuracy is low, you quickly realize that either there are no relationship to be found or you need a more complex model.\n",
    "\n",
    "+ Underfitting\n",
    "\n",
    "i. Doesn't capture any logic\n",
    "\n",
    "ii. High loss\n",
    "\n",
    "iii. Low accuracy\n",
    "\n",
    "\n",
    "+ A good Model\n",
    "\n",
    "i. captures the underlying logic of the dataset.\n",
    "\n",
    "ii. Low loss\n",
    "\n",
    "iii. High accuraccy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa829e7-ca0f-4e27-a054-df114765826f",
   "metadata": {},
   "source": [
    "#### Underfitting and overfitting. A classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f446e-f277-426a-b011-3dc8ccdf3830",
   "metadata": {},
   "source": [
    "The two main type of Supervised learning are Regression and Classification.\n",
    "\n",
    "We will be using 2 categories:\n",
    "\n",
    "i . Cats \n",
    "\n",
    "ii. Dogs\n",
    "\n",
    "A good model explaining all of these is a quadratic function with few errors.\n",
    "\n",
    "Following the logic we saw in our previous example , what will be an underfitted model?\n",
    "\n",
    "A linear model . Linear models are not very smart. a simple linear model will underfit if the data is not transformed.\n",
    "Around 60% of the data will be classified incorrectly with an underfitted model.\n",
    "\n",
    "Now,an over fitted model will classify the observation perfectly. it will correctly identify all the cats and dogs photos in the dataset but once we give it a different data set following thesame quadratic function logic , it will perform poorly.\n",
    "\n",
    "A good model is somewhere between an overfiting and underfitting model. This fine balance is often called the bias variance trade off.\n",
    "\n",
    "Check out on memes page: Machine learning for convolutional Teens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c618bb00-39a7-4b76-85e1-122c145f87b2",
   "metadata": {},
   "source": [
    "You cant talk about overfitting without talking  about Bias and variance . Bias is a prediction error.\n",
    "\n",
    "The difference btw predicted and actual value.\n",
    "\n",
    "Variance , if the training model performs well with training but not with testing data set, then variance has occured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da38e674-adf7-4547-8e8e-f53df81ff1fa",
   "metadata": {},
   "source": [
    "\n",
    "+ Overfitting  : it causes low performance. it leads to poor performance.when the variance is high , your model is over fitted.Durring training the error is low, but you get high error during testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8621c0b-b6d5-447a-8bdd-3e1fdc14d13c",
   "metadata": {},
   "source": [
    "+ Under fitting  : yoor bias is high, test error is also high.\n",
    "use training , bias , variance to explain  underfitting\n",
    "the optimum is training and testing error is low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ca4818-8667-4b58-8b30-0eeb815eb79c",
   "metadata": {},
   "source": [
    "+ How to stop ovrfitting\n",
    "\n",
    "Train with plenty data\n",
    "\n",
    "Feature selection\n",
    "\n",
    "Stratified k-fold\n",
    "\n",
    "Regularizatiion of \n",
    "data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53318ce0-1faf-459e-b93b-7d82da35a760",
   "metadata": {},
   "source": [
    "#### Train and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40cc3c-4f1d-4af7-8d70-7e6cf4156ade",
   "metadata": {},
   "source": [
    "+ Over fitting is the real enemy when it comes to machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef79b06-8ede-44af-87ff-45596ac61796",
   "metadata": {},
   "source": [
    "\n",
    "To solve over fiting problem You need to identify it first .\n",
    "Overfittinhg is the real enemy in ml\n",
    "\n",
    "We will be able to spot ovefitting by dividing our available  dataset into three subsets\n",
    "\n",
    "i. Training \n",
    "\n",
    "ii. Validation \n",
    "\n",
    "Iii. Test\n",
    "\n",
    "Training  dataset :\n",
    "It helps us train the model to its final form.\n",
    "That's the place we perform everything we have seen until now.Nothing is new here ,since so far we thought all data is training data but we intetionally labelled the python variables , instead of data in the exercises.\n",
    "\n",
    "training_data = np.load('TF-Intro.npz')\n",
    "\n",
    "\n",
    "+ The validation dataset is the one that will help us to detect and present overfitting.\n",
    "\n",
    "All the  training is done on the training set, in otherwords ,we update the weight for the training set only. Every once, in a while, we stop training for a bit .At this point the model is somewhat trained. What we do next is take the model and apply it to the validation dataset.\n",
    "\n",
    "This time we just run it without updating the weight so we only propagate forward and not backwords. In otherwords , we just calculate its loss function. On average, the loss function calculated for the validation set should be thesame as the one of the training set.This is logical as the training and validation set were extracted from the same initial dataset containing thesame perceived dependencies. Normally we will perform this operations many times in the process of creating a good machine learning algorithm.\n",
    "\n",
    "The two loss functions we calculated are referred to as training loss and validation loss.\n",
    "And because the data in the training set is trained using the gradient descent, each subsequent los will be lower or equal to the previous one. Tha's how gradient descent by definition. So we are sure the train_loss is minimized.Tha is where the validation loss comes into play. At some point, the validation loss can start increasing.That is a red flag.\" WE are Overfitting\" We are fetting better at predicting the training set but we are moving away from the overall logic data. At this point we should stop training the model.\n",
    "\n",
    "> ILLUSTRATION: \n",
    "Using same example as we did in the last lesson,\n",
    "We start from an underfitting position, by increasing the complexity of the model, we reach a very good model. The traing coast is going down and the validation cost is moving accordingly . At some point though , we start Overfitting. at this point, the trainging loss is still decreasing while the validation loss is increasing. 'This is when we should stop\"\n",
    "\n",
    "\n",
    "+ It is extremely  important that the model is not trained on validation sample. This will eliminate the whole purpose of the above mentioned process. The training set and the validation set should be seperate without overlapping each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135b969-f8ae-4cc6-9b36-bb2b5eb80ff1",
   "metadata": {},
   "source": [
    "#### Train vs validation vs test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c31d17c-508e-4288-9eae-637eac975985",
   "metadata": {},
   "source": [
    "> Machine Learning Test.\n",
    "\n",
    "In the previous lesson we introduced the validation dataset, in addition, we divided the initial dataset into 3 parts.\n",
    "\n",
    "Training , validation and Test.\n",
    "\n",
    "Dataset: This is where the training happens.\n",
    "\n",
    "Validation : Helps us prevent Overfitting.\n",
    "\n",
    "Test  measures the final predictive power of the model\n",
    "\n",
    "\n",
    "After we have trained the model and validated it, it is time to measure  it's predictive power.Logically ,this is done by running the model on a new dataset it has not seen before. This is equivalent to applying the model in real life. So, our datseet is trained and validated. We now have the final version of the ML black box. we are ready to test it with the test dataset. The accuracy of the preddiction we will get from this test is the accuracy we will expect the model to have if we deploy in real life.\n",
    "So the test dataset is th elast step we take.\n",
    "\n",
    "+ In summary, \n",
    "\n",
    "> 1. you get a dataset,\n",
    "\n",
    "> 2.Then you split it into three parts, This s where you probably like to ask a question, do we split the dataset evenly How do practitioners approach this?\n",
    "There is no set rule, but splits like 80% Training , 10% Validation and 10% Test or 70%, 20% , 10% are commonly used.\n",
    "Obviously, the  dataset where we trained the model should be considerably larger than the other two. You want to devote as much data to training of the model while having enough sample to  valdate and test the model.\n",
    "\n",
    "> 3. we train the model using the training datseet and the training dataset only!\n",
    "\n",
    "> 4. Every now and then, we validate the model by running it for the validation dataset. That's where you will probably like to ask your second qeustion.\n",
    "\n",
    "'What doe every now and then mean'?\n",
    "Usually ,we validate the dataset for every epoch. Every time we adjust all weight and calculate the training loss we validate, if the training loss and the valiation loss go hand in hand, we carry on training the model. If the validation loss is increasing , we are overfitting so we should stop.\n",
    "\n",
    "if (training_loss goes hand_in hand with_validation_loss):\n",
    "print(\"nothing to see here move along\")\n",
    "elif(validation_loss is _increasing):\n",
    "print(\"Overfitting, overfitting,overfitting !!!!\")\n",
    "\n",
    "Final step, you test the model with the test dataset. The accuracy you obtain at this stage is the accuacy of our ML algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a389ee7-f025-47d7-9ad8-61aed3702fa1",
   "metadata": {},
   "source": [
    "#### N-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60948fe-d08e-49c1-a82f-93e0dd99142e",
   "metadata": {},
   "source": [
    "\n",
    "Using Training , Validation and Test is a standard mehanism and usually when ML is appropriate, we have enough dats to apply it. \n",
    "\n",
    "What if we have a small dataset? w can't afford to split it into 3 dataset as we will lose someof the underlying relationships or worse we can have so little data left for training that the algorithm may not \"learn anything\"\n",
    "There is an anwer to this issue and it called N-Fold CROSS-VALIDATION( K-fold cross validation)\n",
    "\n",
    "\n",
    "This is a strategy that resembles the general one but combines the training and validation dataset in a clever way. However, it still requirse a test subset. We are combining the training and validation set but we can't avoid the test stage.\n",
    "\n",
    "Let's say we have a dtaset that have 11,000 observations, we will save 1000 observation for the test. What we are left with are 10,000 samples . Notice that this dataset is not very big,In data Scince we often deal with Ginormous dataset. Ginormous dataset have their own problems - being so large, they often have a lot of missing values. Such a dataset is often referred to as being \"Sparse\"\n",
    "\n",
    "this introduces a whole new spectrum of issues. In any case we want o train on 9000 data points, validate on 1000 and split the remaining data into the 10 subsets containing 1000 observations eah. we folsd it 10 times \n",
    "\n",
    "\n",
    "N-fold is used in small data, you merge training and validation.but dont touch the test. So this is a 10 fold cross validation. 10 is  acommonly used value , reason we picked it for the illustration. We treat one subset as a validation set and the other 9 combined as a training set.\n",
    "\n",
    "We have 10 combinations , the first is the validation set while the remaining 9 are the training set.\n",
    "\n",
    "During the first epoch, the first chunk of data serves as validation. in the second epoch, the second chunk of data serves as validation.In this way, for each epoch we dont overlap training and validation as it should be. Moreover, we managed to use the whole dataset except for the test part. As with all good things this comes with a price. we have to train on the validation set which is not a good idea.It is less likely that the overfitting flag is raised and is possibel that we overfitted a bit.\n",
    "\n",
    "The trade off is between not having a model or having a model that is possibly overfitted a bit.\n",
    "\n",
    "N-fold cross validation solves the scales data isssue but should by no means be used as a norm. Whenever,you can divide your dataset into three parts, Training , Validation and Test. Only if it doesn't  manage to learn much because of data scarcity, you should try the N-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26050c2e-c2ff-424a-b9f3-44694e10e8fc",
   "metadata": {},
   "source": [
    "+ Cross validation is checking your  data to know how well your model performs after traioning\n",
    "\n",
    "+ checking the efficiency of your model after training by giving your model a data it has not seen before to see if it has learnt\n",
    "\n",
    "+ K-fold  and stratified cross validation are commonly used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd97ef-1348-41e0-a76d-60c3407018ef",
   "metadata": {},
   "source": [
    "#### Early stopping â€“ motivation and types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac80e80-5474-4c5d-9b09-aa598818b9ac",
   "metadata": {},
   "source": [
    "We said that a dozen times we train the model until the loss function is minimized.We can go on doing that for ever but at some point we overfit. That is we introduced the validation dataset and said a thing or two about breaking the process.\n",
    "\n",
    "We will explore additional rule that will indicate our model has been trained. \n",
    "This is called \"Early Stopping\"\n",
    "\n",
    "Generally,Early stopping is a technique  to prevent overfitting.\n",
    "It is called Early stopping  as we want to stop early before we overfit.\n",
    "\n",
    "Let's explore the most common ways to do that.\n",
    "\n",
    "+ 1. Train for a present number of epochs\n",
    "In the minimal example we trained for 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6913b748-5f70-4db6-83c7-0e926c1405b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning:\n",
    "\n",
    "#for e in range(100):\n",
    "  \n",
    "   # =' curr_loss = sess.run([optimize, mean_loss],\n",
    "                            #feed_dict = {inputs: training_data['inputs'],targetstraining_data['targets']})\n",
    "\n",
    "# print(curr_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faac610a-f575-4690-98f4-efec242a638a",
   "metadata": {},
   "source": [
    "This gives us no guarantee that the minimum have been reaches or passed. A highing of learning rate will even cause the loss to diverge to infinity.The problem was so simple that putting mistakes aside ,very few output will cause a satisfactory result. However, our ML skills have improved so much that we will  not even consider using this naive method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c0f7ef-58e3-44f4-9f78-d479267f37ee",
   "metadata": {},
   "source": [
    "pros :\n",
    "\n",
    "i. Eventually ,solves the problem\n",
    "2. we are sure the liss is minimized\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48cc026-b80f-4892-9ea8-4c56ba13bfb7",
   "metadata": {},
   "source": [
    "Cons:\n",
    "    \n",
    "i. No guarantee that the min is reached\n",
    "\n",
    "ii. Maybe doesn't minimize at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ae14c-5631-42a3-8d3e-b80fa8ba87bb",
   "metadata": {},
   "source": [
    "+ 2. Stop when updates becomes too small\n",
    "A bit more sophisticated technique is to stop when the loss function update becomes sufficiently small.\n",
    "We had a note on that when we introduced the gradient descent.\n",
    "\n",
    "A common rule of thumb is to stop when the relative decrease in the loss function becomes less than 0.001 or 0.1%\n",
    "This simple rule has two underlying ideas.\n",
    "\n",
    "First ,we are sure we will not stop brfore we have reached the minimal> This is because the way gradient descent works. It will descent until a minimum is reached. The loss function will stop changing making the update rule yieding in the same weight, in this way we will be stuck in the minimum. \n",
    "\n",
    "The second idea is that we want to save computing idea by using a few iterative power as possible.\n",
    "As we have said once we have reached the minimum or diverged to infinity,we will be stuck there, Knowing that a gazlian more epochs will not change a thing, we can just stop there.\n",
    "\n",
    "This saves us the trouble of iterating uselessly without updating anything. This is a level up from the previous method. While the preset number of epoch approach may ultimately minimize the loss, chances are we can't guess the number of required epoch. Probably the algorithms would have performed thousands of iterations that did not update the weights.\n",
    "\n",
    "Obviously, each epoch that changes nothing is useless and should be dropped,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cd54e9-1643-43f0-ac6e-219a598bec61",
   "metadata": {},
   "source": [
    "So the first technique didnt deal with any problems except for minimizing the loss . he second technique uptimized the cost and saved computing power.But both can lead to tremenduos over fitting> It is only natural that we need a more advanced technique.\n",
    "\n",
    "This is called the Validation set strategy. This is the simplest clever techniques for early stopping that prevents overfitting.\n",
    "\n",
    "\n",
    "Let's state the rule once again using the proper figure\n",
    "\n",
    "A typical training start from a set point , as time goes by the erroe becomes smaller, the distribution is exponentially as initially we are finding better weights quickly.The more we train the model the harder it gets to achieve an improvement. At some points ,it becomes almost flat.If we put the validation curve on the same graph, it will start with the training cost.\n",
    "\n",
    "At the point where we start over fitting the validation cost will start increasing. At a  point at which the two points begin diverging , that is the red flag and we should stop the algorithm before we do more damage to the model.\n",
    "\n",
    "Depending on the case, different types of early stopping can be used. The preset number of iteration method was used in the minimal example, that was not by chance, the problem was linear and supper simple,a more complicated method for early stopping wil be a strectch.\n",
    "\n",
    "The second method that monitors the relative change is simply and clever but doesn't address overfitting.\n",
    "\n",
    "\n",
    "The validation set  strategy is simple,cleverer and prevents overfitting ,however,it may take our algorithm a really long time to overfit.\n",
    "\n",
    "It is possible that the wights are balely moving and we still haven't started overfitting.\n",
    "\n",
    "That why is important to use a combination of both method.\n",
    "\n",
    "My rule will be \"Stop when the validation loss starts increasing or when the training loss becomes Small\".\n",
    "\n",
    "Why using the Validation set Strategy \n",
    "pros;\n",
    "1. we are sure the validation loss is minimized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6eb74b-6524-40bd-a596-4c97b84483db",
   "metadata": {},
   "source": [
    "### Week 13: Day 3 â€“ Initializing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f261a91-0bc4-4f6a-84a6-5593e3abe7c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3bb581-3219-4ef3-aabe-f6bcb98c614a",
   "metadata": {},
   "source": [
    "This is a crucial part of ML\n",
    "\n",
    "Initialization:\n",
    "    \n",
    "This is the process in which we set the initial values of weights.\n",
    "It was important to add to this section to the cousrs as inappropriate initailization will cause an unoptimizable model.\n",
    "When we introduced the simplest gradient descent ,we used the function 5 * X2 + 3 *  -4.\n",
    "\n",
    "We performed the gradient descent in excel, We should have that file available. We arbitrarily choose 4 as an initial value, in other words we initialize the weight with a value of 4.\n",
    "\n",
    "The next occasion on which we  stumbled upon initial weight was on our \"simple example\". We initialized them randomly in a range\n",
    "\n",
    "-0.1 to 0.1.\n",
    "\n",
    "+ \"Does it really matter what the initial weights are \"\n",
    "\n",
    "Yes it does.\n",
    "\n",
    "In the minimal example ,we said we will ned a random initial weight but we didnt elaborate  why.\n",
    "\n",
    "We will use thesame scheme we used for our back propagation model. This is a model with a single hidden layer.\n",
    "Let's initialize our weights and biases in sucha way that they are equal to a constant. It doesn't matter which constant. As you cam=n see the 3 hidden units are completely symmetrical with respect to the inputs.Each hidden unit is a function of one way coming from x1 and one from x2.\n",
    "\n",
    "If all the weights are equal, there is no reason for the algorithn to learn that h1,h2 and h3 are different. Forward propagating, there is no reason for the algorithm to think that even our outputs are different . Based on this symmetry, backpropagating all the weights are bound to be updated without distinguishinng between the diffent nodes in the net.\n",
    "\n",
    "Some optimization will soon take place, so we would want to build the initial values still the weight will remain useless.\n",
    "\n",
    "\"How are we supose to initialize the weight then\" ?\n",
    "A simple approach is to initialize weight randomly  within a small range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0781b7f-ed62-442e-87f7-953489d40b2c",
   "metadata": {},
   "source": [
    "IT is the process where we set the initial values of weight.\n",
    "\n",
    "we can initialize our weigtht until is equal to a constant.\n",
    "\n",
    "The three hidden unit are symetrical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5a9a37-2a98-46d6-9c61-20ecb927eb01",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "#### Types of simple initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa62cb6-cd75-474a-a5a0-5bb20751a767",
   "metadata": {},
   "source": [
    "+ 1. initialize using a small range in a uniform manner\n",
    "\n",
    "A simple approach is to initialize weight randomly within a small range.We did that in  the minimal example.We used the the numpy method, random uniform and our range was between -0.1 and 0.1. This approach chooses the values randomly but in a uniform manner. Each one has the exact same probability of being chosen. Equal propability of being selected sounds intuitive but is important to stress it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38cb41a-85ca-48aa-bfe8-bad7b8e5c8fa",
   "metadata": {},
   "source": [
    "+ 2. Normal initializer: \n",
    "\n",
    "Thesame idea but numbers  picked from 0 mean normal distribution the chosen variance is arbitrary but should be small as you can gues since it follows the normal distribution.Values closer to 0 are much more likely to be chosen than other values.\n",
    "\n",
    "An example of such initialization is to draw from a normal distribution of \n",
    "mean 0, std  of 0.1. Both method are some what problematic although there were the norms until 2010.It was just recently that academics came up with a solution.\n",
    "\n",
    "Let's explore the problem.\n",
    "\n",
    "Weigts are used in linear combinations. Then the linear combinations are activated.Once more we would use the sigmoid activator. The sigmoid is peculiar around its mean and its extremes. Activation functions takes as inputs the linear combination of the units from the previous layers. Well if the weights are too small, this will cause values that fall around this range. In this range unfortunately, the sigmoid is almost linear.If all  our inputs are in this range,what will happen if we use small weights? The sigmoid will not apply a non linearity\n",
    "but a linearity to the linear combination.\n",
    "\n",
    "As we discussed non-linearity are essential for deep net.Convously ,if the values are too large or too small, the sigmoid is almost flat which causes the output of the sigmoid to be only ones or only zeros respectively. A static output of the  activation minimizes the gradient while the algorithm is not really trained, so what we want is a wide range of input for the sigmoid. This input deppends on the weight, so the weight has to be initialized in a reasonable range ,so we have a nice variance along the linear combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6f542f-d74b-41cd-b1b5-e7a07fe4eb36",
   "metadata": {},
   "source": [
    "#### Xavierâ€™s initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1499f26-db50-4340-be6f-4a428dd1a169",
   "metadata": {},
   "outputs": [],
   "source": [
    " video is not opening !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea8ded-b998-4a7a-b011-68ea1c99297a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e450ee2-228c-4af6-9d79-7a810b9b0dcc",
   "metadata": {},
   "source": [
    "#### Week 13: Day 4 â€“ Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a4a59-2e54-4371-9d22-c026fbaf1eb9",
   "metadata": {},
   "source": [
    "#### SGD&Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6fa0d1-2366-448e-ad69-e754f6ed0ef0",
   "metadata": {},
   "source": [
    "Here we will be discussing about optimization , An in this context have to do with Algorithms we will use to vary our model.s parameter.So far we have seen the gradient descent only and now it is time to discus improvements that will lead to enhanced algorithms\n",
    "\n",
    "Most of what we have learnt is invaluable from a theoritical view point but slow when it comes to practical execution.However, there are simle steps to take to turn things around. \n",
    "\n",
    "We will start with the clumsiest optimizer, \"the Gradient descent\" The GD short for gradient descent iterates over  the whole training set before updating the weight. Each update is very small, that's due to the whole concept of gradient descent driven by the small value of the learning rate.\n",
    "\n",
    "We coldn't chose a value to high as this geopadises the algorithm, therefore we have many epochs  over many points using a very small learning rate. This is slow, it is not descending . it is basically snailing down the gradient .\n",
    "\n",
    "Fortunately for us there is a simple solution to the problem.It is a simple Algorithm called the SGD(sochastic gradient descent) It works in the exact same way, but instead of updating the weights once per epoch, it updates them in real time inside a single epoch. Let.s elaborate on that, SGD is closely related to the concept of batching.\n",
    "\n",
    "Batching is the process of spreading data into n-batching often called many batches. We update the weight after every batch,instead of every epoch.let.s say we have 10,000 training point. If we chose a batch size of 1,000 then we have 10 batches per epoch. So for every full iteration over the trained data set, we would update the weight 10 times instead of once.\n",
    "\n",
    "This is by no means a new method, it is thesame as the gredient descent but much faster.\n",
    "\n",
    "As all good things goes , the SGD comes at a cost,it approximates things up there so we loose a bit of accuracy but the trade off is whot it. Every one in the industry uses SGD and not Gradient Descent\n",
    "\n",
    "\"Why does this speed up the Algorithm so drastically\" ?\n",
    "\n",
    "This is  related to hard ware (CPU), splitting the training test to hardware allows the CPU cores or the GPU cores to train on different batches and parallel.This gives an incredible speed boost.This is why practitioners rely on it.\n",
    "\n",
    "Ackchyually, SGD is when you update after every input,so your batch size is 1.\n",
    "\n",
    "\n",
    "So far we have explained Mini-batch gradient descent, however, more often than not practioners refers to the mini-batch GD as SGD.\n",
    "\n",
    "The plain GD we talked about earlier in the course is called Batch GD as it has a single batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008ce02-27be-418b-8432-6a11f6b44d32",
   "metadata": {},
   "source": [
    "### Local minima pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e8bf01-7511-456d-ae26-1c8c2fbede01",
   "metadata": {},
   "source": [
    "+ Gradient descent Pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0977dfe4-f810-42d9-ac7f-fd09893d47a9",
   "metadata": {},
   "source": [
    "GD and SGD are the logical ways to train our model. We have a graph of our model , a GD will start descend from the top point and start descending. A single batch GD will be slow but eventually reach the minimum in a consistent manner.\n",
    "ASGD will move through a small number of points much faster at the end ,it is likely getting an aproximate answer rather than the exact one.\n",
    "\n",
    "With the saving intent of computational speed , it is well worth the trade off. In real life though, loss functions are not so regular. You will not be able to see the whole content of a grahp using a loss function, it shows only one of its minima, a local imposter(local minimum) rather than an extrema.  When we zoom out m we can see the global minimum point. Each local minimum is a sob optimal solution of the machine learning optimization.  Gradient descent is prone to this issue,\n",
    "\n",
    "Often it falls into the closest minimum  to the starting point rather than the global minimum. It also depends on the learning rate,  a higher learning rate may miss the first minimum and fall directly into the global valley. Howevre it is likely to isolate and never reaching. \n",
    "\n",
    "\n",
    "\" Does this means thhat the Gradient Descent optimization is not almihgty\"?\n",
    "\n",
    "Not necessarily, remedies can be applied to it to achieve the desered result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ebbd25-72df-4fb6-8344-170c7c13f518",
   "metadata": {},
   "source": [
    "\n",
    "In real life ,losss fuction is not regular\n",
    "\n",
    "GD depens on the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87f25b-cc7a-44e3-b9b0-6d8fb9bf503b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b27904-3c5b-4988-b998-600388767c3c",
   "metadata": {},
   "source": [
    "Additinalways of reaching the globall minimum rather than stuck in a local in a local one. The GD and the SGD are good way to train our models .\n",
    "We need not change them, we should simply extend them. The simplest extention we should apply is called \"Momentum\"\n",
    "\n",
    "Imagine the gradient descent as rolling a ball down the hill,the faster the ball rolls the higherr its momentum, A small deep in the grass will not stop the ball, it will rather continue rolling until it has reached the flat surface out of which it cannot go. The small deep is the local minimum while the big valley is the global minimum. If there was no mementum ,the ball will not reach the desired final destination. It would have rolled with some non increasing speed and would have stopped in the deep.\n",
    "\n",
    "The momentum accounts for the fact that the ball was actually going down the hill.In our GD framework so far, we didn't consider momentum. there is no reason to ignore it. Therefore, we created algorithms that will likely fall into a dip,if there was one ,instead of descending to the optimal solution.\n",
    "\n",
    "+ How do we add momentum to the algorithm?\n",
    "\n",
    "The rule so far is ,W equals  w minus etta times the gradient of the loss with respect to w ,including momentum. We will consider the speed with which e=we have been descending so far.\n",
    "W <- W - Î· aL / aw.\n",
    "\n",
    "For instance , if the ball is rolling fast the momentum is high otherwise the momentum is low. The best way to check how fast the ball rolled is to check how fast it rolled a moment ago.\n",
    "\n",
    "That is also the method adopted in machine learning.\n",
    "We add the previous step to the formula\n",
    "\n",
    "W <- W -Î·  aL / aw(t) -   Î± Î· aL / aw(t-1)\n",
    "   \n",
    "(curent update)           (Update a moment ago)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7ca167-2581-4b32-ba7f-c672a6f562ec",
   "metadata": {},
   "source": [
    "we want to multiply by some coefficient otherwise we will assign thesame update to the current update and the previous one. Usually we use an alpha of 0.9 is to adjust the previous update . Alpha is a hyperparameter and we can play around with it for better result. 0.9 is a conventional rule of thumb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10bc2c0-0013-407d-9216-7749e9e15ab0",
   "metadata": {},
   "source": [
    "Momentum is like rolling  a ball form a top of  a mountain.\n",
    "small deep = local minimum\n",
    "\n",
    "big valley is the global minimum\n",
    "    w <- w (t)- eta l /aw (t) - alpha eta al /aw(t-1)\n",
    "    \n",
    "alpha = 0.9 is a conventional rule of thumb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be71f1e-d987-4a23-9f50-71c102bee195",
   "metadata": {},
   "source": [
    "### Learning rate schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ca3c67-98ab-49de-81bc-8ef5621b2ac2",
   "metadata": {},
   "source": [
    "Hyperparameters are preset by us e,g \n",
    "\n",
    "+ width, Depth, learning rate(Î·), Batch size , momentum Cofficieant (Î±)  Decay Coefficient (c)\n",
    "\n",
    "+ Parameters are found by optimizing  e.g Weighs(W), Biases(b)\n",
    "\n",
    "Learning rate (Î·) is a type of hperparameter. It must be small enough so we gently descend through the loss function, instead of oscilliating while we are around the minimum and never reaching it  or diverging to infinite. it also has to be big enough so that the optimization takes place in a reasonable amount of time. In the excel file we provided for the gradient descent for one- parameter you can play around with the learning rate. Moreover ,in one excercise  coming with the minimal example ,you have the same chance but for the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60212c-9939-4f4e-b771-0b58257e1733",
   "metadata": {},
   "source": [
    "+ Learning Rate Schedules \n",
    "    \n",
    "A smart way to deal with choosing the proper learning rate is adopting the earning Rate Schedules . earning Rate Schedules gets the best of both small enough and big enough . The rational is the following:\n",
    "\n",
    "> 1. We start from a high initial learning rate .\n",
    "This lead to faster training. In this way we approah the minimum faster.\n",
    "\n",
    "> 2. At some point we lower the rate to avoid oscillation.\n",
    "\n",
    "> 3. Around the end of the training we want a very small learning rate to get a pricise answer.\n",
    "\n",
    "+ How are learning Schedules implemented in practice ?\n",
    "\n",
    "There are two ways to do that \n",
    "\n",
    "i. Setting a predetermined peace-wise constant learning rate\n",
    "For e.g we can use a learning rate of 0.1 for the first 5 epochs.\n",
    "Then 0.01 for the next 5 and 0.001 until the end.\n",
    "\n",
    "This causes the loss function to converge much faster to the sub minimum and will give us an accurate result. However, considering what we have learnt so far,this seems too simple to be the norm. This is true because \n",
    "We need to know how many epoch it will take the loss to converge, but biginners may want to use it, as it makes a great difference compared to the constant learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a043a3-059b-426f-8eb8-cfa06adef8c0",
   "metadata": {},
   "source": [
    "+ Exponential Schedule\n",
    "\n",
    "This is a much smarter approach. It is a much better alternative as it smoothly reduces or decays the learning rate. We usually start from a high value such as Î·0 = 0.1, then we update the learning rate at each epoch using the rule.\n",
    "\n",
    "\n",
    "Î· = Î·0e n/c\n",
    "\n",
    "\n",
    "In this expression, n is the current epoch, C is a constant.Here is a sequence of learning rate that will follow for C = 20.\n",
    "\n",
    "There is no rule for the constant C but usually , it should be the same order of magnitude as the number of epochs needed to minimize the loss.\n",
    "\n",
    "For e.g if we need :\n",
    "\n",
    "100 epochs, 50< C <500\n",
    "\n",
    "1000 epochs,500 < C < 5000\n",
    "\n",
    "C - 20 is ok. since we need much less in this example.\n",
    "\n",
    "However, the exact number of C doesn't matter as much.\n",
    "\n",
    "What makes a big difference is the presence of the learning schedule itself.\n",
    "\n",
    "C is also a hyperparameter as with all hyperparameters may make a difference for a particular problem. You can try differnt values of c and see if it affects the result you obtain.\n",
    "\n",
    "Take note, leaning rate schedules and momemtum comes at a price, we pay the pprice of increasing the number of hyperparameters for which we must pick values. Generally , the rule of thumb values work well but bear in mind that for some specific problem they may not. It is always worth it to explore several hyperparemeter values before sticking with one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8686d33-26b2-45ca-93c4-6f481b90838d",
   "metadata": {},
   "source": [
    "#### Learning rate schedules. A picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b3d181-38ed-410a-9ed8-72d8755624cd",
   "metadata": {},
   "source": [
    "Like they say a picture is worth a thousand words. e can see a graph as a of the number of epochs. So far we have seen the case where we used a small learning rate that reached the goal but slowly. As we said , a high learning rate will minimize the loss fast but only to a certain extent.Then it starts oscilliating and the loss stops moving.\n",
    "\n",
    "A high learning rate will not even minimize the loss.The cause would rather explode upward as seen in the graph.\n",
    "\n",
    "Finally, a well selected learnining rate such as the one that is defined by the exponential schedule would minmize the loss much faster than a low learning rate .\n",
    "\n",
    "Moreover, it will do so more accurately than a high learning rate. Naturally , we are aiming at the good learning rate ,but the problem is that we dont know what this learning rate is for our particular data and model. One way to establish a good learning rate is to plot the graph to show a few learning rate values and pick the one that looks the most like the good curve.\n",
    "\n",
    "Note, that high learning rate may not minimize the loss. A low learning rate eventually converges with a good learning rate but the process will take much longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e80ac1-57a5-47b3-bbc4-545b949d561f",
   "metadata": {},
   "source": [
    "#### Adaptive learning schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4905dfc2-0c74-4d54-a5ce-b1af5144d739",
   "metadata": {},
   "source": [
    "OInstead of using a simple rule to adjust the schedules, we can use advanced academic reseach on this topic.\n",
    "\n",
    "We will consider two types of Adptive Learning Rate:\n",
    "\n",
    "AdaGrad(Adaptive gradint algorithm , 2011)\n",
    "\n",
    "RMSPROP(Root Mean Square Propagation)\n",
    "\n",
    "They build on each and it makes it easier to understand\n",
    "\n",
    "we will see alot of formulas but yo can access the n throuugh Tensorflow by just requsting to use \"AdaGrad\"and the magic happens\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e039f-be5b-4313-915a-7ff46a2fe1d5",
   "metadata": {},
   "source": [
    "AdaGrad \n",
    "Adaptive gradint  algorithm \n",
    "was proposed in 2011. It dynamically varies the learning rate at each update and for every weight individually.\n",
    "\n",
    "The original rule was :\n",
    "\n",
    "w(t + 10) = w(t) - Î· aL / aw (t)\n",
    "\n",
    "\n",
    "w(t + 10)  -> The next weight (at time t+ 1) =\n",
    "\n",
    "w(t) -> The present weight(at time t)  - \n",
    "\n",
    "Î· aL / aw (t)  ->   follow the update rule\n",
    "\n",
    "AdaGrad is a Smat Adaptive learning rate scheduler\n",
    " see te formular below\n",
    " \n",
    " Î”wi(t) = - Î· / âˆš Gi(t) +  aL /e awi (t)\n",
    "\n",
    " Gi(t) + Gi(t-1) + ( aL / awi (t))2\n",
    "\n",
    "\n",
    "G is the adaptative magic\n",
    "\n",
    "Absolom is a small number we need to put there.\n",
    "The adaptation is per weight.\n",
    "if G = 0 will not be able to perform the division\n",
    "Gi(0) = 0\n",
    "Gi(1) = 0 + non-neg\n",
    "\n",
    "Gi(2) = [0 + non-neg] + non-neg\n",
    "\n",
    "\n",
    "\n",
    "AdaGrad is a Smart Adaptive learning rate scheduler\n",
    "Adaptive means thatthe effective learning rate  is based on the training itself.It is not the preset learning rate like the exponential one where all the eta values are calculated regardless of the training process.\n",
    "\n",
    "Another inportant point is that the adaptation is per weight, this means every individual weight in the whole network keeps track of its  G function to norminalize its own steps . It is an important observation as different weights do not reach the optimal value simultaneously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc6e44-ebb8-4fe9-89f6-5d4be823d057",
   "metadata": {},
   "source": [
    "+ RMSPROP\n",
    "(Root Mean Square Propagation)\n",
    "\n",
    "It is very similar to AdaGrad, the update rule is defined in the same way, but the G function is a bit different\n",
    "\n",
    "Î”wi(t) = - Î· / âˆš Gi(t) +  aL /e awi (t)\n",
    "\n",
    "\n",
    "Gi(t) = Î²Gi(t-1) + (1 - Î²) ( aL / awi (t))2\n",
    "\n",
    "with beginning point Gi(0) = 0\n",
    "\n",
    "The two terms are assigned weights ( Î² aand 1-Î² respectively)\n",
    "effectively keeping track of the moving average of the G values \n",
    "\n",
    "Î² - a new hyperparameter is a number between 0 and 1\n",
    "The value 0.9 is very typical\n",
    "\n",
    "We had a similar situation with the alpha in the momentum section. The great implication here is that the function is no longer monitonuosly increasing \n",
    ", hence  eta divided by the squareroot of G is now monotenuosly decreasing. Emperical evidence shows that in this way they will adapt much more efficiently.\n",
    "\n",
    "Both methos are very logical and smart.\n",
    "\n",
    "However ,there is a third method based on this two which is superior . It is called \" Adaptive moment estimation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfafc86d-c238-46d5-a697-1fde44473da1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adaptive moment estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0973c297-f7dd-4f13-a5d1-06c902685768",
   "metadata": {},
   "source": [
    "This combines the learning rate schedule and momentum to get a  satte of the art algorithm called Adam(Adaptive moment estimation).\n",
    "\n",
    "It is the most advanced optimizer applied in practice.\n",
    "It is also new as it was proposed on the 22nd of december ,2014.\n",
    "\n",
    "Started diffeerently, if someone you know studied ML in 2014 or 2015 ,he probably did not see this method. This also reinforces the idea that ML preparation does not stop after a course you take. The trends are ever changing and you should stay informed and up to date .\n",
    "\n",
    "If you obser the  AdaGrad and the RMSprop did not include momentum. Adam steps on RMSprop and introduces momentum into the equation. So the updat rule derived from RMSprop changes from \n",
    "\n",
    "Î”wi(t) = - Î· / âˆš Gi(t) + aL /e awi (t)\n",
    "\n",
    "to \n",
    "\n",
    "Î”wi(t) = - Î· / âˆš Gi(t) + E Mi(t)\n",
    "\n",
    "(t) - epoch t\n",
    "m = momentum but a bit transformed here \n",
    "\n",
    "Mi(t) = Î± Mi(t - 1) + (1 - Î±) aL / awt(t)\n",
    "\n",
    "Naturally, Mi(0) = 0\n",
    "Adam is mostly used because it it a coding edge machine learning method.\n",
    "\n",
    "As with all science , data science is a long chain of academic research building on top of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0701ab9-59cc-4f5d-a204-271107d74514",
   "metadata": {},
   "source": [
    "### Week 13: Day 4 â€“ Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e327f-34d1-4043-bb96-e3f446d383e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab2f044-8b51-4de2-bebf-15e5669fc1c8",
   "metadata": {},
   "source": [
    "Preprocessing refers to any manipulation applied to the dataset before running it through the model.\n",
    "If you must work with data in an excel file ,csv etc saving it into an npz file will be a type of preprocessing.\n",
    "\n",
    "we will focus more on data transformation rather than re-ordering as before.\n",
    "\n",
    "What is the 'motivation'for Preprocessing?\n",
    "\n",
    "+ i.  compatibility with the libraries we use . for e.g Tensorflow works with Tensors and not excel sheet.In data science , you will often by given data in whatever format and you must make it compatible with the tools you use.\n",
    "\n",
    "\n",
    "+ ii. Orders of magnitude\n",
    "we may need to adjust inputs of different magnitude. for instance, if we are forest traders, if one input we are working with is the end of the day Euro/Dollar exchange rate, it will be around ~1. However.if another input is the daily trading volume.we would have values like ~100,000 higher. Obviously, the orders of magnitutde are quite different.\n",
    "\n",
    "A linear combination of numbers based on such differrent skills is problematic. In purely mathematical terms, ta value of one is neglbible regarding a value of 100,000\n",
    "(~1000,000) + (~1) = ~ _100,000\n",
    "As all the input are on an equal footing in a vector or a matrix, the algorithm is likely to ignore all values around 1, these values essentially represents Euro/Dollar exchange rate itself, so they are often more important than the volume of trading. Obviously , something needs to be done to solve this issue\n",
    "\n",
    "+ iii.  Generalization\n",
    "\n",
    "Problems that seems different can often be sold with similar model.Standardizing inputs of different problems allows us to re-use the exact same models. Some times, there are cases we may re-use already trained networks.\n",
    "Imagine that you have trained a model previously,you face a new problem ,you text your model and it works like a charm, that is not unusual in mmachine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b57fa-1c6d-4884-ac0f-3de7bca6471c",
   "metadata": {},
   "source": [
    "### Basic preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb48680-d030-4115-a3a6-44e20387dc24",
   "metadata": {},
   "source": [
    "Often ,we are not interested in an absolute value but a relative value.This is usually the case when working with star prices.If you go to google and type apple stock price, what you get is apple stock price with red or green numbers the relative change in apple stock price.\n",
    "This is an example of preprocessing we dont consider in our search.\n",
    "Relative matrix is essentially useful when we have a time series data like stock prices,forex exchange rate and so on. \n",
    "\n",
    "\n",
    "We can further transform the relative changes into logarithms\n",
    "Many statistical and mathematical methods take advantage of logarithms  as they facilitate faster computation.\n",
    "\n",
    "In Ml, log transformation are not as common but can increase the speed of learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c4062e-07b5-4cb0-b1bf-6682de86aec7",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c465f6-ac75-4c3c-b3d3-74969a25643c",
   "metadata": {},
   "source": [
    "The most common problem when working with numerical  data is about the differnce in magnitude like we mentioned in the first lesson. An easy fix to this issue is Standardization.\n",
    "Standardization is also called feature scaling or Normalization.\n",
    "\n",
    "However, Normalization. can refer to a few additional concept given within machine learning which is why we will stick with the term standardization and feature scaling .\n",
    "\n",
    "Standardization or Feature Scaling is the procees of transforming the data we are working with into a standard scale. A very common way to tackle this problem is by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "Standard Variable = X - Î¼ /Ïƒ\n",
    "\n",
    "Î¼ = Mean of the original variable\n",
    "\n",
    "Ïƒ = standard deviation of the original variable\n",
    "\n",
    "X = original variable\n",
    "\n",
    "In this way,regardless of the dataset we will always obtain a distribution with a mean of zero and  a standard deviation of 1 which can easily be proven.\n",
    "\n",
    "Using an FX example,our algorithm have 2 input variables\n",
    "Euro/dollar excange rate and the Daily trading volume. We have 3 days work of observation.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d51f0fb-044f-4b51-8933-e8e7756350bf",
   "metadata": {},
   "source": [
    "  Day   Euro/dollar excange rate        Daily trading volume\n",
    "  \n",
    "  1        1.3        (1.3)             -0.25      (110,000)\n",
    "  \n",
    "  2        1.34       (1.34)            -0.85       ( 98,700)\n",
    "  \n",
    "  3         1.25       (1.25)            1.1       (135,000)\n",
    "  \n",
    "           mean = 1.3\n",
    "            \n",
    "           std = 0.045"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbc6935-1658-4b0f-9edb-1357128d387a",
   "metadata": {},
   "source": [
    "Let's standardize these figures ,  if we use the fomula,  X - Î¼ /Ïƒ the values will change to (1.3), (1.34), (1.25), when we standardize daily trading volume with same formula, we will obtain (-0.25), (-0.85), (1.1). In this way ,we have focused figures of differnt scales to appear similar, that is why another name for standardization is called Feature Scaling. This will ensuer our lnear combination  treats the two variables equally , also it is much easier to make sense of the data.The transformation of trading volumes allowed us to trnasform the volumes from (110,000), ( 98,700), (135,000) to \n",
    "-0.25, -0.85,   1.1  , In this way the 3rd day is considerably higher than the average while the 1st one is around the average. We can confiderable say that 135,000 trades per day is a high figure while 98,700 is low.\n",
    " ( Note, we only used 3 observation as an example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e705df-92f0-4254-b2e5-51b12c0e6e7d",
   "metadata": {},
   "source": [
    "Besides Standardization, there are other popular methods too,\n",
    "\n",
    "Initially we said that normalization refers to several concepts.\n",
    "One of them which often comes up in machine learning often consists of converting each sample into a unit line vector using the L1 or L2 -norm\n",
    "\n",
    "Another preprocessing method is PCA  which stands for principal component analysis. It is a Dimension reduction technique often used  when working with /  combine several variables into a bigger(latent) variable.\n",
    "  \n",
    "For instance, if we have data about one,s religion,voting history, particiption in association and upbringing , we can combine this four to reer to his or her attitude towards immigration . \n",
    "This new variable will normally be standardized with a mean of 0 and standard deviation of 1.\n",
    "\n",
    "Whitening\n",
    "  \n",
    "Whitening  is another technique frequently used for  preprocessing. It is often performed after PCA and removes most of the underlying  correlations between data points(uncorrelated \"attitude towards immigration\" )\n",
    "\n",
    "Whitening can be useful when conceptually the data should be uncorrelated but that is not reflected in the observations\n",
    " \n",
    " It is worthy of note that each strategy is problem specific.\n",
    " Standardization is the most common one and we will be using it in the practical examples for this course.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff67dc75-4ab8-446c-ad2f-058f19199f89",
   "metadata": {},
   "source": [
    "#### Dealing with categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7346a4-b4a8-48c9-a30d-e7ddb88b77ea",
   "metadata": {},
   "source": [
    "Categorical data refers to groups or categories such as non- numerical data such as our cat/dog examples.But the machine learning algorithm takes only numbers as values, Therefore,the question when working with categorical data is ,how to convert a cat category in to a number so that we can input it into a model or output it in the end.Obviously , a different number should be associated with each category or a Tensor.\n",
    "\n",
    "Imagine our shop has 3 products, bread, yoghurt and muffins. How do we convert this categories to numbers.\n",
    "\n",
    "A possible solution wil be to enumerate them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9d3d8-1696-4b51-b82a-c01041d9938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bread            Yoghurt                  Muffins\n",
    "\n",
    "  1                2                        3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285a27da-5fdc-4d98-b92c-8633cb053fb9",
   "metadata": {},
   "source": [
    "Unfortunately , this have a different implication as it is saying that bread < yoghurt < muffins.ie muffins is more than youghurt and youghurt is more than bread.\n",
    "\n",
    "Think about prices,if we have instead  $1,$2 , $3 , it means that muffins is more expensive than  youghurt and youghurt than bread which is not the concept behind it .This two illustrastion will assume that data has some order why it hasn't.\n",
    "\n",
    "Typically , that is an issue when our data is divided into categories. Think about the product in the shop and about differnt car brands or about people.Our quetion becomes how to encode such categories in a way which will be useful for ML Algorithm. Two main ways are adopted : One-hot encoding and Binary encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3601840-b821-4e6a-8fdc-cb94f8d70e9b",
   "metadata": {},
   "source": [
    "#### One hot vs binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5df59a9-8bdd-4b4a-92b2-4caf27e1dcc1",
   "metadata": {},
   "source": [
    "+ Binary Encoding : \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c35072-e43e-42c3-bd1d-9a2f8735248e",
   "metadata": {},
   "source": [
    "We use binary when dealing with many variable. Binary encoding means turning the values into two variables.\n",
    "It proves problematic, but we need to introduce some other ways of solving the problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "287943e0-8f29-400d-b89a-dee30aeb6e6b",
   "metadata": {},
   "source": [
    "                    Ordinal           var 1      var 2                   \n",
    "                        \n",
    "\n",
    "Bread                  1                 0          1\n",
    "\n",
    "\n",
    "Yoghurt               2                  1           0\n",
    "\n",
    "\n",
    "Muffins               3                  1            1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070342e2-7576-457d-acb7-48ea03f39cd2",
   "metadata": {},
   "source": [
    "Binary encoding implies we shoul turn this numbers into binary\n",
    "1 in binary is 0 1 so bread will be o 1, 2 in binary will be\n",
    "1 0 so youghurt will be 1 0, 3 in binary will be 1 1 , so muffin will be 1 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e2b524-4ae8-4557-b6f9-dc60ab3ffc3b",
   "metadata": {},
   "source": [
    "The next step of the process is to divide this into different columns as if we are creating two differnt variable ( var 1 and var 2)\n",
    "For the first one red is 0, youghurt is 1 and muffins are 1,, for the second variable red is 1, youghurt is 0 and muffins are 1. We have differentiated between the 3 categories and have removed the order.\n",
    "By splitting the numbers in two variables we have removed the order  but there are still some implied correlation between them ."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc59c212-2684-4985-afad-c9560a02f6f0",
   "metadata": {},
   "source": [
    "                         var 1      var 2                   \n",
    "                        \n",
    "\n",
    "Bread                       0          1\n",
    "\n",
    "\n",
    "Yoghurt                     1         0\n",
    "\n",
    "\n",
    "Muffins                      1        1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f299266d-1fe6-49fa-b0d9-a4660827eee5",
   "metadata": {},
   "source": [
    "For instance,Bread and Yoghurt seem exactly the opposite of each other. Is like we are saying whatever is bread is not youghurt and vice versa.\n",
    "\n",
    "Even if this makes sense if we encode them in a different way, this opposite correlation will be true for muffins and youghurt but nolonger for bread. \" whatever is muffins is not yogurt and vice versa.\n",
    "\n",
    "Therefore, binary encoding proves problematic but is a great improvement regarding the initial ordinary method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dda01e0-9ef9-4ae1-93ff-2151654ea808",
   "metadata": {},
   "source": [
    "+ One Hot Encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303534b-ccd4-43f6-89b9-a2dc5dc32153",
   "metadata": {},
   "source": [
    "One-hot Encoding is very simple and widely adopted. It consists of creating as many columns as there are possble values. Here, we have 3 products.We need 3 columns or 3 variables. Let's call them bread ,yogurt and muffins. Imagine this product as asking the question, is this product bread , is this product yogurt and is this product muffins.\n",
    "\n",
    "1 means yes , 0 means No.\n",
    "\n",
    "For the product that is bread ,we will have 1,0, 0\n",
    "\n",
    "For a product that is yogurtmwe will have 0, 1, 0\n",
    "\n",
    "For a product that is muffins we will have 0, 0 , 1\n",
    "\n",
    "\n",
    "This is very intuitive as the product can only be of one type at thesame time.Thus , there will be only one  value '1' and everything else will be 0.\n",
    "\n",
    "This means the products are uncorrelated and unequivocal, which is uswful and usually works like a charm."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c13701bd-45ca-44e7-af54-875dc02e295f",
   "metadata": {},
   "source": [
    "             Bread            Yoghurt                  Muffins\n",
    "\n",
    "Bread          1                 0                       0  \n",
    "\n",
    "Yoghurt         0                 1                       0\n",
    "\n",
    "Muffins         0                0                        1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e7a76-9d7e-40b5-ba2b-5e50e144074e",
   "metadata": {},
   "source": [
    "In previous lessons, we were talking about cats, dogs and horses classification.The target vectors there are one-hot encoded, so we have thesame type of vectors.\n",
    "\n",
    "One hot encoding have one big challenge,It requires alot of new variables. for example, IKEA offers around 12,000 products, do we want to include 12,000colums in our inputs? Definitely not, if we use binary, the 12,000 columns will be represented by 16 columns only.Since .the 12,000 columns will be written as shown below in binary,\n",
    "\n",
    "This is exponentally lower than the 12,000 columns we will need for one-hot encoding. Insuch cases we must use binary,even though that will introduce some unjustified correlation between the product.\n",
    "\n",
    "There is a tradeoff between one-hot and binary, we will prefer one-hot when we have \n",
    "Few categories and binary when dealing with many categories."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a29d934-81ba-4893-a381-e4133af5768c",
   "metadata": {},
   "source": [
    "                       ONE-HOT         BINARY\n",
    "                       \n",
    " New Columns          12,000              16\n",
    " \n",
    " \n",
    " 12,000 columns in binary : 10111011100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41bd29e-360a-4212-ae73-7f87cd613a0d",
   "metadata": {},
   "source": [
    "#### Week 13: Day 4 â€“ Deeper Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3486c36-32d9-4601-81f0-8af26eccc4c0",
   "metadata": {},
   "source": [
    "#### MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc579e3d-9f2f-44c8-9466-b4c160530cb4",
   "metadata": {},
   "source": [
    "The problem we have chosen is the \"Hello World !\" of machine learning as often this is one of the first problems students encounter.\n",
    "\n",
    "It is called MNIST classification.\n",
    "The M\n",
    "\n",
    "the MNIST dtaset consists around 70,000 images of  handwritten digits.\n",
    "Since we have 10 digits ,there are 10 cloasses from\n",
    "0-9\n",
    "\n",
    "Our objective is to build an algorithm that takes as input an image and correctly determines which number is showing in that image.\n",
    "\n",
    "There are few reasons we started with this algorithms. Initially ,we wanted to create an example not used before but then we though there is a reason MNIST does the \"Hello World !\" of ML.\n",
    "\n",
    "Reasons MNIST does the \"Hello World !\"\n",
    "    \n",
    "+ i. It is a visual problem.\n",
    "\n",
    "You can see the data and you know what to expect amd this makes the problem easy to define and understand.\n",
    "\n",
    "\n",
    "+ ii.  Extremely common\n",
    "\n",
    "It has been tested by almost everyone who have ever touched the deep learning algorithm . we want our students to know what people are talking about when they say , 'you now when i first did the MNIST ,i though there is a math\n",
    "\n",
    "+ iii. Easy to build up to CNN\n",
    "\n",
    "It is easy for you to build up to the convolutional Neural Network from the MNIST example.\n",
    "\n",
    "+ iv. Very big and preprocessd\n",
    "\n",
    "The dataset is sufficiently large and clean. Clean means there are no misssing value,wrong labels ,much pictures etc.\n",
    "\n",
    "The dataset was developed by \n",
    "Yan LeCun\n",
    "\n",
    "Corinna Cortes\n",
    "\n",
    "Christopher Burges\n",
    "\n",
    "You can find more about it on yan lecun's website @\n",
    "yann.lecun.com\n",
    "\n",
    "Yan is one of the founding fathers of CNNs and modern image recognition.\n",
    "\n",
    "He is the director of AI resarch at facebook.\n",
    "Doing ML at facebook involves alot of work and images. He is not only a pioneer but a leader in AI reseach and implementation ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114ba59-0c67-4c2f-b8ff-3cd63c14a0dd",
   "metadata": {},
   "source": [
    "#### MNIST â€“ Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945931f0-eca1-4fcb-8b2b-6cee7733f125",
   "metadata": {},
   "source": [
    "Assignment\n",
    "\n",
    "*** You will find this text in the 'TensorFlow_MNIST_Exercises_All.ipynb' file that is attached ***\n",
    "\n",
    "There are several main adjustments you may try.\n",
    "Please pay attention to the time it takes for each epoch to conclude.\n",
    "Using the code from the lecture as the basis, fiddle with the hyperparameters of the algorithm.\n",
    "\n",
    "1. The *width* (the hidden layer size) of the algorithm. Try a hidden layer size of 200. How does the validation accuracy of the model change? What about the time it took the algorithm to train? Can you find a hidden layer size that does better?\n",
    "2. The *depth* of the algorithm. Add another hidden layer to the algorithm. This is an extremely important exercise! How does the validation accuracy change? What about the time it took the algorithm to train? Hint: Be careful with the shapes of the weights and the biases.\n",
    "3. The *width and depth* of the algorithm. Add as many additional layers as you need to reach 5 hidden layers. Moreover, adjust the width of the algorithm as you find suitable. How does the validation accuracy change? What about the time it took the algorithm to train?\n",
    "4. Fiddle with the activation functions. Try applying sigmoid transformation to both layers. The sigmoid activation is given by the string 'sigmoid'.\n",
    "5. Fiddle with the activation functions. Try applying a ReLu to the first hidden layer and tanh to the second one. The tanh activation is given by the string 'tanh'.\n",
    "6. Adjust the batch size. Try a batch size of 10000. How does the required time change? What about the accuracy?\n",
    "7. Adjust the batch size. Try a batch size of 1. That's the SGD. How do the time and accuracy change? Is the result coherent with the theory?\n",
    "8. Adjust the learning rate. Try a value of 0.0001. Does it make a difference?\n",
    "9. Adjust the learning rate. Try a value of 0.02. Does it make a difference?\n",
    "10. Combine all the methods above and try to reach a validation accuracy of 98.5+ percent.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f16912-981e-4cbd-ab0a-322e0224d70a",
   "metadata": {},
   "source": [
    "#### How to tackle the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d4a70-ff57-4682-a613-a2c410cbfe76",
   "metadata": {},
   "source": [
    "\n",
    "Each image is 28 pixels  by 28 pixels\n",
    "\n",
    "We can think about it as an image of 28 by 28 where input values are from 0 to 255.\n",
    "\n",
    "0 - 255\n",
    "0 = black\n",
    "255 = white\n",
    "\n",
    "For example a hand wrttten sentence in a matrix will look like 7, that is an approximation, but the idea is more or less the same.Because all the images are of thesame size, a 28 X 28 photos will have 784 pixels.\n",
    "\n",
    "The approch for dip fit  nueral network is to transform or flatten each image into a vector of lenghth 784. \n",
    "\n",
    "So from Each image we will have 784 pixels(inputs)\n",
    "Each input corresponds to the intensity of the color to the corresponding pixels. We will have 784 .\n",
    "\n",
    "Each pixel is an input for our neural network \n",
    "\n",
    "Each pixel corresponds to the intensity of the color (255 is white , 0 is black)\n",
    "\n",
    "We will have 784 inputs unit on our input layer, then we will linearly combine them and add a non linearity to get the first hidden layer. For our example , we will build the model with two hidden layers. Two hidden layers are enough to produce a model with very good accuracy.\n",
    "\n",
    "Finally ,we will produce the output layer, there are 10 digits so 10 classess, therefore we will have 10 output units to the output layer.The output will then be compared to the target.\n",
    "\n",
    "We will use both one-hot encoding for both the output and the target.\n",
    "\n",
    "For example ; \n",
    "\n",
    "The digit 0 will be represented by this vector : [1,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "And digit 5 by [0,0,0,0,0,0,0,0,0,0 ]\n",
    "\n",
    "Since we would like to see the probability of the digit b labelled, we will use a softmax function for the output layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The MNIST Action plan\n",
    "\n",
    "+  1. Prepare our  data and preprocess it .We will creaate training ,validation and test datasets as well as the batch size\n",
    "\n",
    "+ 2. We must outline the model and choose the activation functions we want to enploy.\n",
    "\n",
    "+  3. We must Set the appropriate advanced optimizers and the loss function\n",
    "\n",
    "+ 4. We will make it learn . \n",
    "The algorithm will backpropagate its way to accuracy.At each epoch we will validate.\n",
    "\n",
    "+ 5 .Test the  accuracy of the model\n",
    "\n",
    "Finally we will test the accuracy of the model regarding the \n",
    "test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d0c246-7a06-4862-a512-532d1fad08ed",
   "metadata": {},
   "source": [
    "#### MNIST â€“ Importing libraries and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ea901-f784-40d1-bae9-5e6f18115993",
   "metadata": {},
   "source": [
    "##### Deep Neural Network for MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa8e08-e1a5-4b69-bd98-f3a918bc4ca4",
   "metadata": {},
   "source": [
    "Tis dataset provides 70,000 images (28 X 28 pixels) of handwritten digits (1 digit per image)\n",
    "\n",
    "The goal is to writ an algorithm that detects which digit is written . Since there are only 10 gigitss(0,1,2,3,4,5,6,7,8,9),this ia a classification problem with 10 classes.\n",
    "\n",
    "Our goal would be to build a neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1bc9b-4685-4efc-9d76-4d2dffd6eed5",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "##### Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b36c0d02-f187-4a0b-922e-5717bfd3a3af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3-TF2.0\\Lib\\site-packages\\opt_einsum\\backends\\tensorflow.py:7\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds # in order to use the tensor flow properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21343f61-3182-471d-8f6b-28fb57d3ee4e",
   "metadata": {},
   "source": [
    "#### Preprocess the data â€“ create a validation dataset and scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0615f90f-3617-4418-8740-b5319e06481e",
   "metadata": {},
   "source": [
    "Is time to extract the trained data and test data but luckily we have built in refernces that will help us achieve it.\n",
    "\n",
    "But where is the validation dataset, by default tensorflow have training and test datset but no validation dataset, but it gives us the opportunity to practice splitting dataset on our own As we can see the trained datset is bigger than the test one so we take our validation data from the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7c273-920e-4385-a947-6376edefd655",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6093ff55-1894-4bf7-9822-75478d92c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire our data and store it on the MNIST dataset \n",
    "#To lod te data we use tfds.load and the name of the \n",
    "# data we want to load (MNIST) Tensor flow datset have\n",
    "# a large number of dataset ready for modelling . \n",
    "# with this operation we can download the datset in the default directory\n",
    "# For instance,on a window system with the usual default part you will find \n",
    "# it in C users , your user name , tensorflow dataset or your home directory\n",
    "#~/tensorflow_datasets/, the fist time you execute tfds a dataset will be \n",
    "# downloaded on your computer.Therefore each consecutive time you run the code,\n",
    "# you will automaticaly load this local copy on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62652576-a307-4f39-b0b5-b5ddae44cbd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mnist_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241m.\u001b[39mload(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmnist\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfds' is not defined"
     ]
    }
   ],
   "source": [
    "mnist_dataset = tfds.load(name='mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b165166c-d1a9-471a-acfc-d4e670c3f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist_dataset = tfds.load(name='mnist', as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a34ab34-69a3-4010-b3e3-15e419b289df",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1940212409.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[18], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    mnist_dataset = tfds.load(name='mnist',with_info=True as_supervised=True)\u001b[0m\n\u001b[1;37m                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# we have successfully loaded the dataset, we can either execute this line of code \n",
    "# or continue or continue our preprocessing in thesame cell\n",
    "# Note, that the first time it will take a bit longer to execute\n",
    "# since we wiil be downloding the dataset is better we run the cell..\n",
    "mnist_dataset = tfds.load(name='mnist',with_info=True as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09da34d3-644a-4317-a0d3-c80c72dc1635",
   "metadata": {},
   "source": [
    "There are two twist we can make, we can set the arguement from supervised to true and this will load the dataset  in a two trupple structure\n",
    "input and target . in addition, then we will include one final arguement with infor = true and store it in the variable mnist info , this provids a tuple containing info about version,features,\n",
    "#number of samples of the dataset\n",
    "tdfs.loads(name,as_supervised)loads a dataset from Tensorflow datasets-> as_supervised = True, loads the data in a 2-tuple structure['input, target]\n",
    "\n",
    "We can either load the dataset or continue our preprocessing in the same cell.\n",
    "\n",
    "Note , that the first time ,it will take a bit longer to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "470699e3-4fd3-4599-82e5-efbd6761de46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (2.10.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from datasets) (0.13.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from datasets) (2023.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d080ecd7-c4d9-4b41-97ae-11f2f90adb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b37bb76c-aa6d-4e6e-9d46-89690919262f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#tfds.loads(name) - loads a data from tensorflow dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# tfds.loads(name,with_info,as_supervized) :- \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# as_supervised=True will load the dataset in a 2-tuple structure (input, target) \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# (2) with_info - True,provides a tuple containing information about version,features, #samples of the data\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#inputs between 0 and 1 helps our result to be more numerical stable\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m mnist_dataset,mnist_info \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmnist\u001b[39m\u001b[38;5;124m'\u001b[39m,with_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, as_supervised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m mnist_train,mnist_test \u001b[38;5;241m=\u001b[39m mnist_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],mnist_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     21\u001b[0m num_validation_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m mnist_info\u001b[38;5;241m.\u001b[39msplits[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnum_examples\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfds' is not defined"
     ]
    }
   ],
   "source": [
    "#tfds.loads(name) - loads a data from tensorflow dataset\n",
    "# tfds.loads(name,with_info,as_supervized) :- \n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target) \n",
    "# alternatively, as_supervised=False, would return a dictionary\n",
    "# obviously we prefer to have our inputs and targets separated \n",
    "# (1) as supervised - True,loads the data in a 2-tuple structure(inputs,targets)\n",
    "# (2) with_info - True,provides a tuple containing information about version,features, #samples of the d\n",
    "\n",
    "#tfds.loads(name) - loads a data from tensorflow dataset\n",
    "# tfds.loads(name,with_info,as_supervized) :- \n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target) \n",
    "# alternatively, as_supervised=False, would return a dictionary\n",
    "# obviously we prefer to have our inputs and targets separated \n",
    "# (1) as supervised - True,loads the data in a 2-tuple structure(inputs,targets)\n",
    "# (2) with_info - True,provides a tuple containing information about version,features, #samples of the data\n",
    "#inputs between 0 and 1 helps our result to be more numerical stable\n",
    "mnist_dataset,mnist_info = tfds.load('mnist',with_info = True, as_supervised = True)\n",
    "\n",
    "mnist_train,mnist_test = mnist_dataset['train'],mnist_dataset['test']\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "# let's cast this number to an integer, as a float may cause an error along the way\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['train'].num_examples\n",
    "# once more, we'd prefer an integer (rather than the default float)\n",
    "num_test_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# We will scale to make the result more numerically stable\n",
    "# we will se input btw 0 and 1\n",
    "def scale(image, label):\n",
    "    # we make sure the value is a float\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)\n",
    "    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 \n",
    "    image /= 255. \n",
    "    #the dot signifies we want the result to be a float\n",
    "    # return image and original label\n",
    "    return image, label\n",
    "\n",
    "#dataset.map(*function*) allows us to apply custom transformation to a given set,it can only take an input and a label and return an input and a label\n",
    "# you can scale your data in other ways if you see fit but it must take image and label and return same\n",
    "#hence, you are simply transforming the values\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "# Preprocess the data- shuffle and batch\n",
    "# shuffle means to keep thesame information in a different order, is possible the data is shuffled in ascending order resulting in x batches of 0 targets in one batch and the next batch having only 1 as a target.\n",
    "# Hence , we shuffle to get a random spread of data\n",
    "# We shuffle in batches e.g 10,000 per batch, if we set at 20,000 it will take 20,000 at once but if bufer size = 1 no shuffle will happen, Note , if buffer size >= number_samples,shuffling will happen at once uniformly\n",
    "# If buffer size of btw 1 and total sample size we will be optimizing the computational power of our computer.\n",
    "# we use the already available shuffle method and specify buffer size\n",
    "# if buffer size = 1 ,no shuffling will happen\n",
    "#  we use it whe we are dealng with enormous dataset\n",
    "BUFFER_SIZE = 10000  \n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "#After scaling and shuffling data we proceed to extract the train and validation dataset\n",
    "# we use 10% ot train data ,to validate, and we have already stored it in non validation samples\n",
    "# use the method take to extract the many samples\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "# We will use mini -batch as the trade off between accuracy and speed is optimal\n",
    "# we must set a batch size. batch size = 1 = stochastic gradient descent(SGD)\n",
    "#batch size = # samples =  (single batch)CD ie gradient descent we have seen up till now\n",
    "# We want the number to be small to the dataset but reasonably high so as to allow us to preserve the underlying dependencies\n",
    "# lets set the batch size - 100, a hyper parameter to play with when you fine tune the algorithm\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "# baching helps to reduce noise in our dataset\n",
    "# There is  a method batch, dataset.batch(batch_size) a method that allows us to combine the elements of a dataset into batches\n",
    "# we can also take advantage of the occasion to batch the train data\n",
    "# this would be very helpful when we train, as we would be able to iterate over the different batches\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "# When we validate or test we forward propagate once but when batching we find the average loss and avg.accuracy, \n",
    "#During validation we want the exact values, so we take all the dats at once , when we forward propagate we dont use much computation power , so is not expensive to calculate, but the model expects our validation set in batch form too.\n",
    "\n",
    "#validation_data = validation_data.batch(num_validation_samples) \n",
    "\n",
    "#Single batch, having size equal to number of validation samples\n",
    "# We overrite validation data with the code below because of the reason above\n",
    "# WE will have a single batch with the batch size = to the total no of validation samples or num validation samples\n",
    "# it shows that our model will take the whole value at once when it utilizes it\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "# To handle our test data we dont need to batch it either, we use same approach used in the validation data\n",
    "#test_dataset = test_dataset.batch(num_test_samples)\n",
    "test_dataset = test_dataset.batch(num_test_samples)\n",
    "# Our validation data must have same shape and object property as the train and test data, the mnist data is iterable and in in 2- turple performace as we set the arguement as supervised to true, we must extractand convert the validation input and target well\n",
    "# lets store them in validatin input and target,we store then in iter, whcich is the pythonic syntax for making the element an iterator by defauilt, it makes the dataset iterable but not lowr than the datam then load the next batch, but since we have just one batch , it will load the input and the target\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2532a835-4616-4e53-8cee-3c79d918e6e8",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e6ef5-4d05-4520-b27e-2d7f7dc28eb3",
   "metadata": {},
   "source": [
    "####  Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e895ee23-d6f8-43ec-9559-61da98c2c031",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 35\u001b[0m\n\u001b[0;32m     13\u001b[0m hidden_layer_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# We define the actual model and store it in a variable called model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# The first layer is he input layer,our data is such that each observation is 28 X 28 X 1 or a tensor of rank 3\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Since we dont know CNNs , we need to flaten the images into a vector\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Therefore, we must sort for the softmax\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Our model has been built\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m     36\u001b[0m                             tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mFlatten(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m1\u001b[39m)),\n\u001b[0;32m     37\u001b[0m                             tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(hidden_layer_size, activation \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     38\u001b[0m                             tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(hidden_layer_size, activation \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     39\u001b[0m                             tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(output_size, activation \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)   \n\u001b[0;32m     40\u001b[0m                             ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# We have 784 inputs , and this will be our input layer\n",
    "# We have 10 output nodes one for each digit\n",
    "# We will work with 2 hidden layers consisting of 50 nodes each\n",
    "# Note, with and depth are hyperparameters\n",
    "# We dont know the optimal width and dept but we know what we choose is sub optimal\n",
    "# Assignment ,fine tune the hyperparameters of our model and obtain an improved result\n",
    "# We declare 3 variables for inputs, outputs and hidden layers\n",
    "# Underlying assumption is thal all hidden layers are same size, we can also create hidden layers\n",
    "# with diff. width and see if it works better for you.\n",
    "\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "\n",
    "# We define the actual model and store it in a variable called model\n",
    "# The first layer is he input layer,our data is such that each observation is 28 X 28 X 1 or a tensor of rank 3\n",
    "# Since we dont know CNNs , we need to flaten the images into a vector\n",
    "# This  a common operation in deep learning, we us e a method called 'flatten' which is  apart of the layers model\n",
    "# it takes it arguement into the object we want to flatten\n",
    "#tf.keras.layers.Flatten(original shape) transforms(flattens) a tensor into a vector\n",
    "# indicate the input shape we want to flaten (28,28,1)\n",
    "# next step build neural network in a very similar way to our tensor flow intro model , using Dense\n",
    "# Dense is used to find the dot product of input and weight,adding the bias\n",
    "# tf.keras.layers.Dense(output size) takes the inputs, provided to the model and calculates the dot product of the \n",
    "# inputs and the weights and adds the bias. This is also where we can apply an activation function.\n",
    "# tf.dense takes the mathematical operation of the first hidden layer and we can include activation function as a second arguement\n",
    "# We will use Relu cos it works very well for this problem, In practice, each neural network have a differnt combination of activation function\n",
    "# Find out if it matters for Mnist ?\n",
    "# We can create the 2nd hidden layer in thesame way\n",
    "#outlining the model is easy and you can stack as many layers as possible using this structure\n",
    "# Finaly , we use the denss function to cretae the output layer but this time we specify the output layer size and not the hidden layer size\n",
    "# For the activation bcos we are creating a classifier, the activation function of the output layer must transform the values in the probabilities\n",
    "# Therefore, we must sort for the softmax\n",
    "# Our model has been built\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation ='softmax')   \n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6ca13-d53c-454e-bf3e-2a3e79f9bf49",
   "metadata": {},
   "source": [
    "Iter() creates an object which can be iterated one element at a time (e.g, m a for loop or while loop)\n",
    "\n",
    "Next() loads the next element of an iterable object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99e983-3184-4d09-80e0-d0548ff0c437",
   "metadata": {},
   "source": [
    "#### Select the loss and the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f22fe8-2c12-4dc5-8ce8-cb3498731b32",
   "metadata": {},
   "source": [
    "We have taken care of the data and the model , the next step is to choose the optimizer and the loss through the compile method we call on the model object.\n",
    "\n",
    "model.compile(optimizer, loss) configures the model for training\n",
    "\n",
    "We start by specifying the optimizer and one of the best choices we have is the adaptive moment estimation or ADAMS .\n",
    "\n",
    "Tensorflow allows us to use a string to find the optimizer,\n",
    "To select the ADAMS optimizer we silply write adam\n",
    "Note , the stings are not case sensitive so you can capitalize the first letter or all letters if you wish.\n",
    "\n",
    "for the loss fuction, we will apply a loss used for classifiers.\n",
    "Cross entropy will be our first choice normally, but there are different types of cross entropy.\n",
    "\n",
    "In terms of float 2 ,thre a re three built-in variation of the cross entropy loss, \n",
    "The are \n",
    "+ Binary_crossentropy(...)\n",
    "+ Categorical _crossentropy(...)\n",
    "+ sparse_categorical_crossentropy(...)(...)\n",
    "\n",
    "+ Binary_crossentropy refers to the case where we have binary encoding.\n",
    "\n",
    "The two below are equilaent \n",
    "+ Categorical _crossentropy(...) =>  expects tht you have one-hot encoded the targets\n",
    "\n",
    "+ sparse_categorical_crossentropy(...)(...)   are equivalent => applies one -hot encoding\n",
    "\n",
    "but the difference is that  sparse_categorical_crossentropy applies one hot encoding to the data.\n",
    "\n",
    "Is our data one-hot encoded?\n",
    "\n",
    "That was not the pre-processing step we went through , however the output and the target layers should have matching forms.\n",
    "\n",
    "Our model and optimizer expects the output shape to match the target shape in a one -hot endoded format.\n",
    "\n",
    "This means we will opt for the sparse_categorical_crossentropy.\n",
    "\n",
    "Finally we can add a third arguement to compile.\n",
    "We can include matrix that we wish to calculate to have the training and testing processes.\n",
    "Typically, that is the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4821e980-b749-45e1-9f21-a74f3efb2cf7",
   "metadata": {},
   "source": [
    "##### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "192037b1-81b3-469d-aa57-6ee80db7a36e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad25b1-9d88-4028-bf0b-1a3dfec92494",
   "metadata": {},
   "source": [
    "#### Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41568a-8231-4973-8132-b60009aba1fa",
   "metadata": {},
   "source": [
    "+ Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd993d8-5540-4c33-9a44-67972f1784d4",
   "metadata": {},
   "source": [
    "This is the most important part of this ml process. This is where we fit the model we built and see if it actually worked perfectly.\n",
    "\n",
    "+ Create a number of epochs you want to train ,we wil set it to 5\n",
    "\n",
    "+ fit the modelusing the fit method , we then specify the dat, in this case , train data, then set the number of epochs as epochs=num_epochs.\n",
    "\n",
    "You need to parameterize it in a neat way so that you can clearly inspect the number of epochs.\n",
    "Whenever we have hyperparameters such as the buffer size,batch size, input size,output size and so on we prefer to create dedicated variables that can be easily spotted when we fine tune our deburg or code. This alone will be enough to train the model.\n",
    "\n",
    "However, we also need to validate.\n",
    "\n",
    "Since we have already prepared our validation dat, what we shoulld next is to include it as an argement with the same method equal to the validation input and validation target we created earlier.\n",
    "\n",
    "Finally , i will set verbose to 2 to make sure we will receive only the most important information for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e7943b-e79a-45a9-b8a9-5fbc070cd5d8",
   "metadata": {},
   "source": [
    "What  happens inside an Epoch\n",
    "\n",
    "+ At the beginning of each epoch, the training loss will be set to zero.\n",
    "\n",
    "+ The algorithm will iterate over a preset number of batches, all from train_data.\n",
    "Essentially the whole training set will be utilized but in batches.\n",
    "\n",
    "+ Therefore the weights and biases will be updated as many times as there are batches\n",
    "\n",
    "+ At the end of each epoch we gat a value for the loss function,indicating how the training is going.\n",
    "\n",
    "+ Moreover, we will also see a training accuracy thanks to the last arguement we added.\n",
    "\n",
    "+ At the end of the epoch, the algorithm will forward propagate the whole validation set in a single batch through the optimized model and calculate the validation accuracy.\n",
    "\n",
    "When we reach the maximum number of epoch, the training will be over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "841d6e22-1f6a-4260-aba7-2b610bc2f15f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m NUM_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mfit(train_data, epochs \u001b[38;5;241m=\u001b[39m NUM_EPOCHS, validation_data\u001b[38;5;241m=\u001b[39m(validation_inputs, validation_targets), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b921f157-b3b9-417d-80b0-5e433b83fb75",
   "metadata": {},
   "source": [
    "Explanation of the output\n",
    "\n",
    "We have several lines of output\n",
    "+ information about the number of epoch 1/5\n",
    "\n",
    "+ we have the number of batches which is 540/540 because if we have a progress bar that will fill out gradually\n",
    "\n",
    "+ The third information is the time it took the epoch to conclude which is around 5 to 6seconds per epoch.\n",
    "\n",
    "+ Next, we have the training loss, is good to consider the training loss acrosss epochs ann not to investigate it seperately. We observe that it was decreasing but it didnt change too much.\n",
    "\n",
    "Even after the first epoch, we still have 540 different weights and bias updates one for each batch.\n",
    "\n",
    "What follows is the accuracy, the accuracy shows in what % of the cases our output is equal to th target.\n",
    "Logically , it follows the trend of the loss afterall they both represent how well the output match the targets.\n",
    "\n",
    "Finally, we have the loss and the accuracy for the validation set. They are our check.\n",
    "We usually keep an eye on the validation loss(or set early stopping) to determine whether the model is overfitting.\n",
    "\n",
    "The validation accuracy on the other hand is the true accuracy of the model for the epoch.\n",
    "\n",
    "This is because the training accuracy is the average acuracy accoss batches while the validation accuracy is the whole validation set.\n",
    "\n",
    "To access the overall accuracy of our model we will look at the validation accuracy of the last epoch. It is at 97%.\n",
    "\n",
    "This is a remarkable result already.\n",
    "\n",
    "But can we do better? \n",
    "Let's try fitting a bit with the model.\n",
    "We can change many of the hyper parametersLets start from the hidden later side.\n",
    "\n",
    "Instead of 50 nodes lets use 100\n",
    "\n",
    "With this we drastically increased the accuracy of our model to 98% \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5701dea6-f5e6-465c-8806-eafa42d2e4b6",
   "metadata": {},
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e931c6-b187-41fc-9848-afe187d0e218",
   "metadata": {},
   "source": [
    "With this we drastically increased the accuracy of our model to 98% \n",
    "Note , this is the validation accuracy and we still need the test the model on the test accuracy because the final accuracy of the nodel comes from simply forward propagating the test dataset and not the validation .\n",
    "The reason is that we may have overfit.\n",
    "The accuracy we obtain at this stage is the accuracy of the algorithm.\n",
    "\n",
    "We may think we have already dealt with over fitting , but is important to note the diffence between validation and test data set.\n",
    "\n",
    "+ We rain on the training data\n",
    "\n",
    "+ We validate on the validation data\n",
    "This is how we make sure our parameters , the weight and biases - dont overfit.\n",
    "\n",
    "Once we train our first model, we fifddle with the hyperparameters.\n",
    "\n",
    "Normally , we will not change only the weight of the hidden layers ,we can also adjust the depth, the learning rate , the batch size, the activation function for each layer and so on.\n",
    "\n",
    "Each time we make a change , we run the model once more and check if the validaton accuracy improved.\n",
    "\n",
    "After 10 to 20 different combinations we may reach a model with outstanding validation accuracy. In essence, we are trying to find the best hyperparameters but what we found are not the best hyper parameters in general .\n",
    "These are the hyperparameters that fit our validation dataset best. Basically, by fine tunning them, we are overfitting the validation dataset.\n",
    "\n",
    "During the training stage , we can overfit the parameters or the weights and biases.\n",
    "\n",
    "The validation dataset is our reality check that prevents us from overfitting the parameters.\n",
    "\n",
    "After fiddling with the hyperparameters, we can overfit the validation dataset as we are considering the validation accuracy as a bench mark for how good the model is.\n",
    "\n",
    "The test dataset then is our reality check that prevents us from from overfitting the hyperparameters( width, depth, batch size, epochs etc)\n",
    "\n",
    "It is a dataset the model have never seen before.\n",
    "\n",
    "We can access the test accuracy using the method evaluate() \n",
    "\n",
    "model.evaluate () returs the loss value and metrics values for the model in 'test mode'\n",
    "\n",
    "\n",
    "We will be propagating the test data to the next, with our current model structure ,there will be two output , the loss function and te accuracy. the same ones we had in the training stage.\n",
    "\n",
    "To make it clear , let's store them in test loss and test accuracy.\n",
    "\n",
    "If we run the code nothing comes out as we still have not displayed them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1ef406d-d7d5-4821-b4f8-8b6c7ea07872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 28, 28, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_1_input'), name='flatten_1_input', description=\"created by layer 'flatten_1_input'\"), but it was called on an input with incompatible shape (28, 28, 1, 1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 28, 28, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_1_input'), name='flatten_1_input', description=\"created by layer 'flatten_1_input'\"), but it was called on an input with incompatible shape (28, 28, 1, 1).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 277, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_1\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 784, but received input with shape (28, 28)\n    \n    Call arguments received by layer \"sequential_1\" \"                 f\"(type Sequential):\n      â€¢ inputs=tf.Tensor(shape=(28, 28, 1), dtype=float32)\n      â€¢ training=False\n      â€¢ mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileiqrx6wll.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 277, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_1\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 784, but received input with shape (28, 28)\n    \n    Call arguments received by layer \"sequential_1\" \"                 f\"(type Sequential):\n      â€¢ inputs=tf.Tensor(shape=(28, 28, 1), dtype=float32)\n      â€¢ training=False\n      â€¢ mask=None\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "# if we run the code nothing will come out bcos we are yet to display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e758cb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 28, 28, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_1_input'), name='flatten_1_input', description=\"created by layer 'flatten_1_input'\"), but it was called on an input with incompatible shape (28, 28, 1, 1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 28, 28, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_1_input'), name='flatten_1_input', description=\"created by layer 'flatten_1_input'\"), but it was called on an input with incompatible shape (28, 28, 1, 1).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 277, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_1\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 784, but received input with shape (28, 28)\n    \n    Call arguments received by layer \"sequential_1\" \"                 f\"(type Sequential):\n      â€¢ inputs=tf.Tensor(shape=(28, 28, 1), dtype=float32)\n      â€¢ training=False\n      â€¢ mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileiqrx6wll.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 277, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_1\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 784, but received input with shape (28, 28)\n    \n    Call arguments received by layer \"sequential_1\" \"                 f\"(type Sequential):\n      â€¢ inputs=tf.Tensor(shape=(28, 28, 1), dtype=float32)\n      â€¢ training=False\n      â€¢ mask=None\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "# if we run the code nothing will come out bcos we are yet to display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89edad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 28, 28, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_1_input'), name='flatten_1_input', description=\"created by layer 'flatten_1_input'\"), but it was called on an input with incompatible shape (28, 28, 1, 1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 28, 28, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_1_input'), name='flatten_1_input', description=\"created by layer 'flatten_1_input'\"), but it was called on an input with incompatible shape (28, 28, 1, 1).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 277, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_1\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 784, but received input with shape (28, 28)\n    \n    Call arguments received by layer \"sequential_1\" \"                 f\"(type Sequential):\n      â€¢ inputs=tf.Tensor(shape=(28, 28, 1), dtype=float32)\n      â€¢ training=False\n      â€¢ mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileiqrx6wll.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\user\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 277, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_1\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 784, but received input with shape (28, 28)\n    \n    Call arguments received by layer \"sequential_1\" \"                 f\"(type Sequential):\n      â€¢ inputs=tf.Tensor(shape=(28, 28, 1), dtype=float32)\n      â€¢ training=False\n      â€¢ mask=None\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "# if we run the code nothing will come out bcos we are yet to display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "927b30eb-deec-4067-aa1e-d7fc61ddd77b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Lets print the result using some nice formating\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_loss: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m0:..2f},Test_accuracy: \u001b[39m\u001b[38;5;132;01m{1:.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mtest_loss\u001b[49m, test_accuracy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100.\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Lets print the result using some nice formating\n",
    "print('test_loss: {0:..2f},Test_accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf6e53-c37a-4520-81b4-560de54228f0",
   "metadata": {},
   "source": [
    "\n",
    "Our model have a final test accuracy around 97.5%.\n",
    "\n",
    "This is also the final stage of the machine learning process.\n",
    "\n",
    "After we test the model conceptually, we are no longer allowed to change it.\n",
    "\n",
    "If you start changing the datset after this point, the test data will no longer be  adatset the model have never seen .\n",
    "\n",
    "You have feed back of 97.5% accuracy with this particular configuration.\n",
    "\n",
    "The main point of the test dataset is to insunerate model deployment.\n",
    "\n",
    "If we get 50% or 60% model accuracy, we will know for sure that our model have overfit and we will fail miserably in real life.\n",
    "However, getting the value very close to the validation accuracy shows we have not overfit.\n",
    "\n",
    "Finally, the test accuracy is the accuracy we expect to observe if we deploy the model in the real world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795264cc-0240-4fa1-9e3d-6876b0d58cf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Week 14: Day 1 â€“ Business Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80e946b-f062-464e-acbb-531c3b39c517",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Exploring the dataset and identifying predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443161ac-959c-40e3-9be9-a6798bddda4d",
   "metadata": {},
   "source": [
    "\n",
    "Thi is the application of everything learnt\n",
    "so far.\n",
    "\n",
    "+ Problem Statement\n",
    "\n",
    "You are given data from an audio book app. Logically,it relates to the audio version of books only. each customer on the data base has made a purchase atleast once.\n",
    "That is the condition to be included. We want to create a machine learning algorithm based on our data that can predict if a customer will buy again from the audio book company. The main idea is that the company should extend its advertising budget targeting individuals who are unlikely to come back.\n",
    "\n",
    "If we focus our efforts on customers likely to convert again, we can obtain an improved sales and profitability figures.\n",
    "\n",
    "So our model will take several matics and will try to predict human behaviour. a side effect of the model is that the model which of the most important metrics for a customer to come back (conversion). \n",
    "Having the data and the technology to identify prospective customers creates alot of values and growth opportunities.It is one of the better application of data science.\n",
    "\n",
    "Here is our data. the csv file is included in the lecture resources. when you download it the column headers will not be included as we want no test of the data when trianing the model.\n",
    "Each row represents a person. lwt's go through the model and see why each one of them could be abused. \n",
    "First, we have customer id, id is like a name. Whether the id is 1, 2, 3 , or john1 ,john2,john3, it makes no difference. As no information is contained in the id we will skip it in our algorithm.\n",
    "\n",
    "Next,we have  book_length, the overall book_length is the sum of the length of all purchases, we also have the average book length which is basically the sum divided by the number of purchases. So if someone have bougth single audio book ,the average length and the overall length for this person will be equal.\n",
    "\n",
    "There is no need to include the number of purchases as it is contained in the two variables we just described.\n",
    "\n",
    "We then have the overall price paid and he average price paid. These variables were constructed in the same way as those of book length.  The price is in dollars , althogth it makes no difference to the algorithm.\n",
    "\n",
    "The price variable is almost always a good predictor of the earlier.\n",
    "\n",
    "The next variable is review. Review is a boolean. It hows if  acustomer left a review. This is a matrix that shows enagement on the platform.\n",
    "Our assumption is that people who leave reviews are more likely to come back again.\n",
    "\n",
    "We have  reviews_out_of_10, This is a different variable.It measures the reviews of a customer on a scale of 1 to 10( Reviews 10/10). Pay attention here as we will show you the first pre-processing trick.\n",
    "\n",
    "Logically, we will only have a value for people that left a review.By examing the table , we quickly see that that most people dont leave review as in most market places. This is bad for our dataset and bad in general. We have decided to leave the reviews posted in the platform and substitute all missing values with the average review. the average is 8.91 .\n",
    "\n",
    "for our machine learning algorithm, 8.91 = status quo, a review bigger than 8.91 will indicate above average feelings while a review lesser than 8.91 will indicate below average feelings.\n",
    "We use the word feelings because review is another variable that is an average. A customr may have bougth 2 or 3 books from the platform, the average reviews she left indicates her feelings towards the content on the median more better. The median as a whole.\n",
    "An average of 2 out of 10 indicates a person did not have a pleasant experiance on the audio books especially when the average is 8.91. It is logical, that such a customer is not likely to buy again.\n",
    "\n",
    "Next we have toal minutes listenened. This is a measure of engagement.\n",
    "\n",
    "Next, we have completion, this is the total minutes listened divided by the total length of book a person have purchased.\n",
    "Assuming people dont eally listen to books . both variables are self explanatory.\n",
    "\n",
    "The next variable is support requests, it is numerical and shows the total number of support requests a person have opened. Support is anything from forgoten password to insistance on using the platform. Once more, this is a measure of enagement.\n",
    "\n",
    "It may turn out that the more support a person is needed the more he/she gets fed up with the platform and abandons it or he/she likes it so much that by using it he stumbles on different issues unlike someone who never opens the app.\n",
    "\n",
    "Finally,we have a variable measuring the difference between the last time a person interacted with the platform and th first purcahse day. This is anothe measure of engagement. The bigger the difference the better. If a person enagaes regularly with the platform. His difference will be bigger, thus the customer is likely to convert again. If the value of this variable is zero,It shows customer has never accessed what he has bought or perhaps , he did it on the first day only and it is unlikely here she will convert again.\n",
    "These are our inputs.\n",
    "\n",
    "It is always necessary to ask how the data was gathered. This piece of information is valuable for any analysis.\n",
    "\n",
    "The day it was gathered from the audio book app as we said. It represents two years work of engagement.\n",
    "\n",
    "Now we are doing supervised learning. So we need Targets. The targets will be a boolean.\n",
    "\n",
    "1 if a person converted and 0 if he /she didnt.\n",
    "What does it mean to convert?\n",
    "We have taken an extra 6 months of data after 2 years to check if a user converted. So we took 2 years and 6 months. We have 2 years and 6 months of data .\n",
    "\n",
    "The first 2 years are contained in the datset (inputs) we already explained.\n",
    "\n",
    "The next 6 monts will show us if a person converted , in other words if he or she bought another book and if it happened , we can count that as conversion, the target will be 1 , otherwise 0.\n",
    "\n",
    "This is how we created the Targets column. 6 months sounds reasonable for us. If one buys no new books , it means they might have gone to outr competitor or dont ike the audio book way of digesting information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc5220-a03b-4fe4-a722-ae1512d10414",
   "metadata": {},
   "source": [
    "+ Task \" Create a Machine leaning algorithm that can predict if a customer will buy again. This is a cassification problem wih 2 classess. \n",
    " Wont buy\n",
    " Will buy\n",
    " \n",
    " represented by 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f827f83-0c03-4ad6-bd5e-b6fdf00309cd",
   "metadata": {},
   "source": [
    "#### Outlining the business case solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a308a69-aa52-4a62-a2be-ceba6b2997a9",
   "metadata": {},
   "source": [
    "The Business case Action Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb876e4-962d-405c-9e08-c4fe780fea6a",
   "metadata": {},
   "source": [
    "Since we are working with real life data we must pre-process it. \n",
    "\n",
    "+ 1. Pre- Processing is important for machine learning. \n",
    "In a data science team, there may be a person whose job is to prepare dataset for analysis.\n",
    "\n",
    "+ Common techniques associated with pre-processing\n",
    "\n",
    "To create a ml algorithm from scatch we need 3 important steps\n",
    "> 1.1 balance the dataset\n",
    "> 1.2 Divide the dataset into 3 parts: Training, validation and test. The essence is to prevent overfitting\n",
    "\n",
    "> 1.3 Save the data in a tensor friendly format using the .npz\n",
    "\n",
    "+ 2. Create the machine learning algorithm\n",
    "\n",
    "The code is 90% thesame as the one in the mnist example. Here is the true power of Tensorflow. We will use the same structure to create a different model which will be equally powerful. By the end you can take any dataset to repeat this operation using the code to create deep neural network \n",
    "for countless problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a8fa1-b20c-4740-aeac-f0bf59c9e869",
   "metadata": {},
   "source": [
    "#### Balancing a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48df2ded-b1b7-411b-a5f0-3e2c07db8f60",
   "metadata": {},
   "source": [
    "Before pre-procesing , it is important to balance your dataset. Let's think about a photo classification problem with two classes.\n",
    "Cats and Dogs\n",
    "What accuracy do you expect from a good model?\n",
    "\n",
    "If we correctly classify 70% of the photos, it is not too bad. 80% accuracy is good and 90% is very good for beginners.\n",
    "\n",
    "We are not talking about google or facebook which may have 99.99% accuracy.\n",
    "90% accuracy for most problems is an impressive accomplishment.\n",
    "\n",
    "Imagine a model that takes animal photos and outputs only cats . No matter what you feed to the algorithm, it will always output cat as the answer, this is a bad model.\n",
    "\n",
    "Is this machine learning you may ask?\n",
    "It is definitely not the result we want from an algorithm, but is common result.\n",
    "\n",
    "Imagine that in our algorithm,90% of the photos are cats and 10% dogs. The model will classify all photos as cats since 90% of the dtaset are cats.\n",
    "\n",
    "What is the accuracy of the algorithm?\n",
    "It is 90%\n",
    "Why do this problem arise?\n",
    "\n",
    "Since , the machine learning algorithm try to optimmize the loss,it quickly realises that if so  \n",
    "many targets are cats, the outputs shold most likely be cat to achieve a great result . Hence,it comes up with the most prediction at all times. Cats!\n",
    "\n",
    "If the distribution photos are 90% cats and 10% dogs,a model with an 80% accuracy is a bad model.\n",
    "\n",
    "This is because the Dumb model with outputs only cats will do better than it (90%)\n",
    "\n",
    "Therfore ,only a result above 90% will be the most favourable one.\n",
    "\n",
    "We referred the initial probablity of picking a photo of some class as a prior.\n",
    "\n",
    "The priors are 0.9 for cats and 0.1 for dogs.\n",
    "\n",
    "The priors are balanced when 50% of the photos are cats and 50% dogs or 0.5 and 0.5.\n",
    "\n",
    "Examples of unbalanced priors are \n",
    "0.1 and 0.9\n",
    "0.7 and 0.3\n",
    "0.6 and 0.4 and so on and each pair is prone to the issue discussed above.\n",
    "\n",
    "A ml algorithm may quickly learn that one class is much more common than the other and decide always to output the value with the gigher prior.\n",
    "\n",
    "If we have 3 classes\n",
    ", \n",
    "Cats   33%\n",
    "\n",
    "Dog    33%\n",
    "\n",
    "Horses 33%.\n",
    "\n",
    "Balancing the dataset would imply picking the dataset. Each class amount to approximately 1/3 of the dataset. If we have 4 clasess, 25% each.\n",
    "\n",
    "In our business case, by exploring the target, we quickly realised that most customers did not convert in the given time spent. We must surely balance the dataset to proceed. This is done by counting the total number of target ones and matching the same number of zeros to them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cf6960-0427-4166-bd95-144d71942132",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9043b3da-a1a3-4689-b254-0cc6238083c7",
   "metadata": {},
   "source": [
    "#### Practical example. Audiobooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a685faa3-0e5e-452e-a736-88bedb896f91",
   "metadata": {
    "tags": []
   },
   "source": [
    "+ Preprocessing the data, Balance the dataset,Create 3 datasets: training,validation and test. Save the newly created sets in a tensor friendly format(e.g.* .npz)\n",
    "\n",
    "Since we are with the real data we will need to preprocess it a bit . This is the relevant code which is not that hard but refers to data engineering more than machine learning.\n",
    "\n",
    "If you want to know how to do that, go through the code and the comments. In any case, this should do the trick for all datasets organized in the way. Many inputs and then 1 cell containing the targets(all supervized learning datasets)\n",
    "\n",
    "Note that we have removed the header row, which contains the names of the categories. We simply want the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5d25a2-2347-45df-8888-0a58f13cabf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post1.tar.gz (3.6 kB)\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.20.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=9eb9414d4fc120993f3281ea59f6d0ca3bf22b93a174ebe34c4f405cd521ff19\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\e4\\7b\\98\\b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\user\\anaconda3\\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\pip-install-g_czpw_y\\\\sklearn_576a581d3d85478a8969f640bdf4bf9f\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\pip-install-g_czpw_y\\\\sklearn_576a581d3d85478a8969f640bdf4bf9f\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-pip-egg-info-my4vfewa'\n",
      "         cwd: C:\\Users\\user\\AppData\\Local\\Temp\\pip-install-g_czpw_y\\sklearn_576a581d3d85478a8969f640bdf4bf9f\\\n",
      "    Complete output (18 lines):\n",
      "    The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "    rather than 'sklearn' for pip commands.\n",
      "    \n",
      "    Here is how to fix this error in the main use cases:\n",
      "    - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "    - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "      (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "    - if the 'sklearn' package is used by one of your dependencies,\n",
      "      it would be great if you take some time to track which package uses\n",
      "      'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "    - as a last resort, set the environment variable\n",
      "      SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "    \n",
      "    More information is available at\n",
      "    https://github.com/scikit-learn/sklearn-pypi-package\n",
      "    \n",
      "    If the previous advice does not cover your use case, feel free to report it at\n",
      "    https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
      "    ----------------------------------------\n",
      "WARNING: Discarding https://files.pythonhosted.org/packages/db/1e/af4e9cded5093a92e60d4ae7149a02c7427661b2db66c8ea4d34b17864a2/sklearn-0.0.post1.tar.gz#sha256=76b9ed1623775168657b86b5fe966d45752e5c87f528de6240c38923b94147c5 (from https://pypi.org/simple/sklearn/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92318192-5043-4d6c-844d-d2ab82e38930",
   "metadata": {},
   "source": [
    "##### Extract the data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e776d8cf-4f1c-4cc0-ab9c-6cb941838224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "# we will use the sklearn capabilities for standardizing the e inputs\n",
    "# it is one line of code which drastically inproves the accuracy of the\n",
    "# model. Almost always we standardize all input as the quality of the \n",
    "# algorithm improves significantly , without standardizing the input \n",
    "# we reach 10% less accuracy of the model.0.5% improvement can give\n",
    "# a great success to our model\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c93b4298-da89-4bcc-a4c6-e67dc759e424",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "C:\\Users\\user\\tensorflow_datasets\\downloads\\Audiobooks_data.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12848/2723127750.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw_csv_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\user\\\\tensorflow_datasets\\\\downloads\\\\Audiobooks_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0munscaled_inputs_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_csv_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtargets_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_csv_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#targets is the last column of the csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[0;32m   1063\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1065\u001b[1;33m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1066\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    529\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: C:\\Users\\user\\tensorflow_datasets\\downloads\\Audiobooks_data.csv not found."
     ]
    }
   ],
   "source": [
    "\n",
    "raw_csv_data = np.loadtxt (\"C:\\\\Users\\\\user\\\\tensorflow_datasets\\\\downloads\\\\Audiobooks_data.csv\")\n",
    "\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "targets_all = raw_csv_data[:,-1]\n",
    "#targets is the last column of the csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0873c4be-7bd6-4a34-9507-95459ad2009b",
   "metadata": {},
   "source": [
    "##### Balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3295ac17-9d39-4c80-8226-bb814090355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will count the number of targets that are 1s\n",
    "# We will keep as many 0s as there 1s, though the n0 of 0s are more\n",
    "# if we sum all targets that can take only 0 and 1 as values\n",
    "# We will get the no of targets that are 1s\n",
    "\n",
    "num_one_targets = int(np.sum(targets_all))\n",
    "# We will declare it as an integer as targets all may be a boolean\n",
    "# depending on the programming language.\n",
    "# Then keep as many 0s as 1s.\n",
    "#lets set a counter for the 0 target = 0\n",
    "zero_targets_counter = 0\n",
    "# we need a variable that records the indices to be removed\n",
    "# for now it will be empty , we want it to be a list or turple\n",
    "# so we put empty bracket\n",
    "indices_to_remove = []\n",
    "# lets iterate over the dataset and balance it\n",
    "# Targets all contains all targets,its hape on the zero axis is the length of the vector\n",
    "# It will show us the no of all targets\n",
    "# In the loop we want to increase the counter by 1 if the target at position i is 0\n",
    "# In the same if we put another if which will add an index to the variable indices to remove\n",
    "# if the 0 conter is over the number of 1s\n",
    "# We will use the append method which simply adds the element to a list\n",
    "# After the counter of 0s matches the number of 1s, we note all indices to be removed\n",
    "# After we run the code ,the variable indices to remove will contain the indices of all targets we will not need\n",
    "# Deleting these entries will balance the dataset\n",
    "\n",
    "\n",
    "for i range(targets_all.shape[0]):\n",
    "    if targets_all[i] ==0\n",
    "       zero_targets_counter > num_one_targets:\n",
    "        indices_to_remove.append(i)\n",
    "        \n",
    "# lets create a new variable unscaled inputs= np.delete nscaled inputs all\n",
    "# np.delete(array,obj to delete,axis) is a method that delete an object on axis 0 of the vector\n",
    "# targets-equal-priors = np.delete (targets all indices to remove on axis 0\n",
    "# we have a balanced dataset\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis =  0)\n",
    "targets_equal-priors = np.delete (targets_all, indices_to_remove, axis=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53167468-7eea-43a1-aad3-130765f78df1",
   "metadata": {},
   "source": [
    "##### Standardize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0460d4-450c-41c5-a86f-efd55b103445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To standardize means ti scale the inputs so as to improve the algorithm\n",
    "# A single line of code will do the work\n",
    "# The scale method standardize the dataset along each variable\n",
    "# Thats the preprocessing library we imported from sklearn\n",
    "# preprocessing.scale(X) is a method that standardizes an array along each variable\n",
    "# Basically all inputs will be standardized\n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ccf5b9-f1cd-468c-aa88-026015695986",
   "metadata": {},
   "source": [
    "##### Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3149c-1666-4e1b-bc33-2725eb3f86c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will suffle the inputs and targets\n",
    "# we keep thesame information in a different order.\n",
    "# Is possible the original datset was collected in the order of \n",
    "# date,since we will be batching we must shuffle the data.\n",
    "# It should be as randomly spread as possible so batching works fine\n",
    "# Image the data is ordered so that each batch reps a diff. day of purchase\n",
    "# inside the batch, the data is homogenuos, while between batches ,it is very heterogenuos\n",
    "# due to promotions , day of the week effect and so on.this will confuse the sochastic\n",
    "# gradient descent when we average the loss across batches,overall we want them shuffled\n",
    "# First, we take the indices of the axis 0 of the scaled input shape and place them in a variable\n",
    "# np.arrange([start],stop) is a method that returns a evenly spaced values within a given interval.\n",
    "# Then we use the np.shuffle method to shuffle them\n",
    "#np.random.shuffle(X) is a method that shuffles the numbers in a given sequence.\n",
    "# Finally, we create shuffle inputs and shuffle targets variable equal to the scaled inputs and the targets\n",
    "# Where the indices are the shuffled indices\n",
    "\n",
    "shuffled_indices = np.arrange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e96e5-4f7d-422c-a266-bf5fa51d6558",
   "metadata": {},
   "source": [
    "##### Split the dataset into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48f368-bbe7-4d86-910e-c75cb2911066",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Determine the size of the 3 dataset using the 80 /10/10 split\n",
    "# We want to make sure the no is integer.\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "# let's extract them from the big dataset, the train input is given by\n",
    "# the first train count of the preprocessed input\n",
    "# The train target are the first train sample count of the targets\n",
    "# Analogically,the validation inputs are the inputs in the interval form\n",
    "# train sample count totrain sample count + validation sample count\n",
    "# The validation targets in the same interval\n",
    "# finally the test is everything that is left\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]n_samples_count:]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:] \n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "# it is useful to check if we have balanced the dataset\n",
    "# we may have balanced the whole dataset but not the training ,validation and test dataset\n",
    "# lets print the no of 1s for each dataset\n",
    "# The total number of samples and the proportion of 1s as a part od the total\n",
    "# They should all be around 50%\n",
    "\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_sample_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_sample_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_sample_count)\n",
    "\n",
    "# from the output the training set is considerably larger than the validation and test and this is how we want it to be.\n",
    "# The total number of observation is around 45 hundred(3579 + 447 + 448 =4474 ) which is relatively good amount of data\n",
    "# data. Although we started with around 15000 samples in the csv\n",
    "# All three sets are balanced.The proportions or priors are ok as they are almost 50%. note , that 52 or 55% are still ok.\n",
    "# Howevr, we want to be as close to 50% as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1aca70-2f11-4655-8278-960ca0f054a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Save the three datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9761561-6c9a-4e0f-b0bf-6227aca07f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will save the data so that we can use them later\n",
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets= validation _targets )\n",
    "np.savez('Audiobooks_data_test',inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47209772-2630-46d0-befb-9eb49194e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each  time we run the codes , we will get different proportions as we\n",
    "# We suffle the indices randomly.So training, validation and test dataset \n",
    "# will contain different samples.\n",
    "# You can use this same code to preprocess any dataset where you have two classes\n",
    "# The code will skip the first column of the dta, as here we skip the id and the\n",
    "# last column wili be treated as target. \n",
    "# If you want to customize the code for problems with more classes, you must \n",
    "# balance the dataset classes instead of 2, everything else should be the same.\n",
    "# The preprocessing is over, henceforth,we will only work with the .npz files\n",
    "# We will save this in jupiter notebook and continue with machine learning in a seperate one.\n",
    "# Make sure you hhave the raw csv when you run the code on your computer, in this way you will \n",
    "#create the npz files which we wil use in our machine learning.\n",
    "# There are some additional adjustments you can make on the code to improve the preprocessing\n",
    "# Check out the exercises below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba65f42-c58e-45d1-9a1e-ba296e63896b",
   "metadata": {},
   "source": [
    "#### Preprocessing Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1059c33-54d4-4cb6-81a6-246884b52608",
   "metadata": {},
   "source": [
    "Assignment\n",
    "\n",
    "It makes sense to shuffle the indices prior to balancing the dataset. \n",
    "Using the code from the lesson, shuffle the indices and then balance the dataset.\n",
    "\n",
    "Note: This is more of a programming exercise rather than a machine learning one. Being able to complete it successfully will ensure you understand the preprocessing. \n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a3096-ff06-4c37-87bf-83440e1981aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load the preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d07f5-f4b4-45bb-bf84-ff14c597a0fd",
   "metadata": {},
   "source": [
    "Here,we have an input layer consisiting of 10 units, those are the inputs from our csv. Thre are only two output nodes as there are only 2 possibilities, 0 and 1.We will build the net with two hidden layers. The number of each hidden layers will be 50 but as we know very well, this is extremely easy to change, therefor for product type of aan algorithm, 50 is a good value.\n",
    "\n",
    "50 hidden units in the hidden layers provide enough complex\n",
    "ity, so we expect the algorithm to be much more sophisticated than a linear or logistic regression. \n",
    "At the same time, we dont want to put too many units initially, as we want to complete the learning as fast as possible and see if anything is been learned at all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26113c4d-3ff4-4bcc-8677-81c9cd064900",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### create the machine learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aef5a82-30f7-4412-ae32-5c4d092c2bb5",
   "metadata": {},
   "source": [
    "##### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c5d6120-a453-47a7-bc10-7f9708e1972b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3-TF2.0\\Lib\\site-packages\\opt_einsum\\backends\\tensorflow.py:7\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9421f630-d38b-4863-aa5d-d538876a9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next logical step is to load the data\n",
    "# Lets declare a temporary dataset called npz that will store each of the 3 dataset as we load \n",
    "# them. To load the trained data, we use np.load and the name of the trained dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c81676-00c1-4e2c-97ea-a81b084fe6ee",
   "metadata": {},
   "source": [
    "+ Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18adc1d-4752-4cc7-954a-7d8a405bfc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "npz = np.load(\"C:\\\\Users\\\\user\\\\tensorflow_datasets\\\\downloads\\\\Audiobooks_data.npz\")\n",
    "            \n",
    "# We expect all nputs to be float\n",
    "train_inputs = npz['inputs'].astype(np.float)\n",
    "# extract the trained target from npz using the keyword targets\n",
    "# Our targets are 0s and 1s but we are not completely dertain it \n",
    "# will be extracted as integers or booleans, is good practice to \n",
    "# use the same method as type and make sure that the data type will be np.int\n",
    "# Even if we know inwhat format we saved them\n",
    "train_targets = npz['targets'].astype(np.int)\n",
    "\n",
    "# For validation and test we start loading the next npz, audiobook data in \n",
    "# a temporary variable npz, then we proceed in a similar way to extract the\n",
    "# validation input and targets making sure of there data type.\n",
    "npz = np.load(\"C:\\\\Users\\\\user\\\\tensorflow_datasets\\\\downloads\\\\Audiobooks_data.npz\")\n",
    "validation_inputs, validation_targets =  npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "\n",
    "\n",
    "npz = np.load(\"C:\\\\Users\\\\user\\\\tensorflow_datasets\\\\downloads\\\\Audiobooks_data.npz\")\n",
    "# npz = np.load('Audiobooks_data_test.npz')\n",
    "test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targats'].astype(np.int)\n",
    "# Note that our train,validation and test data is simply an array formm instead \n",
    "# of the interators we used for the mnist. In this business example, we will \n",
    "# train our model with every day arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51810fbf-404b-4ca5-a34b-b7ab3e8e7215",
   "metadata": {},
   "source": [
    "#### Learning and interpreting the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08739796-ebf1-4dc1-8776-1f5cc1e8b8d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Model\n",
    "\n",
    "###### Outline,optimizers,loss,early stopping and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da97ac-c0cf-4bac-a137-0929ada038dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We claim that tensorflow codes is extremely re-useable, we will now prove \n",
    "# it as infact true . We will copy the mnist model outline , then adjust it,\n",
    "# First the imput size of our model must be 10 as there are 10 predictors \n",
    "# The output size must be 2 as we have 1s and 0s\n",
    "# We can leave the hidden layers the way it is cos we are not sure of the optimal \n",
    "# value is . \n",
    "# In the mnist code we used the method flatten to flatten the image to a vector\n",
    "# Here we have already preprocessed our data, so we can delete the flatten line the \n",
    "# rest remains unchanged. We have 2 hidden layers each activated by a redo activation \n",
    "# function. We know that the model is a classifier, therefore our output layer should be activated with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c0ee57-d073-462d-89d3-e9a9a1cfee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 50\n",
    "# hidden_layer_size = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            #tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation ='softmax')   \n",
    "                            ])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "model.fit(train_inputs,\n",
    "          train_targets,\n",
    "          batch_size = batch_size,\n",
    "          epochs = max_epochs,\n",
    "          validation_data=(validation_inputs, validation_targets),\n",
    "          verbose=2)\n",
    "# Next we choose the optimizer and the loss function, we copy and paste from the mnist e.g\n",
    "# The choosen optimizer is adam , while the loss is cross categorical crossentropy, we use use\n",
    "# use thisloss so as to ensure that our integer targets are one hot encoded appriopriately when \n",
    "# calculating the loss. We are happy at obtaining the accuacy for each epoch, we will now set 2 \n",
    "# of our hyperparameters the batch size and number of epoch. For batch size we will not take\n",
    "# advantage of iterable objects that contains the data instead we will enploy simple arrays, but\n",
    "# the batching itself will be indicated when we fit the model in the module 2\n",
    "# max number of epoch = 100, max epochs = 100, next we fit the model()\n",
    "# Then lets start with the trained input and the trained target, we can fit the 2 turple object \n",
    "# containing both of them as we did with the mnist or we can feed them seperately.\n",
    "# To show both approaches we already extracted the input and targets into seperate variables\n",
    "# inside the fit method we place the input 1st and then the targetm our firstarguement will be train \n",
    "# inputs, train targets and then the batch size.if you are dealing with arrays, indicating the batch size\n",
    "# will automatically batch the data during the training process, batchsize= batch size\n",
    "# next is the maximum no of epochs\n",
    "# validation data ,we have 2 arrays of interest(validation input and target)\n",
    "# We now set vebose to 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fe828-7b67-4706-a39f-f34d7dfcda5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The result we got is outstanding,after 100 epochs of training we have \n",
    "# reached we have reached a validation accuracy of around 91 to 92%\n",
    "# Why did our model train for all 100 epochs?\n",
    "# Isn't there a danger of overfitting afetr training for so long?\n",
    "# yes pricisely,if we check the training process overtime , we will\n",
    "#notice that while the training was consistably decreasing our validation\n",
    "# loss was sometimes increasing, hence, is obvious we have overfitted the model\n",
    "# When we trained the mnist we didnt set an early stopping procedure\n",
    "# and we missed the step here as well, for the mnist it was not really crucial\n",
    "# bcos the dataset was so well prepared that it will barely make a difference, \n",
    "# this time though it does.\n",
    "# we will need an early stopping mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd039f8-df8a-48c1-8bb1-6e3edd63dab1",
   "metadata": {},
   "source": [
    "#### Setting an early stopping mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10b55b-5549-4008-9cdf-e986907684d2",
   "metadata": {},
   "source": [
    "We will explore how to set up an early stopping mechanism with tensorflow.\n",
    "The fit method contains an arguement called callbacks. Callbacks are functions  called at certain points during model \n",
    "training. There are many different readily available \n",
    "callbacks. You can try your training process in tensor\n",
    "board( class Tensorboard), you can strem the result in a csv file or server(class CSVLogger), class modelcheckpoint, \n",
    "classRemoteMonitor, save the model after each epoch, adjust the learning rate (class learningRateScheduler), You can also define any custom call back you may want to use.\n",
    "\n",
    "We will be focusing on early stopping\n",
    "class Earlystoping: Stop training when a monitored quantity has stopped improving.\n",
    "This is a definition of a utility called at certain point during training . Each time the validation loss is calculated\n",
    "it compares to the validation loss one epoch at a go. if it starts increasing, the model is overfitting and we should stop training. Since the early stopping mechanism is a hyperparameter in a way we decalre a new varaible called early stopping which will as follows early stopping = tf.keras.callback.Earlystopping, there is a readily available structure we can use, Hence, we need to take care of other particulas of this early stopping mechanism.\n",
    "By default, this object will monitor the validation loss and stop the training process the first time the validation \n",
    "loss process starts increasing.\n",
    "\n",
    "Is time to implement it in our training process, as suggeste, lets add a callback arguement to our fit method equal  to a list of callbacks. In our case , it will have a single element our early stopping variable.\n",
    "Lets retrain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb4c20-3d1f-491c-84f9-38c315cd2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 50\n",
    "# hidden_layer_size = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            #tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation ='softmax')   \n",
    "                            ])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "#early_stopping = tf.keras.callbacks.EarlyStopping()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "model.fit(train_inputs,\n",
    "          train_targets,\n",
    "          batch_size = batch_size,\n",
    "          epochs = max_epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(validation_inputs, validation_targets),\n",
    "          verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb23b58-543a-4239-b408-e1b102587b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After retraining the model, we can see that the new \n",
    "# training lasts for less than 20 epochs and the final\n",
    "# accuracy of the model is around 90%. Obviosly the\n",
    "# first time we trained we had overfit and the result\n",
    "# was about 2% higher and we can attribute this solely \n",
    "# to over training it.Now if we value the validation \n",
    "# loss, we will notice that the first time it increased\n",
    "# was during the last epoch, moreover ,it increase only \n",
    "# slightly, Some times , if we notice that the validation \n",
    "# loss has increased by insignificant amount , we may prefer\n",
    "# to let one or two validation increase slide.\n",
    "# To allow for this tolerance, we can adjust the early\n",
    "# stopping object. There is an arguement called patience, \n",
    "# which by default ,is set to 0, we can specify the no \n",
    "# of epochs with no improvements after which the training will stop\n",
    "# tf.keras.callbacks.EarlyStopping(patience) configures \n",
    "# the eraly stopping mechanism of the algorithm.\n",
    "# it is a bit too strict to have no tolerance for a random \n",
    "# increase in the validation loss , thus let's set the patience to 2,\n",
    "# This way we will be completely sure if the model have started to overfit\n",
    "# With the adjustment we can re run the code.\n",
    "# Depending on your problem in dataset, the difference may not be crucial\n",
    "# However , this is yet anothe deburging tool you have at your disposal\n",
    "# This time the accuracy is between 90 to 91%, definitely worse than the overfitted\n",
    "# modelbut slightly butter than the one with no patience.\n",
    "# intepretation: the final validation accuracy of the model is around 90%, the priors \n",
    "# is around 50% , this means our ml algorithm has learnt alot. It managed to classify \n",
    "# around 90% of the customers correctly.\n",
    "# In other words , if we are given 10 customers and their audiobook activity we will \n",
    "# be ablle to correctly identify future customer behaviour of 9 of them.\n",
    "# How does this help us in practice ?\n",
    "# We can focus our marketing efforts only on those customers who are likely to convert again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d4154d-d7f9-45de-8741-c79f71880c34",
   "metadata": {},
   "source": [
    "In summary, it is extremely hard to predict human behaviour , however with the machine learning algorithm what we create here is a new tool in your aesenal that has given you an incredible edge!\n",
    "Moreover, using the algorithm is a skill you can easily apply in any business out there.\n",
    "What we did was leverage the power of artificial intelligence (AI)to reach a business insight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b09d4-1a33-41f8-b958-7fec6f0425e8",
   "metadata": {},
   "source": [
    "#### Testing the business model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a177aee-8dee-44db-ad7e-6723823ac1a4",
   "metadata": {},
   "source": [
    "To test the model we use the model evaluate\n",
    "model.evaluate() returns the loss value and metrics values for the model in test mode.\n",
    "\n",
    "Let's declare two variables, test loss and test accuracy = model evaluate of the test input and test target.\n",
    "Recall evaluate the returns the loss and any other mettics we have requested in our model outline. This was the accuracy, to make the result pritty ,we can print them with some nice \n",
    "fomatic.\n",
    "This is the final acuracy of the model.(91%)\n",
    "Natrally, it is close to the validation accuracy as we did not fiddle too much with hyperparameters.\n",
    "\n",
    "Note ,that sometimes , you can get a  test accuracy\n",
    " higher than the validation one.That is nothing but pure luck. theoritically, \" \n",
    " + The test accuracy should be equal or lower than the validation one\n",
    "+ from this point on, you are no longer allowed to change the model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800afdbe-95b7-48a8-a618-dd403b8dbbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs,test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04159fe2-5a05-4805-bae0-66be0092aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTest loss: {0:.2f}. Test acuracy: {1:.2f}%'.format(test_loss, test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff10f0-e04b-4ddf-9195-4141f6234199",
   "metadata": {},
   "source": [
    "+ Random forest are used for classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
