{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "962f8649-f5d8-4d4f-a0c1-e03845936051",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Week 15: Day 1 â€“  Text Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded22d46-7fec-457b-aafe-11c753f1b17d",
   "metadata": {},
   "source": [
    "#### Introduction to Text classification (Reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd4579e-f29a-4f67-a51c-b8f40de483af",
   "metadata": {},
   "source": [
    "#### Text Classification explained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d109da-5923-4b88-a9b9-2f2dc3c797cb",
   "metadata": {},
   "source": [
    "+ Text Classification\n",
    "\n",
    "Since the beginning of time , written text has been a means to communicate,express and document something of signifance.\n",
    "\n",
    "\n",
    "Even in the modern age, it has been proven alot of times that an an individual writing style can be a defining aspect of one's psyche. Ever sice social media emrged, Microblogging became the new form of writing ,expressing and documenting an event.\n",
    "This gave rise to alot of unstructured data and a need to understand that data. This is where Text Classification can be put to our advantage\n",
    "\n",
    "> Text Classification means classifying unstructured text data into various categories such as Technology,Sports, Entertainment etc.\n",
    "\n",
    "Before we simplify how we use Text classification,let's look at this example, you have a product that was launched awhile ago.And you have also get a record of all the reviews(Good Reviews and Bad Reviews) that the product wa sbought in all the platforms accross the internet such as Trustpilot, Consumer Reports, Facebook,Google.\n",
    "What we have is unstructured text data and to do text classification on this unstructured Text data we can use two approaches:\n",
    "\n",
    ">  You can either make a few rules where a collection of word will decide the sentiments of the input textm this approach can be useful for a handful of data but analyzing large sets of data is neither efficient nor cost-effective."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd3d2c72-5bdf-4e5d-aedd-caebe29879e3",
   "metadata": {},
   "source": [
    "+ Good Review      Bad Review      Average review\n",
    "\n",
    "> Terrific       terrible          moderate\n",
    "\n",
    "> excellent       horible          potential\n",
    "\n",
    "> great           worst             improve\n",
    "\n",
    "\n",
    "\n",
    "Input Review - The product is excellent\n",
    "\n",
    "        Good Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dd1d94-e812-45ae-b410-9abd3d894f8d",
   "metadata": {},
   "source": [
    "ii. A better Approach is making use of Natural Language Processing and classification using machine learning.\n",
    "\n",
    "> Logistic Regresion\n",
    "\n",
    "> Artificial Neural NetWork\n",
    "\n",
    "> Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25234269-9378-4249-947e-7a672ea35cd4",
   "metadata": {},
   "source": [
    "For this ,we should train a classifier using seperate class labelled data and using this model we can do classification on the input data.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf61f7d7-9519-4790-a0f1-10db206eaf8b",
   "metadata": {},
   "source": [
    "Seperate class labelled data\n",
    "\n",
    "Review                             class label\n",
    "\n",
    "    \n",
    "This is very good product           Good\n",
    "\n",
    "\n",
    "It have good potential              Average\n",
    "to improve\n",
    "\n",
    "Terrible product ,                   Bad\n",
    "highly disappointed\n",
    "\n",
    "\n",
    "                       |\n",
    "    \n",
    "      Text analytics Operations\n",
    "        \n",
    "                    tokenization\n",
    "        \n",
    "                   Lemmatization\n",
    "        \n",
    "Classifier ->       pos tagging\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1efb1da-7747-46c2-b574-4947f811569a",
   "metadata": {},
   "source": [
    "When we give input to the classifier , we will get an output with the class category based on our train model, telling us if our review wsa bad approach or good.\n",
    "\n",
    "In this way , Text Classification on user Reviews can help us improve user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7028adb-839a-4544-8512-f1ffbebb3fd4",
   "metadata": {},
   "source": [
    "+ Input document:\n",
    "\n",
    "The product is very good in perfomance and delivers fantastically with lower cost\n",
    " \n",
    " \n",
    "+ Text analysis Operations\n",
    " \n",
    " tokenization\n",
    " \n",
    " lemmatization\n",
    " \n",
    "  stopwords \n",
    "  \n",
    " pos tagging         \n",
    " \n",
    " \n",
    " Classifiers -. outcome: good Review\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7374d8-4f41-4805-959b-6ff2ca190fa4",
   "metadata": {},
   "source": [
    "+ Data is the new feul, thus even bad reviews can help us identify the attributes which can help us improve our upcoming campaigns.\n",
    "\n",
    "Businesses and organizations are following this trend to understand users sentiment and user behaviour.\n",
    "\n",
    "Text classification can also be used for applications like spam detection in emails,targeting customer needs etc"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0785e44-fc19-4c64-991e-718b1a7c7d4a",
   "metadata": {},
   "source": [
    "+ Businesses and organizations\n",
    "\n",
    "\n",
    "         + Text classification\n",
    "    \n",
    "User sentiment       |         user behavior\n",
    "                     |\n",
    "    \n",
    "spam detection in     |        targeting customer\n",
    "emails                        needs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dbc750-fb74-41ef-8130-05b041561a50",
   "metadata": {},
   "source": [
    "+ In this day and age where data is generated every second of the day, Text classification becomes an asset for any organization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abafda3a-e145-41d4-969c-9cc1425c7ea6",
   "metadata": {},
   "source": [
    "#### Text Classification In Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7f4f4f-1efc-4e27-a3b3-0ad22f21b29d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Objectives' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1052/1566770198.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m+\u001b[0m \u001b[0mObjectives\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Objectives' is not defined"
     ]
    }
   ],
   "source": [
    "+ Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c5823b-3425-452b-8d3e-b0a0189cd0ce",
   "metadata": {},
   "source": [
    "+ Recall the basic mechanism of machine learning\n",
    "\n",
    "+ Understand the concept of Bag of Words\n",
    "\n",
    "+ Demonstrate Count Vectorization technique \n",
    "\n",
    "+ Implement the concept of TF-IDF over the csr matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b953e-4cfc-44fc-a02d-b10ea8e45eb0",
   "metadata": {},
   "source": [
    "##### Introduction to Machine learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852cc668-277d-4ff2-86db-f62ee419ab80",
   "metadata": {},
   "source": [
    "\"Machine Learning is a subset of artificial intelligence (AI) which provides systems/machines the ability to learn automatically and improve from experience without being explicitly programmed...\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f84c02f-054e-4abf-ab87-a3760b139b95",
   "metadata": {},
   "source": [
    "+ ML Process Flow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78c0d93b-8892-4d94-86bb-d7a455e71f7c",
   "metadata": {},
   "source": [
    "> Historical Data  \n",
    "\n",
    "+ Feature Engeenering \n",
    "\n",
    "i Split Datasets    Hyperparameter\n",
    "                        |\n",
    "a. training  ->  build a model -> training results\n",
    "                     |    \n",
    "b. Validation ->  Models   -   Validation Results\n",
    "    \n",
    "                     |\n",
    "    \n",
    "c. Hold-out-test   ->  Tests   -> Compare models\n",
    "                    Results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8441e566-4e93-4ab1-baf6-3817b8d6034e",
   "metadata": {},
   "source": [
    "+ We have a historical data,we features those that are of interest to us, we will not work on all the columns but on the ones that are of interest to us,we split the dataset into training and validation or training or testing . The  training dataset will build a model,  e.g logistic regression,Regression analysis or random forest.\n",
    "\n",
    "Then we train the dataset and see the training result.This means mapping the source to the destination. once the model is ready, you use the validation dataset or testing dataset, then predict the values.\n",
    "\n",
    "The predicted values will be compared with the actual values and we will come to knw if the actual values are correct or not.\n",
    "What is the accuracy of a particular model, If the accuracy is high we take some data to predict the feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd594d94-159b-47a8-b6f3-6e6cb4b6e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "+ types of Machine learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e57474-e2db-40a4-a255-1c8e07266f92",
   "metadata": {},
   "source": [
    "> supervised Learning\n",
    "\n",
    "Here we know the dataset completely. We know the column heading, the labels of the data . In this type of data we actually know the output we are going to get.\n",
    "\n",
    "This is a machine learning algorithm were we know the input and outputs, dependent variables, the independent variables, what we are looking at. the complete understanding of data is available.\n",
    "e.g rainfall, humidty, temperature In this data you know completetly what exactly the data is talking about.\n",
    "\n",
    "Is a predefined dataset, you know what is a dock and what is not a dock.\n",
    "\n",
    "Supervised learning is something like a document classifier, you know what type of document it is, maybe a text file, csv file base on the file extention. You classify the file depending the extention.\n",
    "\n",
    "so with supervised learning, we know what we are doing, the results or output we are expecting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b0e76a-9158-47dc-9434-66abe2093ae1",
   "metadata": {},
   "source": [
    "+ Condider you have a text data which has to classified into different classes as a result of it, it has to be converted into numerical data prior application of ML, algorithm.\n",
    "\n",
    "+ Machine learning algorithm cannot work on text so we need to convert to numbers, then we can predict or make some decisions from the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c6b1e3-674c-470a-bbda-797f76353f41",
   "metadata": {},
   "source": [
    "+ Unsupervised learning :\n",
    "\n",
    "we do not know the column headings or what the data is all about. For example, You found a new bone inside the water. You will need to figure it out based on the features from the particular data. By feature ,we mean the texture, color, size of the bone etc and based on that we match the particular data whether the new bone found matches with old ones or not. We use K-nearest neighbor or K-means nearest neighbour for the clssification, or clustering where you cluster the data based on the similarities.\n",
    "Here we get how much is the likelihood and not accuracy. So we classify based on similarities or disparities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a18b9-6cd9-47b0-95bc-c2546b4a6770",
   "metadata": {},
   "source": [
    "+ Reinforcement Learning :\n",
    "\n",
    "This is a mix of supervised and unsupervised learning. It is a type of learning where we dont know the outcome .\n",
    "\n",
    "Reinforcement is learning by experience  . the sysytem is learning from the images or numbers. for example aa aeroplane before taking off they know what the temperature is all about, rainfall, humidity, tempperature, the pressure of the rainfall and they use it to take the best decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6e1612-5855-4942-a687-2d688c375ce1",
   "metadata": {},
   "source": [
    "+ The bag of Words Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661226f-d8c4-4bf4-a947-756f4743b040",
   "metadata": {},
   "source": [
    "In common terms,it basically creates a list of all the unique words present across all the documents and then counts the frequency of each of these words appearing in the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9c6a8-2dca-476c-8419-f369e6700805",
   "metadata": {},
   "source": [
    "+ Example : Consider the below reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca12441-e276-4db8-8d78-fada2248f25d",
   "metadata": {},
   "source": [
    "review_1 = 'The movie was good and we really like it'\n",
    "review_2 = 'The movie was good but the ending was boring'\n",
    "review_3 = 'We did not like the movie as it was too lengthy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66362baa-ccc8-42b1-9132-b759100bead3",
   "metadata": {},
   "source": [
    "> the goal is to classify the movie reviews into positive or negative. Something like a sentiment analysis , finding out if negative or positive reviews are more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de16ff6-cb1d-4dd6-a3b5-8874e6c80f92",
   "metadata": {},
   "source": [
    "+ Creating Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d55dd5-8fd9-4bf8-8659-105e6a17615e",
   "metadata": {},
   "source": [
    "+ Creating Unions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7495718-11c8-4302-a7f9-38f30e783495",
   "metadata": {},
   "source": [
    "We will create union of all the words so that we get unique words from all the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf684c5-dffb-436d-a7d8-1597ecba3e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type \"copyright\", \"credits\" or \"licence()\" for more information.\n",
    "set1=set('aaaabbbbbccccccc')\n",
    "set1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f44ea-3a43-49ef-b249-9f9bca4f6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "set2=set('bbbbbbbbbbbbccccccccccccddddddddddddd')\n",
    "set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d70f50-f87f-40d8-80c6-68c2e8feb473",
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 | set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac678e7-4dc8-463d-bec8-7616300b93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 & set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8051ae78-d8f6-4ed8-aee4-f56e33c6ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can create  a union of values\n",
    "set1.union(set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e0eb7-6403-42ae-8ccd-aab492960309",
   "metadata": {},
   "source": [
    "+ we will read all the sentences and convert them into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21700537-ae0b-424a-a80a-b838342db702",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1 = 'The movie was good and we really like it'\n",
    "review_2 = 'The movie was good but the ending was boring'\n",
    "review_3 = 'We did not like the movie as it was too lengthy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190583d-ea22-48bd-91c3-689dcf5faafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ea3be-d34b-44d0-bc4f-3d59f5b8fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1_tokens = word_tokenize(review_1)\n",
    "print(review_1_tokens)\n",
    "review_2_tokens = word_tokenize(review_2)\n",
    "print(review_2_tokens)\n",
    "review_3_tokens = word_tokenize(review_3)\n",
    "print(review_3_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af0ff78-8b13-4413-a5f6-3f522706bf67",
   "metadata": {},
   "source": [
    "+ We'll now join all the three reviews by creating a set to get all the unique words accross all the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50bb633-b3ff-4589-bc8c-5467838aeb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take a union of all of them\n",
    "# This will print out all the values which are unique\n",
    "review_tokens = set(review_1_tokens).union(set(review_2_tokens)).union(set(review_3_tokens))\n",
    "print(review_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe87a60-6ac6-4440-94a2-e128dcc6e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(review_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c87a5f-1bea-4c44-9401-adea9d5333bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71c59c-27d6-453f-8fd0-446658f566b2",
   "metadata": {},
   "source": [
    "+ Processing Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4ae50-e4d1-4038-afef-9b97012a05bc",
   "metadata": {},
   "source": [
    "We'll now create a dictionary where the keys will be the 19 tokens and the default value of each token will be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51c138-fd68-4757-97b7-dc01e1731634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the from keys function to create a dictionary\n",
    "review1_dict = dict.fromkeys(review_tokens,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a50729c-f3b8-4459-a357-dd085f0088d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "review1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38584ea4-2f05-48e6-9f12-4cc1f1d082ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now create review2 and review3 dict in same manner\n",
    "review1_dict = dict.fromkeys(review_tokens,0)\n",
    "review2_dict = dict.fromkeys(review_tokens,0)\n",
    "review3_dict = dict.fromkeys(review_tokens,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf69eed7-fccb-4d02-b767-f0493e5550da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to find out if all the words in review1 is available if yes, it should be identified with + 1 \n",
    "# The rest will be zeros\n",
    "for token in review_1_tokens:\n",
    "    review1_dict[token]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6464f93f-cc8a-4711-abda-517305f54da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "review1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87703c1c-b06b-4c5e-880a-6b1bd1505d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will do the same for review2 and review 3\n",
    "for token in review_2_tokens:\n",
    "    review2_dict[token]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575afb7b-f89a-41eb-85b7-242d4e395c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "review2_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2ee3f-fdc5-4f2b-a63e-22b760ce6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in review_3_tokens:\n",
    "    review3_dict[token]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b70b7-ce90-4c0a-a645-3344e3b0339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "review3_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffcd737-3c8c-4344-b44e-3c22dd261fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now convert everything to a dataframe\n",
    "\n",
    "reviews_Dict_DF = pd.DataFrame([review1_dict,review2_dict,review3_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15126439-e477-4015-926a-c8d81aecad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_Dict_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe66fae0-7c3e-4f48-8c6d-baa8abba6526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the movie review for review 1, 2 and 3\n",
    "# As we can see , we have all the tokens as columns and all the three reviews as rows and the occurence of each of the words in each of the reviews is mentioned in the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb51902-91de-4661-94cc-f51a04342ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can now use this to implememnt any machine learning algorithm to find the clustering ,relational indices, random forest etc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098ed8d-f8af-4c41-9398-c0dd5ce1f15b",
   "metadata": {},
   "source": [
    "+ Adding Counts To The Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff49a185-b50d-409b-b826-6ebf12bc5fae",
   "metadata": {},
   "source": [
    "Create a for loop which for each of the tokens in the review will add 1 to the value of that token in th dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673536c-63bd-4305-b3bb-d5e8ec22861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in review_2_tokens:\n",
    "    review2_dict[token]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d2dec-6062-45c9-aa64-5d2733b5cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check thesame\n",
    "review1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5dc39c-b5dc-49ad-9c19-bc005ccfa752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create 3 different dictionaries, we add the 3 diffrent dict. in the form of a dataframe, and this is what we know as bag of words.\n",
    "# The list of words against their counts is known as Bag of Words because it contains all the sentences with their frequencies.\n",
    "# We have all the tokens as columns and the 3 reviews as rows.\n",
    "# And the occurances of each of the words in each review is mentioned in the cell\n",
    "# Hence , we get clear picture in what sentance, or what word or what frequency we have in that particular word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a90ec-592e-4b0a-94a5-5cf863dccc98",
   "metadata": {},
   "source": [
    "+ the bag of Words approach has solved all the problems and it has solved our problem of using Text in machine learning\n",
    "\n",
    "One of the best algorithms to play with is binary values 0 and 1. (independent dataset)\n",
    "\n",
    "Logistic Regresion is another good algorithm when the dependent values is binary values 0 and 1., True or false.\n",
    "\n",
    "We can also use Random Forest Regression as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f7f2ed-e6ae-48e6-87b6-f56b03a309a8",
   "metadata": {},
   "source": [
    "+ The process of creating bag of words, We learnt just now is a bit lenghty and hanfully scikit Leran provides a much easier approach for the same . Let's see how!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7d9f11-7e2a-4a54-88b5-c22b6c460967",
   "metadata": {},
   "source": [
    "Let's say you have 10,000 words, this 10,000 words will be converted intoa dictionary, 10,000 loops will run and we need to filter out the values and putting dataframes 10,000 times, Hence, a very cumbersome process. \n",
    "When you have  avery big document, BOW will not be very effective for it. This is where Count Vectorization comes handy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c68e89-a2e2-4dc9-99cd-364cb48346f9",
   "metadata": {},
   "source": [
    "+ Count Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a3520-fae1-4e7c-b8a7-51f389f81705",
   "metadata": {},
   "source": [
    "> Count Vectorization\n",
    "\n",
    "+ Each word in a document its occurence is counted and noted in a vector.\n",
    "\n",
    "+ The vector is typically huge as it contains as many elements as words occurs in th whole dataset.\n",
    "\n",
    "It is counting the frequency of each word in a document and putting it in a form of a vector\n",
    "The above step is called vectorization.\n",
    "\n",
    "The count vectorization is a technique in which the whole document  is read , each word is counted and put in a form of a matrix bas structure, and the frequency of each word is updating everytime the new sentence comes in ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c893c04d-bcea-4480-a15b-1d22c9e38dba",
   "metadata": {},
   "source": [
    "+ Count Vectorization - Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25197e2-b01b-41cc-b73e-4d2f8d0e40c5",
   "metadata": {},
   "source": [
    "For the two statements = \"How to format my hard disk\" and \"Hard Disk format Problems\"the vectors are shown below:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0b381b3-2851-41cc-9ef0-6f508a3a45c2",
   "metadata": {},
   "source": [
    "word   Occurence in post 1    Ocurrence in post 2\n",
    "\n",
    "disk            1                    1\n",
    "format          1                    1\n",
    "how             1                    0\n",
    "hard            1                    1\n",
    "my              1                    0\n",
    "problems        0                   1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da316e4f-5167-43ba-be21-ac83fb742e92",
   "metadata": {},
   "source": [
    "+ Count Vectorization In Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e185bfb8-a93d-4e9d-be3b-0a0a5e076380",
   "metadata": {},
   "source": [
    "+ Let's start by importing the necessary libraries as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9ae01-9f60-4e92-939e-e3148d1dd604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70725474-f1b2-435c-9bec-06b33efa5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using thesame review create list of same reviews:\n",
    "review_list = [review_1,review_2,review_3]\n",
    "review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16c6d8b-3b76-4449-bd2f-cc1ffa3967ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now,instantiate te count vectorizer and fit transform it with the list:\n",
    "count_vect = CountVectorizer()\n",
    "count_vect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce62119-96a6-4d4f-ae66-1b944c33771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts = count_vect.fit_transform(review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b43a5c-ed9b-44d4-8722-3169c32f9b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the type of count vectorized matrix:\n",
    "type(X_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cafbdc-ee28-41db-954b-00a0ef36803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_names = count_vect.get_feature_names() # a feature of countvectorizer\n",
    "X_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb069ad-2516-4b9a-898c-d2b28a1eda58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all the unique names we have in our vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0845cf00-abe3-4555-95ac-0f2abd5c355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert to a dataframe,convert to an array then put the column nanes\n",
    "# The feature names are the vector names\n",
    "a = pd.DataFrame(X_counts.toarray(),columns=X_names)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b4613-8169-4c24-a2c2-a0b29637257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the list of unique names as a result of count vectorization:\n",
    "# count vectorizer omits words that are less than 2 letters long\n",
    "# Count vectorizer removes the words that are not relevant at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd36509-a987-4de1-8794-a50e86c8c5cd",
   "metadata": {},
   "source": [
    "In the process below the countvectorizer, although counts the frequencies of each of the words but also includes few irrelevant words in the text, which are of no importance in training a ML model.\n",
    "Hence,we will use another process known as Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381d3cf0-1a5b-4d6a-82a1-6d514fb1780e",
   "metadata": {},
   "source": [
    "+ TD - IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac1180-86b9-4c87-bef8-7c164e6450c9",
   "metadata": {},
   "source": [
    "A statistice which shows ,how important a word is in a collection of documents\n",
    "\n",
    "+ FF(t,d): The total number of occurances of word in the instances of document d\n",
    "\n",
    "+ IDF: Lof(total number of documents/number of documents containing t)\n",
    "\n",
    "TF-IDF Score: TFIDF(d,t) = TF(d,t) = TF(d,t) * IDF(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddcf87f-bf48-4812-ace4-6d701fbe8c90",
   "metadata": {},
   "source": [
    "+ TF - IDF In Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86b50e-aa56-4544-8c1f-6c5792ef87e9",
   "metadata": {},
   "source": [
    "Let us start with importing the libraries for TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3fac36-604b-475b-997c-fb36f60dc7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b2ed65-7833-414a-84d7-f0865c35f063",
   "metadata": {},
   "source": [
    "Instantiate the TD IDF Vectorizer by passing the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b087fc-4e0d-4d75-b8b3-805b295a47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (min_df=1 tells the vectorizer to ignore the words that have a document frequency less than this number\n",
    "# lowercase=True is a boolean parameter to convert all the words into lowercase before tokenizing\n",
    "# many times proper nouns are capital letter like name of a place,a person \n",
    "# stop+words=english specifies the language you want to use in tokenizing\n",
    "tf_vect = TfidfVectorizer(min_df=1,lowercase=True,stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5703b0e-5b4a-4054-b340-6ab36c08a8fa",
   "metadata": {},
   "source": [
    "Once, we have instantiated the vectorizer we can pass the review list of all the three reviews to the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac198c4a-90b2-46c8-9abb-7cfbfea29e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fit_tranform review list to get a complete list of all the sentence\n",
    "tf_matrix = tf_vect.fit_transform(review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2ad46-6024-4874-b1ca-d1306b6f88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above will result into a scipy csr matrix with 3 rows and 8 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11036981-b7cb-4d70-8d95-2c1960b44a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix =  tf_vect.fit_transform(review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd5bf6a-2bf2-41c8-981d-e5e15156a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c2d9d-21f9-4bc6-942e-c47139bbee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the matrix\n",
    "tf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e862616-181e-4007-91cc-79d1f396b451",
   "metadata": {},
   "source": [
    "+ TF - IDF Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696d11cb-4eb2-4b01-9543-5b55010cd416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give you a list with 8 tokens\n",
    "tf_names = tf_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54cd6f5-d9bf-49aa-9ebf-aaf6fb9df6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c7db73-b07b-403c-a576-08cf24581eba",
   "metadata": {},
   "source": [
    "+ CSR Matrix To  Pandas DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150135d-3914-4092-a794-ad6f785724a9",
   "metadata": {},
   "source": [
    "you can create a pandas dataframe by passing by passing a scipy csr matrix as values and the list of tokens, you get above as column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c7f61b-8e5e-4d69-8d40-e54676233a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a dataframe after converting to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d8d324-f2b0-4912-90ad-a263cfe1b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df = pd.DataFrame(tf_matrix.toarray(),columns=tf_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518dceb-3853-426d-819c-feb2b2d52693",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ec137-87c5-4c9d-9073-5f08e6e3520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataframe has tf-idf values for all the 8 tokens in floats across all the 3 reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e2488-2e8d-4b7c-b464-c05ac994f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a sparse matrix\n",
    "# A sparse matrix or sparse array is a matrix which most of the elements are zero\n",
    "# By constrast, if most of the elements are non zero then the matriz is considered dense\n",
    "# The number of zero-valued elements divided by the total number of elements(e.g. m x n for an m x n matrix)\n",
    "# is called is called the sparsity of the matrix(which is equal to 1 minus the density of the matrix)\n",
    "# Sparse matrix is used in network programming to find out how words are related to each other, like \n",
    "# a sting having words attached to each other\n",
    "# Let's say you have 10,000 sentences and you are checking the frequency of every word, in a single\n",
    "# sentence all the words cannot come cos you have many zeros there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087bde4-0347-4a9e-a853-5a4cfe2b2373",
   "metadata": {},
   "source": [
    "### Text Classification using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a756c-648b-4978-96b2-fd57ad57e2cd",
   "metadata": {},
   "source": [
    "+ Use Case - Text Classification using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb92dcc-f734-4bd4-b06b-8566a0586776",
   "metadata": {},
   "source": [
    "To perform text classification of News Headlines and classify news into different topics for a News Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2862c320-3148-4524-9b5b-a207c3d053ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Firts ,we go to google news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbef8f9-4a6f-4ff8-91d9-9024ccbc3d49",
   "metadata": {},
   "source": [
    "+ Use Case - Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38167c5b-faff-4616-bc47-9542b90459c7",
   "metadata": {},
   "source": [
    "Using the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba539ba-b4b8-4e38-9f03-e33c5db9dc9c",
   "metadata": {},
   "source": [
    "+ Loading the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb3353d-4b57-4603-8f16-a2d228f691bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np #for sklearn module and maths function\n",
    "import matplotlib.pyplot as plt # for graph functions\n",
    "import seaborn as sns; sns.set() # sits on top of pyplot and good for heat maps\n",
    "from sklearn.datasets import fetch_20newsgroups # for analyzing and tokenizing\n",
    "data = fetch_20newsgroups() # categories we have assigned 20 diff.categories.\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31388c25-90a1-4767-b46d-585db9c89e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining all the categories\n",
    "categories = ['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']\n",
    "# Training the data on these categories\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "#Testing the data for these categories\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "#printing training data\n",
    "print(train.data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a18c1e-e91c-4948-b882-9a2d1f3da23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing test data\n",
    "print(test.data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a294979-f6ed-42bf-a4e8-65945e0ba100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find out the length of the training data\n",
    "#printing length training data\n",
    "print(len(train.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8603cb-0f49-41f8-9bd5-389f5647884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 11314 articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4759c738-da5e-48d0-9a87-01c929bc2f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the type of article using n0.5\n",
    "print(train.data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdee440-c32e-4869-8641-689f0c2408f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to figure out what category it fits in based on these words is where the challenge comes from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98bf5d5-8336-42b3-b7e0-5bdfadcd9643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have viewded our data\n",
    "# we need to do the actual predictions using the naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584608b-d513-409f-90aa-394365233f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # helps you to figure out the weights in the article\n",
    "from sklearn.naive_bayes import MultinomialNB #NB means naive bayes\n",
    "from sklearn.pipeline import make_pipeline # we ll take the infor. we get rom tfidf vect.and pump it into the the multinomialNB , the pipeline will help us organize how things flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f5919-a40c-46ec-9661-c7f88a8b4a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model based on Multinomial Naive Bayes\n",
    "# We use the pipeline model created to fit the data\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64002d04-f968-4d33-acf7-17c7e80fc4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with the train data\n",
    "# it helps to categorize the article\n",
    "# The train data will go into the tfidf vec.and weighs all the words in there\n",
    "# We have 1000s of words with diff.weights,it takes those words gives them a weight\n",
    "# Based on the words and the weight it puts it into the multinomialNB\n",
    "# then once we go into our naive bayes , we put our train target\n",
    "model.fit(train.data, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eebff07-fd0a-4eb1-a2d9-81140d2d777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating labels for the test data\n",
    "# after fitting the model we set the labels  , using model.predict to get answers \n",
    "# then we put our test data to find out ow good our labels are\n",
    "# if they match what they should\n",
    "labels = model.predict(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2722588d-cbb0-4e2e-b9c3-21b89a12888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the confusion matrix is basically going to ask how confused is our answer\n",
    "# is it correct or did we miss something in there or have some missed labels\n",
    "# Then we will put it o a heat map and add some nice colours to it to see how it plots out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35624d02-640c-4a9c-a51b-54080b4760ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating confusion matrix and heat map\n",
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(test.target, labels) # we have test data that is not part of the training. Always remember to keep the test data seperate else it'll not be a valid model\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d',cbar=False     #  we create our heatmap which sits on top of the pyplot\n",
    "              , xticklabels=train.target_names                     # we take our confusionmarix mat.T,annt=True is what tells you where to put the number\n",
    "             , yticklabels=train.target_names)                     # xticklabels and ytick ars target names\n",
    "\n",
    "# plotting heat map of confusion matrix\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5f8d1-4d04-4dad-b867-b6f3188b9699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the graph , we would want to look at how the color scheme have come out, you will see a line right down at the middle diagonally from  upper left to buttom right. we have our predicted labels on the left and True label on the right . \n",
    "# Those are the numbers of the prediction and true label comes together\n",
    "# So the heat map helps us to se the letters\n",
    "# we have a couple of mixed red spots. It means they miss labelled some of these, like christian vs alt atheism  these are similar topics  reason it mis-labelled it but over all it did a good job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46cfea6-7299-4ed8-a8d4-3fb832518f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create these models , we need to create a definitionm, predict category, then send in a string \n",
    "# We will pass in a training model , train =train, then our pipeline model=model, the definition helps us to do this\n",
    "# So we dont need to train the model each time\n",
    "# Then we set the pred= model.predict  , this helps to predict the model we sent to it by pushing it to the model pipeline\n",
    "# then tokenize it and put it through the tfidf and convert it into numbers and weigths for all the diff. documents and words\n",
    "# and then put that through our naive bayes\n",
    "# Then we get our predictions, we will preict what value it is\n",
    "# hence, we will return train.target names pred[0]\n",
    "# Remember that train .target names is for categories, we can also do categories .pred[0]\n",
    "# e take a look at the cat which is a number , we are converting it into  words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c7836f-bacd-4f49-afd1-c14e5b51d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting category on new data based on trained model\n",
    "def predict_category(s, train=train, model=model):\n",
    "    pred = model.predict([s])\n",
    "    return train.target_names[pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a75613-8571-461f-938b-652428ebe484",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('Jesus Christ')  # Relogion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793b629-212f-468d-8245-08682e75d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('Sending load to International Space Station')  # Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638ed065-fbc5-4f81-a291-d47046dab324",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('BMW is better than Audi') # Autos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493841f7-5fbe-4990-8377-2a7fa14bf272",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('President of India') # Politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eeb28f-a938-4c80-9651-4f22df6aa5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('Suzuki is a very fast motorcycle')  # motorcycles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213fd63-65fe-427d-975f-620a62546f22",
   "metadata": {},
   "source": [
    "+ We were able to correctly classify texts into different groups based on which category they belong to using the Niave Bayes Classifier .\n",
    "\n",
    "One of the main uses of the naive bayes is with  the tfidf vectorizer? tokenize which vectorizes the word next levels\n",
    "\n",
    "We also used the pipeline because we need to push all the data through ant it makes it easy and fast.\n",
    "\n",
    "These helps in understanding the industry in data science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf8370-5107-4801-9dc3-d2db02bc09f7",
   "metadata": {},
   "source": [
    "#### Text Classification: Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a84ce-5f0a-412a-a003-4b2ecffb88ca",
   "metadata": {},
   "source": [
    "1.__Text classification___________________ the process of assigning predefined categories to the text documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb24be6-eed2-404d-9dab-5a71f19c5c15",
   "metadata": {},
   "source": [
    "2. An example of text classification is _Classification of e-mail messages as spam and non-spam________________?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91cf0d-dea4-4e7d-a8b0-63c98fd56f27",
   "metadata": {},
   "source": [
    "3. Text classification can be categorized as follows: __________ and ______\n",
    "\n",
    "Content and request based classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b28b2d0-5186-470c-ac7a-57f4fb28cb0c",
   "metadata": {},
   "source": [
    "4. Which of these is NOT a text categorization method? Gradient descent\n",
    "\n",
    "Below are text classification method\n",
    "1. Decision tree\n",
    "2. K-Nearest Neigbor\n",
    "3. Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03bf134-14eb-41a6-b95c-82af5fab62e8",
   "metadata": {},
   "source": [
    "5. Which of this is an example of text categorization method?\n",
    "\n",
    "Naive Bayes classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347832ab-0793-4eda-89a7-e313667be66d",
   "metadata": {},
   "source": [
    "### Week 15: Day 2 â€“ Recommendation System \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de89da65-7538-46b2-932b-58e7ec6f5c76",
   "metadata": {},
   "source": [
    "#### Introduction to Recommendation system (Reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18a7a3-f9ac-417a-ae32-d06879ba247b",
   "metadata": {},
   "source": [
    "#### Overview of Recommender systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d87cc3-9150-4231-9d10-a82248822c2b",
   "metadata": {},
   "source": [
    "#### Mining of Massive Datasets\n",
    "\n",
    "##### Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7054b6c2-24c4-49fb-adfd-479d1cc3aec2",
   "metadata": {},
   "source": [
    "+ Overview\n",
    "\n",
    "+  Content - Based Systems\n",
    "\n",
    "+  Collaborative Filtering \n",
    "\n",
    "+ Evaluation Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44bbcd-f29e-4af8-a0e0-e0b3b1a5f359",
   "metadata": {},
   "source": [
    "+ Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6adef-2efc-41e7-a211-00edf6078450",
   "metadata": {},
   "source": [
    "+ Imagine any situation where user have many large catalogue of items.\n",
    "These items can be products from amazom.com' Movies from Netflix,music items, news items from google news etc\n",
    "\n",
    "What matters is that there are tens or millions of items on really large catalogues a user is interacting with these catalogues\n",
    "\n",
    "There are two ways a user can interact with these catalogue of items.\n",
    "\n",
    "\n",
    "> i. Search : The user knows what they are looking for and they search the catalogue for the pricise item they are looking for.\n",
    "\n",
    "> ii. Recommendations:\n",
    "When we have a very large catalogue of items, very often the user doesnt know what they are looking for, and this is where Recommendation comes in. The system recommends to the user certain items they may think the user will be interested in based on what they know about the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1055c75-0e52-4da9-9b4f-7bb76ff027c0",
   "metadata": {},
   "source": [
    "+ Why do we need such recommendations ?\n",
    "\n",
    "+ From Sacrcity to Abundance\n",
    "\n",
    "The key that made recommendation so important and why Recommendations system developed so much in the last10 or 20  years is that it moved from a need of scarcity to a need of abundance .\n",
    "\n",
    "> Shelf space is a scarce commodity for traditional retailers. \n",
    "Imagine that you were shopping 20 years ago, you went to a local retailer and you found a certain number of products on the shelf of a local retailer. Even in a really large retailer like wallmart for instance, shelf space is a scarce commodity.\n",
    "\n",
    "It limits the number of items that a retailer can carry.Shelf space is expensive because it involves real estate cost. Hence, a retailer can carry only certain amount of products.\n",
    "\n",
    "\n",
    "+ Also: Tv networks , movie theatre , ,,, Similar situations can be experienced as well. a Tv network can carry so many shows depending the number of hours in a day. Also there are so many movie theatres so they can do only show a number of movies.\n",
    "\n",
    "\n",
    ">  From Scarcity to Abundance\n",
    "+ Once internet was developed,things changed. \n",
    "The web enabels near-zero cost dissemination of information about products. This means we can have many more products like never before. There is no shelf limitation on the number of products. This is why the number of products in amazon is much more than any physical retail store. The number of movies available on Netflix is much more than the one available in stores etc.\n",
    "\n",
    "\n",
    "This new zero cost disemiation of information  gave rise to a phenomenon  called the \"long tail\" phenomenon.\n",
    "\n",
    "\n",
    "Imagine a graph where the X axis is taking the items in the catalogue, such as music, books, video or news article.\n",
    "\n",
    "The most popular items are on the left and as you move toward the right it becomes less popular.\n",
    "\n",
    "By popular we mean the number of times the purchase maybe. The number of times movies is viewd in a week or a month.\n",
    "\n",
    "In the y-axis we have the actaul popularity of items.\n",
    "\n",
    "When we take items in the catalogue and line , you wil have a graph showing the decline in popularity for some items.\n",
    "When this happens it is adviceable to stop stocking the items because the loss is more than the profit.\n",
    "In a retail store, is adviseable to stop stucking the products. \n",
    "Items that are more popular is available in the retail store and the ones that are less popular are not available at any retail store, the less popular will now be made available on line.\n",
    "\n",
    "Because there are so many items ,you need a beter way for the users to find all these items.\n",
    "The user doesn't even know where to start from \n",
    "\n",
    "+ Examples\n",
    "+ Books ,movies, music, news articles\n",
    "\n",
    "+ people(friend recommendations on facebook, Linkedln,and Twitter): If you go tofaceboo or any of these platforms above, there are many people so facebook make recommendation for you.\n",
    "\n",
    "\n",
    "+ Types of recommendation\n",
    "\n",
    "\n",
    "+ Editorial and hand curated\n",
    "\n",
    " > list of favorites\n",
    "\n",
    " > Lists of \"essential\" items\n",
    " \n",
    "The draw back with this is that is done entirely by the ownres of the website and no space for input by users.\n",
    " \n",
    " + simple agrregates\n",
    " \n",
    " Top 10 , Most popular , Recent Oploads\n",
    " \n",
    " If you o to youtube ,you can see the most popular videos and recent uploads , all these are recommendation system.\n",
    " This recommendation dont depend on the user but on aggregate actiivities of other users.\n",
    " \n",
    " + Tailored to individual users\n",
    " \n",
    " > Amazon, Nerflix, Pandora...\n",
    " This has to do with books, films or music you have used before.\n",
    " \n",
    " > Our focus here is recommendations that are tailored to individual users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8441b25f-cff5-428d-85fa-4929f90a323c",
   "metadata": {},
   "source": [
    "+ Formal Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc128aaa-85c1-4818-830a-21352d2d831c",
   "metadata": {},
   "source": [
    "\n",
    "+ C = set of Customers\n",
    "+ S = set of items\n",
    "\n",
    "\n",
    "+ Utility Function U: C x S -> R\n",
    "We need to create a function called utility function or matrix\n",
    "This is a function that looks at every pair of customer's item and maps it into a rating\n",
    "\n",
    "\n",
    "    > R = set of ratings e.g star ratings from 1 star to 5 stars or number of rating 0 and 10\n",
    "    \n",
    "    > R is a totally ordered set\n",
    "    \n",
    "    > e.g., 0-5 stars, real number in [0,1]\n",
    "    \n",
    "R can also be a totally ordered set where a lower value indicates that the customer likes the product less and a higher value indicates the customer lkies the product more    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87140b9c-6f0c-4bd3-888a-8f9845150cb5",
   "metadata": {},
   "source": [
    "+ Utility Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068e027f-b7dd-479e-8cc6-b9f36c5c7f15",
   "metadata": {},
   "source": [
    "This gives you ratings for certain users and certain ratings . utility ratings is going to be sparsed because viewers most likely will not rate all the films .\n",
    "\n",
    "The key problem with Recommendation systems is to figure out these unknown values.\n",
    "For example, Alice rated only two movies from the 4 movies online. How do we figure out what her ratings may be from the ones that she didnt rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87060800-23bb-49e6-a26b-c5cb7d3e3594",
   "metadata": {},
   "source": [
    "+ key Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb0a5f9-a917-484c-8177-b205877c901a",
   "metadata": {},
   "source": [
    "(1) Gatehring \"Known\" ratings for matrix\n",
    "\n",
    "+  How to collect the data in the utility matrix\n",
    "\n",
    "(2)Extrapolate unknown ratings from the known ones\n",
    "\n",
    "+ Mainly interested in high unknown ratings\n",
    "\n",
    "+ We are not interested in knowing what you don't like but whta you like\n",
    "\n",
    "(3) Evaluating extrapolation methods\n",
    "\n",
    "+ How to measure suceess/performance of recommendation methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d52f05a-8c6e-4310-84fe-07ca2b92c22f",
   "metadata": {},
   "source": [
    "(1) Gatehring \"Known\" ratings for matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8e4ad-730b-4384-bda6-0d7384618a6b",
   "metadata": {},
   "source": [
    "> Explicit \n",
    "\n",
    "+ Ask people to rate items\n",
    "\n",
    "+ Doesn't scale:  only a small fraction of users leave ratings and reviews\n",
    "You can ask people to rate from 0-10 or state if they like an item or doesn't like an item\n",
    "It helps you to get direct response from users but the problem is that it doesn't sacle.\n",
    "\n",
    "Only a small fractio of user, watch a movie or listen to music and as such it may not be able to scale.\n",
    "\n",
    "> Implicit\n",
    "\n",
    "+ Learn ratings from user actions\n",
    "For e.g ,an online shopping website might have a rule that purchasing implies a high rating.\n",
    "Implicit ratings are much more scalable than explicit ratings. Because the users never had to scale the items and there are way more other actions than to weigh more other actions like purchases than there are rating.\n",
    "+ E.g, purchase implies high rating\n",
    "\n",
    "\n",
    "+ What about low ratings?\n",
    "The problem is that is difficult to use implicit ratings to learn low rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9feb10-f2e8-4a97-a642-23ba433dab5b",
   "metadata": {},
   "source": [
    "In practice, most recommender system in most websites use a combination of explicit and implicit ratings. they use the implicit and supplement with explicit when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a11627-c9ab-4af0-bf56-4b8b149c3c17",
   "metadata": {},
   "source": [
    "(2)Extrapolate unknown ratings from the known ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e7778f-90d5-42ec-b061-abbdcf65b151",
   "metadata": {},
   "source": [
    "> Key problem : matrix U is sparse\n",
    "\n",
    "+ Most people have not rated most items\n",
    "\n",
    "+ Cold start:\n",
    "\n",
    " > New items have no ratings\n",
    " \n",
    " > New users have no history\n",
    "\n",
    "\n",
    "+ Three approaches to recommender systems\n",
    "\n",
    "+ 1) Content-based\n",
    "\n",
    "+ 2) Collaborative\n",
    "\n",
    "+ 3) Latent factor based\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9a2f8-5546-420c-b3b4-8c2b8f26c981",
   "metadata": {},
   "source": [
    "### Content-based Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6131cf5-151c-46b8-82b2-2ddfce974de9",
   "metadata": {},
   "source": [
    "Main idea : Recommend items to customer X similar to previous items rated highly by same customer X\n",
    "\n",
    "Examples :\n",
    "\n",
    "+ Movies\n",
    "\n",
    "  > Same actor(s), director,genre,...\n",
    "  \n",
    "+ Websites, blogs, news\n",
    "\n",
    "  > Artcles with \"similar\" content \n",
    "  \n",
    "+ People\n",
    "\n",
    "> Recommend people with many common freinds to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1f5acd-cbb8-4b80-89ad-6ee8c6d729d9",
   "metadata": {},
   "source": [
    "+ Plan of Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f49b2-1089-4a34-b194-bd0e58c8e7ce",
   "metadata": {},
   "source": [
    "First find out the set of items the user likes\n",
    "You need to build an item profile. An item profile is a discription of the item,\n",
    "For example, when you are dealing with geometric shapes. you can make the likes to be red circle or geometric triangle.\n",
    "\n",
    "For exmple an item profile that says that the user likes red items or the user likes circles.\n",
    "\n",
    "From these item profiles we build a user profile, the user profile infers the likes of the user from the profile of the items the user likes.\n",
    "\n",
    "Because the user likes red circle and triangle, we can infer that the user likes the color red .\n",
    "\n",
    "Once we have the profile of the user , we can then match it with the catologue, Some of the catolugue have the color red and we can recommend them to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2326165-5895-4955-b17d-dbe6ea6b245d",
   "metadata": {},
   "source": [
    "+ Item profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a8ceca-8046-4bd2-a690-41f4e6948226",
   "metadata": {},
   "source": [
    "+ For each item, create an item profile and then use it to build user profiles.\n",
    "\n",
    "+ Profile is a set of features \n",
    "\n",
    "> Movies: author , title, actor, director,...\n",
    "\n",
    "> Images , videos: metadata and tags\n",
    "\n",
    "> People: Set of friends\n",
    "\n",
    "+ Convenient to think of the item profile as a vector\n",
    "\n",
    "> One entry per feature (e.g, each actor, director..)\n",
    "\n",
    "Vector might be boolean or real-valued like 0 and 1 for each actor, director depending on whether that actor  or director actually participated in that movie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5509477d-6a14-4f45-b4c6-ccbd03ff280a",
   "metadata": {},
   "source": [
    "+ Text features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497640c8-ca82-4e31-993d-f9a40fd7a575",
   "metadata": {},
   "source": [
    "+ Profile = set of \"important\" words in item (document)\n",
    "The important item profile here is to pick the set of important words in the items\n",
    "\n",
    "+ How to pick important words?\n",
    "\n",
    "+ Usual heuristic from text mining is TD-IDF\n",
    "(Term frequency * Inverse Doc Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142619ac-9672-471e-b75e-09d862df0780",
   "metadata": {},
   "source": [
    "+ Sidenote : TD-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17816f06-8f84-4767-a8a8-a8739e992cd9",
   "metadata": {},
   "source": [
    "fij = frequency of term (feature) i in doc \n",
    "(item)j\n",
    "\n",
    "Note: we normalize TF to discount for 'longer' documents\n",
    "\n",
    "TFij = fy / maXk fkj\n",
    "\n",
    "Time frequency shows the number of times an item appeared in a document.\n",
    "\n",
    "ni = number of docs that mention term i\n",
    "\n",
    "N = total number of docs\n",
    "\n",
    "   IDFi = log N/ni\n",
    "   \n",
    "Note :   The larger the ni the lower the idf\n",
    "    \n",
    "TF-IDF score: Wij = TFij x IDFi\n",
    "\n",
    "Doc profile = set of words with highest TF-IDF\n",
    "   scores , together with their scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff0af1-edf3-4218-b21a-04b12936f94c",
   "metadata": {},
   "source": [
    "+ User profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480222c6-ba6c-4c7b-8fb0-98cf4b384d46",
   "metadata": {},
   "source": [
    "Let's say we have a user who have rated items with profiles ii, ....,in\n",
    "\n",
    "The numbers i1 ....in are our vectors\n",
    "\n",
    "+ simple(weighted) average of rated item profiles,\n",
    "\n",
    "You taket the total number of items the user have rated and then tale the average. This is the simplest way of contucting the user profile while taking t=into account that the user likes certain items more than others. Hence , we may have to use the weighted average.\n",
    "\n",
    "The weight is equal to the rating given by the user for each item.\n",
    "\n",
    "+ Variant: Normalize weights using average rating of user\n",
    "\n",
    "+ More sophisticated aggregations are possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b872219-ee79-47be-a6cb-6967863e4442",
   "metadata": {},
   "source": [
    "Example 1: Boolean Utility Matrix\n",
    "    \n",
    "Weighted average profile and how to normalize weights.\n",
    "\n",
    "In boolean utility matrix , we have the information of whether the user purchased the item or not. each entry is 0 or 1.\n",
    "\n",
    "+ items are movies, only feature is \"Actor\"\n",
    " \n",
    " > item profile: vector with 0 or 1 for each Actor. 0 if the actor did not appear and 1 if he appeared.\n",
    " \n",
    " + Supposed user x has watched 5 movies\n",
    " \n",
    " *  2 movies featuring actor A\n",
    " \n",
    " *  3 movies featuring actor B\n",
    " \n",
    " > The simplest use of profile is the mean of the item profiles\n",
    " user profile = mean of item profiles\n",
    " \n",
    " > Feature A's weight = 2/5 = 0.4\n",
    " \n",
    " > Feature B's weight = 3/5 = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63795649-c10d-483e-8506-d7bb23f9ad04",
   "metadata": {},
   "source": [
    "+ Example 2: star Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3589a22f-308e-43fd-aead-1eb954e3c0ba",
   "metadata": {},
   "source": [
    "More compleex example.\n",
    "\n",
    "Suppose we have star ratings in the range of 1-5\n",
    "\n",
    "> Actor A's movies rated 3 and 5\n",
    "\n",
    "> Actor B's movies rated 1,2 and 4\n",
    "\n",
    "The user likes one of the movies from Actor A and  one from Actor B\n",
    "1,and 2 are negtive ratings.\n",
    "\n",
    "The idea of normalizing ratings helps us capture the idea that some ratings are positive ratings while others are negative.\n",
    "\n",
    "The baseline is that users are different from each other. Some users are more generous , while others are not . To some users 4 might be an average rating.\n",
    "\n",
    "\n",
    "+ Useful step: Normlize ratings by subtracting users's mean rating (3)\n",
    "\n",
    "> Actor A's notmalized ratings = 0, +2\n",
    "\n",
    "> Profile weight = (0 + 2)/2 = 1\n",
    " You divide the total number of movies with the specific case\n",
    "\n",
    "> Actor B's normalized ratings = -2,-1,+1\n",
    "\n",
    "> Profile weight = -2/3\n",
    "It indicates a mild positive preference for actor A and  a mild negative oreeferennc for Actor B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b58d0-4b69-48de-bc0a-fac4abf0729c",
   "metadata": {},
   "source": [
    "+ Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead8f58-bd75-4bbb-8e24-92184200e045",
   "metadata": {},
   "source": [
    "> user profile x, item profile i\n",
    "\n",
    "Both the user and item profile are vectors\n",
    "\n",
    "> Estimate U(x,i)= cos(0) = (x.i)/(|x||i|)\n",
    "You can estimate the vector using the cosine\n",
    "\n",
    "Techincally, the cosine distance is actually the angle 0, And the cosine similarity is the angle 180-0\n",
    "The smaller the angle the more similar the user profile and the item profile i hence the similarity 180 -0 is going to be larger,\n",
    "\n",
    "For convenience, we will use the cosine of thether as our similarity measure.\n",
    "\n",
    "For convenience, we use cos(0)as our similarity measure and call it the \"cosine similarity\" in this context.\n",
    "\n",
    "Notice that as the angle data becomes smaller the cosine data becomes larger, as the angle data becomes larger , the cosine becomes smaller.\n",
    "\n",
    "If it gets to 90 , then the cosine data becomes negative.\n",
    "So as the angle data becomes smaller, x and i becomes similar to each other and more likely high raturn.\n",
    "\n",
    "Given the user x, we compute the cosine similarity of that  user and all the items in the catalogue, then you pick the iitem of the highest cosine similarity and recommend those to the user.\n",
    "\n",
    "This is the theory of content based recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0e5f2f-12bc-4e68-8b99-64d892c93dd5",
   "metadata": {},
   "source": [
    "+ Pros : content - based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d008becc-2e24-4629-8c13-a34cb154e00d",
   "metadata": {},
   "source": [
    "We will be looking at some of the pros and cons of the content based recommendation.\n",
    "\n",
    "\n",
    "Pros:  \n",
    "\n",
    "+ the biggest pro of the content based approach is that you dont need data on other users in other to make recommendations to a specific user.\n",
    "hence, you can start making content based recommendation from day 1 for the very first user.\n",
    "\n",
    "+ Able to recommend to users with unique tastes.\n",
    "\n",
    "In collabrative filtering you need to find data from similar users. The problem with that is that if there is any user with unique taste, there may not be any other similar users.\n",
    "\n",
    "Content baed approach is a bit natural because users can have very unique taste as long as you can build item profile for the item tha the user likes and a user profile based on that and make recommendations for that user.\n",
    "\n",
    "\n",
    "+ Able to recommend new and unpopular items\n",
    "> No first-rate problem\n",
    "\n",
    "When new items comes in, we dont need any rating from users to build the item profile. Item profile depends entirely on the features of the items and not on how other users rate the items.\n",
    "\n",
    "Hence, we dont have first rater problem , that we see in the collaborative filtering approach.\n",
    "\n",
    "you can make recommendation for an item as soon as it becomes available.\n",
    "\n",
    "+ Explanations for recommended items\n",
    "You can provide an explanation to the user whwn ever the content based approach makes recommendaton\n",
    ">You can just list the  content features that caused an item to be recommended.\n",
    "\n",
    "For example, if you recommend a new news article to the user. you might be able to say in the past you spent alot of time reading articles, serial and that is why you are recommending article on serial "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ae500-0742-4cba-af3d-7f324ae69bb0",
   "metadata": {},
   "source": [
    "+ Cons : Content - based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a108b67-4168-4be2-9edc-5871a2d17a31",
   "metadata": {},
   "source": [
    "> Finding the appropriate feature is hard\n",
    "\n",
    "> E.g, images , movies , music\n",
    "Although in the case of movie we sugested actors and directors but viewers are not often loyal to actors or directors.\n",
    "It is very hard to box music as well and images the features are very hard to find.\n",
    "\n",
    "In generally finding appropriate features to make content based recommendation is a very difficult one. This is the reason why the content based approach is not popular.\n",
    "\n",
    "+ Overspecilaization\n",
    "\n",
    "> Never recommend items outside user's content profile.\n",
    "\n",
    "It works with regards to user's previous preferences\n",
    "\n",
    "> People might have multiple interests and might express only some of them in the past. so it may be difficult to recommend to the users because you dont have enough data on the user.\n",
    "\n",
    "> Unable to exploit quality judgements of other users. For example there may be a video or music that is widely popular accross wide section of users current user has not expressed interest in that kind of movie, hence, the content based appraoch will not recommend that movie to that user.\n",
    "\n",
    "\n",
    "+ Cold-startproblem for new users\n",
    "\n",
    "> How to build a user profile?\n",
    "\n",
    "The user profile is built by aggregating profile of the items the user has rated.\n",
    "\n",
    "We have a new user and the new user have aggravated the item to the user profile. The challenging problem is how to build the user profile for a new user.\n",
    "\n",
    "In more practical situations, new users starts with or rather most recommender system starts with often user with average profile base on system wide average and over time use of more "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56aa88-ff31-416b-aa85-67b728671017",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c1893d-778b-4a00-9f8f-a8d7f3f01cae",
   "metadata": {},
   "source": [
    "To address some of the limitations of content-based filtering, collaborative filtering uses similarities between users and items simultaneously to provide recommendations. This allows for serendipitous recommendations; that is, collaborative filtering models can recommend an item to user A based on the interests of a similar user B. Furthermore, the embeddings can be learned automatically, without relying on hand-engineering of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415bd24-7da5-4362-9ca5-10ff37821129",
   "metadata": {},
   "source": [
    "+ A Movie Recommendation Example\n",
    "\n",
    "Consider a movie recommendation system in which the training data consists of a feedback matrix in which:\n",
    "\n",
    "Each row represents a user.\n",
    "\n",
    "Each column represents an item (a movie).\n",
    "\n",
    "The feedback about movies falls into one of two categories:\n",
    "\n",
    "Explicitâ€” users specify how much they liked a particular movie by providing a numerical rating.\n",
    "\n",
    "Implicitâ€” if a user watches a movie, the system infers that the user is interested.\n",
    "\n",
    "To simplify, we will assume that the feedback matrix is binary; that is, a value of 1 indicates interest in the movie.\n",
    "\n",
    "When a user visits the homepage, the system should recommend movies based on both:\n",
    "\n",
    "similarity to movies the user has liked in the past\n",
    "movies that similar users liked\n",
    "For the sake of illustration, let's hand-engineer some features for the movies described in the following table:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c9e60d-20e8-4d76-9156-29c69a526471",
   "metadata": {},
   "source": [
    "+ Movie\tRating\tDescription\n",
    "\n",
    "\n",
    "The Dark Knight Rises\tPG-13\tBatman endeavors to save Gotham City from nuclear annihilation in this sequel to The Dark Knight, set in the DC Comics universe.\n",
    "\n",
    "Harry Potter and the Sorcerer's Stone\tPG\tA orphaned boy discovers he is a wizard and enrolls in Hogwarts School of Witchcraft and Wizardry, where he wages his first battle against the evil Lord Voldemort.\n",
    "\n",
    "Shrek\tPG\tA lovable ogre and his donkey sidekick set off on a mission to rescue Princess Fiona, who is emprisoned in her castle by a dragon.\n",
    "\n",
    "The Triplets of Belleville\tPG-13\tWhen professional cycler Champion is kidnapped during the Tour de France, his grandmother and overweight dog journey overseas to rescue him, with the help of a trio of elderly jazz singers.\n",
    "\n",
    "Memento\tR\tAn amnesiac desperately seeks to solve his wife's murder by tattooing clues onto his body.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab89d8-781b-4033-93a1-84c964b60979",
   "metadata": {},
   "source": [
    "Suppose we assign to each movie a scalar in \n",
    " that describes whether the movie is for children (negative values) or adults (positive values). Suppose we also assign a scalar to each user in \n",
    " that describes the user's interest in children's movies (closer to -1) or adult movies (closer to +1). The product of the movie embedding and the user embedding should be higher (closer to 1) for movies that we expect the user to like.\n",
    "\n",
    "Image showing several movies and users arranged along a one-dimensional embedding space. The position of each movie along this axis describes whether this is a children's movie (left) or an adult movie (right). The position of a user describes interest in children or adult movies.\n",
    "\n",
    "In the diagram below, each checkmark identifies a movie that a particular user watched. The third and fourth users have preferences that are well explained by this featureâ€”the third user prefers movies for children and the fourth user prefers movies for adults. However, the first and second users' preferences are not well explained by this single feature.\n",
    "\n",
    "Image of a feedback matrix, where a row corresponds to a user, and a column corresponds to a movie. Each user and each movie is mapped to a one-dimensional embedding (as described in the previous figure), such that the product of the two embeddings approximates the ground truth value in the feedback matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbbbd28-f330-409d-93c4-82b5c04d17b8",
   "metadata": {},
   "source": [
    "+ 2D Embedding\n",
    "\n",
    "One feature was not enough to explain the preferences of all users. To overcome this problem, let's add a second feature: the degree to which each movie is a blockbuster or an arthouse movie. With a second feature, we can now represent each movie with the following two-dimensional embedding:\n",
    "\n",
    "Image showing several movies and users arranged on a two-dimensional embedding space. The position of each movie along the horizontal axis describes whether this is a children's movie (left) or an adult movie (right); its position along the vertical axis describes whether this is a blockbuster movie (top) or an arthouse movie (bottom). The position of the users reflect their interests in each category.\n",
    "\n",
    "We again place our users in the same embedding space to best explain the feedback matrix: for each (user, item) pair, we would like the dot product of the user embedding and the item embedding to be close to 1 when the user watched the movie, and to 0 otherwise.\n",
    "\n",
    "Image of the same feedback matrix. This time, each user and each movie is mapped to a two-dimensional embedding (as described in the previous figure), such that the dot product of the two embeddings approximates the ground truth value in the feedback matrix.\n",
    "\n",
    "Note: We represented both items and users in the same embedding space. This may seem surprising. After all, users and items are two different entities. However, you can think of the embedding space as an abstract representation common to both items and users, in which we can measure similarity or relevance using a similarity metric.\n",
    "In this example, we hand-engineered the embeddings. In practice, the embeddings can be learned automatically, which is the power of collaborative filtering models. In the next two sections, we will discuss different models to learn these embeddings, and how to train them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f5b98-f0fb-44ba-a94c-fe61d2b0e197",
   "metadata": {},
   "source": [
    "The collaborative nature of this approach is apparent when the model learns the embeddings. Suppose the embedding vectors for the movies are fixed. Then, the model can learn an embedding vector for the users to best explain their preferences. Consequently, embeddings of users with similar preferences will be close together. Similarly, if the embeddings for the users are fixed, then we can learn movie embeddings to best explain the feedback matrix. As a result, embeddings of movies liked by similar users will be close in the embedding space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0802388-f13b-4a7e-8766-5345e414278b",
   "metadata": {},
   "source": [
    "+ Check Your Understanding\n",
    "\n",
    "The model recommends a shopping app to a user because they recently installed a similar app. What kind of filtering is this an example of?\n",
    "Collaborative filtering\n",
    "Content-based filtering\n",
    "Previous\n",
    "arrow_backAdvantages & Disadvantages\n",
    "Next\n",
    "Matrix Factorizationarrow_forward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f647514-4423-43fd-ad72-9e0d99dc8720",
   "metadata": {},
   "source": [
    "#### Recommendation System: Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e694fe-02b6-4ca6-b84c-c05bf81a93f0",
   "metadata": {},
   "source": [
    "1. Which process/system is used behind the spotify new song suggestion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45135d75-60ba-474b-8d5b-ba4999f10653",
   "metadata": {},
   "source": [
    "Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc40dc-441e-408b-9b1a-0337e369484a",
   "metadata": {},
   "source": [
    "2. Select the examples, where Recommendation Algorithm works? ( Choose all that apply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80085787-a82f-4533-b035-0bfda8cbb72d",
   "metadata": {},
   "source": [
    "NetFlix\n",
    "YouTube\n",
    "Amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde7d554-8bdd-4a0b-a2a3-184a7fbde4ba",
   "metadata": {},
   "source": [
    "3. Which filtering uses knowledge of each product to recommend a similar product?\n",
    "\n",
    "Content Based Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe37dfc-48b0-4a11-8a46-1919f9eb5611",
   "metadata": {},
   "source": [
    "Content-based filtering uses item features to recommend other items similar to what the user likes, based on their previous actions or explicit feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba02d7e0-b4e3-4d84-a48f-95c28bc3d44f",
   "metadata": {},
   "source": [
    "4. Which filtering uses knowledge of userâ€™s past purchase/selection ?\n",
    "\n",
    "Collaborate Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b2753-85d0-4d9f-9c6a-ca4500725897",
   "metadata": {},
   "source": [
    "The Collaborative filtering method for recommender systems is a method that is solely based on the past interactions that have been recorded between users and items, in order to produce new recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2091f51d-8f66-48a9-a15f-d71cdcb567fc",
   "metadata": {},
   "source": [
    "5. The class of collaborative filtering algorithms is divided into two sub-categories that are generally called ________  and ______?\n",
    "\n",
    "Memory based and model based approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5232f7d-ac8f-439d-be6e-e783a051c29a",
   "metadata": {},
   "source": [
    "The movie was good and we really like it\n",
    "\n",
    "We want to pass it to 18 unit tokens\n",
    "\n",
    "first we initialize by setting al the 18 unique values as 0\n",
    "\n",
    "add 1 anywhere review 1 token appeared in the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b874b1-f966-44fa-b9e2-aaa98033b799",
   "metadata": {},
   "source": [
    "The movie was good but the end was boring\n",
    "\n",
    "Then pass it on the review\n",
    "review2_dict\n",
    "if the word is lower and upper case , the machine will treat it as different words,\n",
    "It is case sensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ec6d0-8ce6-40d2-a01b-e6c867aa7a19",
   "metadata": {},
   "source": [
    "We did not like the movie as it was too lengthy\n",
    "\n",
    "review3_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ee536-7e5d-488c-a464-1af77b81d403",
   "metadata": {},
   "source": [
    "we will convert it to a dataframe using pandas dataframe.\n",
    "\n",
    "All the unique values\n",
    "\n",
    "We can use bow to tokenize it\n",
    "\n",
    "we can also use count vectorization to achieve it\n",
    "import count vectorizer from sk-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb589df7-7831-4be6-a1d2-911ca248d12b",
   "metadata": {},
   "source": [
    "sparse matrix is number of elements are 0\n",
    "\n",
    "Dense matrix number of elements are non 0\n",
    "\n",
    "\n",
    "How do you give your data weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd91705-84b6-4a97-989f-f0be1458690b",
   "metadata": {},
   "source": [
    "#### Overview of Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7db4d1-4556-41dd-ada5-a430debfd4e1",
   "metadata": {},
   "source": [
    "It recommeds to the user based on what they know about the user\n",
    "\n",
    "gathering known ratings for matrix\n",
    "\n",
    "exterapolate unknown ratings from the known ones\n",
    "\n",
    " extrapolation utilities\n",
    " most people have not rated most items\n",
    "\n",
    "way out\n",
    "explicit method\n",
    "Explicit ask people to rate items\n",
    "prob; ratings from usersa re not enough\n",
    "\n",
    "\n",
    "implicit\n",
    "\n",
    "learn rating s from user actions\n",
    "they are More scalebale\n",
    "\n",
    "in practice  they use both\n",
    "\n",
    "\n",
    "\n",
    "content based approach\n",
    "probs\n",
    "images ,movies ,music features are hard to find\n",
    "people might have multiple interest\n",
    "unable to exploit quality judgement of oyther users\n",
    "\n",
    "overspecilization\n",
    "\n",
    "never recommend items outside users content profie\n",
    "\n",
    "cold start problem for new users\n",
    "how to build user profile for new user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36194bdb-2f21-4fd0-bef4-d5c10873158c",
   "metadata": {},
   "source": [
    "### Week 15 day 3 : Pyspark with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7141e9-84f2-41bc-940a-e8ea824359bc",
   "metadata": {},
   "source": [
    "#### PySpark With Python-PySpark Introduction and Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13932a6d-12a0-4e56-bf40-42ceb6d4be96",
   "metadata": {},
   "source": [
    "+ Apache Spark is written in Scala programming language. PySpark has been released in order to support the collaboration of Apache Spark and Python, it actually is a Python API for Spark. In addition, PySpark, helps you interface with Resilient Distributed Datasets (RDDs) in Apache Spark and Python programming language. This has been achieved by taking advantage of the Py4j library.\n",
    "\n",
    "+ Pyspark is used when you have large amount of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c6d8d3-09df-45e8-9909-007e8bca5efb",
   "metadata": {},
   "source": [
    "you work in cloud using pyspark mostly used for heavy data.\n",
    "\n",
    "+ PySpark is the Python API for Apache Spark, an open source, distributed computing framework and set of libraries for real-time, large-scale data processing. If you're already familiar with Python and libraries such as Pandas, then PySpark is a good language to learn to create more scalable analyses and pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2759f0-59b1-4689-9067-c116896d80a4",
   "metadata": {},
   "source": [
    "+ Installating and importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c5ecc1-10ff-46e8-bf0e-32522dff9796",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaf797f-2907-44b7-ada2-1eee6dddaf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd41d5-d82b-4043-a56c-6f7b68dcbade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53f601-d20e-4f31-80ca-0b3ed051322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(r\"C:\\Users\\user\\Desktop\\test1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626dc0e-fe88-45f3-a798-e70867e3fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to work with pyspark always remember that you need to start a spark session. In order to start a spark session you need to create a variable. lets say spark  = sparksession.builder\n",
    "# If you aer executing for the first time ,it will take some time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df896572-6478-4ecf-b7c9-b22147f9b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0dfddaf-3d8d-4de6-86d2-9e4ea1bbefb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['SPARK_HOME'] = 'C:\\spark\\spark-3.3.0-bin-hadoop3'\n",
    "os.environ[\"JAVA_HOME\"] = 'C:\\Java\\jdk1.8.0_321'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af958ae5-425a-45a2-b1f5-00269554621c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "389dd3cf-e041-4173-92d8-ca11309086d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1052/3712760749.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'practise'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName('practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d4f78-4b23-4fce-afdc-042099664c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f27a514f-cfc0-4442-bfe4-ed1be2e8ed6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1052/3923481620.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# to read a dataset using spark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_pyspark\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\user\\Desktop\\test1.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# to read a dataset using spark\n",
    "df_pyspark=spark.read.csv(r\"C:\\Users\\user\\Desktop\\test1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15414f5-eb4b-42cd-a254-2ebd2bb649b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to see whart you have in your dataset\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad9a9c4-e2be-426c-b782-98fb43f11da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output heading have _ so to solve the problem you need to do this,we use spark.read.option\n",
    "# add.show if you want to see the complete dataset\n",
    "spark.read.option('header','true').csv((r\"C:\\Users\\user\\Desktop\\test1.csv\")\n",
    "# spark.read.option('header','true').csv((r\"C:\\Users\\user\\Desktop\\test1.csv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bcdf40-a3a4-4429-bf08-4d16a613fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save it in df_pyspark\n",
    "df_pyspark=spark.read.option('header','true').csv((r\"C:\\Users\\user\\Desktop\\test1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd0e41-1c41-477d-9e7a-5dc2e82f935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the type of df_pyspark\n",
    "# Note when we read our csv file before introducing pyspark it was pandas dataframe (type(pd.read_csv('test1.csv)),\n",
    "# we saw the type to be pandas.core.frame.DataFrame but when you are reading df_pyspark.read.option('test1.csv') , the type is pyspark.sql.dataframe.DataFrame\n",
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda6be6-85d1-4edd-9a4c-24be87f65505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note ,most of the API are the same. the functionality are thesame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b07f5d2-dd1b-47b3-aca3-639185f99e18",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_pyspark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1052/1013178060.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# to see head\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_pyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_pyspark' is not defined"
     ]
    }
   ],
   "source": [
    "# with .head we can see the rows\n",
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420589f1-7aa4-4c8d-b3f1-e4a044932f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get more information regarding columns\n",
    "# df_..print schema is same like print info\n",
    "# It tells about your column like name string\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7abc8ad-5240-4ae1-adf4-d5c76515aa14",
   "metadata": {},
   "source": [
    "#### PySpark With Python-PySpark DataFrames Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aa3d94-0b11-43f2-8ffe-aaf230ec6ccf",
   "metadata": {},
   "source": [
    "+ Course Outline\n",
    "\n",
    "+ Pyspark Dataframe\n",
    "\n",
    "+ Reading The Dataset\n",
    "\n",
    "+ Checking the Datatype of the Column(Schema)\n",
    "\n",
    "+ Selecting Columns And indexing\n",
    "\n",
    "+ Check Describe option similar to Pandas\n",
    "\n",
    "+ Adding Columns\n",
    "\n",
    "+ Dropping columns\n",
    "\n",
    "+ Renaming Columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577901c1-c928-475c-8683-3c64324eeff1",
   "metadata": {},
   "source": [
    "+ Build the pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "291298be-a71a-4ade-974c-eb915d2bb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f75981c9-bb68-442d-a95c-2db55dda7289",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1052/1098397483.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataFrame'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    193\u001b[0m             )\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             self._do_init(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;31m# preexec_fn not supported on Windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                 \u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    949\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1418\u001b[0m             \u001b[1;31m# Start the process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1421\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m                                          \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName('DataFrame').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a1802-6522-4cff-8606-6948b451ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441c419-842c-4d6d-b85b-79467eed6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "# you can use  read option method\n",
    "# put .show(0 at the end if you want to see the entire dataset\n",
    "spark.read.option('header', 'true').csv(r\"C:\\Users\\user\\Desktop\\test1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620371a-2e0c-4391-b1bf-c794333ed7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in a variable df_pyspark=spark.read.option\n",
    "df_pyspark=spark.read.option('header', 'true').csv(r\"C:\\Users\\user\\Desktop\\test1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469bc2d-f8a6-411b-a2df-774a4b804cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema\n",
    "# schema means the data type , just like in pandas, df.info()\n",
    "# By default the output is in string, if you want to remove the string from the result use the code below\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bee95a-50bf-43fd-91da-9b16c3c6736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove string\n",
    "df_pyspark=spark.read.option('header', 'True').csv(r\"C:\\Users\\user\\Desktop\\test1.csv\",inferSchema=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50653ad-1e18-4815-8945-16cdf7d71723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to read it include header and inferschema \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4a466d-e8da-48dc-8f3b-25a2b6dfe6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv(df_pyspark=spark.read.csv(r\"C:\\Users\\user\\Desktop\\test1.csv\",header=True,inferSchema=True\")\n",
    "df_pyspark.show()                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880bea25-022c-4052-9f89-edb34c52567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the data type of the column\n",
    "### check the schema \n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd932cfa-a21e-46fa-b7d5-2f9b8df13314",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_pyspark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1052/3504840440.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Find the type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_pyspark\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_pyspark' is not defined"
     ]
    }
   ],
   "source": [
    "# Find the type\n",
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9828d6b-c454-4271-8150-210df01e8c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are asked in an interview the definition of dataframe: Dataframe are data structures becauses inside it you can perform different types of operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8089f8e-983e-45be-bb4f-2022379efab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the column \n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb07a43-b3f4-4c0c-b07a-619a34f82052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pick up the head element\n",
    "df_pyspark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c01db23-dd3b-46b6-8b01-d4e6df7dbd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to select a column\n",
    "# fisrt use .show to see all the element\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64351698-89c6-47e7-b270-2561f970642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pick name column\n",
    "df_pyspark.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e0c73a-ba9d-4c2e-9286-45c20031b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the entire column use .show\n",
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c019ac11-094c-4ba3-9b30-abfe80898d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the title of the code above\n",
    "type(df_pyspark.select('Name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea895877-33a9-46f8-a4c1-27b793e5fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pick up multiple columns e.g name and experience\n",
    "df_pyspark.select(['Name', 'Experience'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112f662-cdf2-4067-a409-5add9173c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .show to see all the element\n",
    "df_pyspark.select(['Name', 'Experience']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2759a04f-0206-4c5f-befa-eeb3397aca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When ever you have any issue look at the pyspark documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fca08f-b57b-4b81-987c-6fb7cec65c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see column name, in pandas we can directly pick this way\n",
    "df_pyspark['Name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c0a962-b86f-4644-bbfc-ce0b43b53a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dtypes to check the data type\n",
    "df_pyspark.dtypes  # we also use it in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3783f6-0cfe-45d5-80fb-13637d13c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the describe options similar to pandas\n",
    "df_pyspark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32134709-01bf-4117-bc78-6ba4b0687dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .show to to see all the dataset in a dataframe format\n",
    "df_pyspark.describe().show() # same like pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec520606-ffe3-4fc8-9fa4-8ddd7cc814c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding columns in pyspark dataframe using withcolumn\n",
    "# WithColumn function returns a new dataframe by adding a column or replacing the existing column that has same name\n",
    "df_pyspark.withColumn('Experience after 2 years',df-pyspark['Experince']+2)\n",
    "# use .show to see the complete dataset\n",
    "# You need to assign it to a variable for it to reflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d31169c-3220-4b41-93d6-471b18fac9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets save it \n",
    "df_pyspark=df_pyspark.withColumn('Experience after 2 years',df-pyspark['Experince']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5ceaa-aef7-4540-9e98-79bcff501b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518fd406-9a6e-468c-a50e-69c358d50599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .show to see the element of all the column added \n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc156cd0-0f5a-4745-a769-4fbf62a4c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns in pyspark\n",
    "# By default column name(s)\n",
    "df_pyspark.drop('Experience after 2 years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbfe2bc-093c-4c9d-8093-f38cb9812601",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.drop('Experience after 2 years').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d428f-cb5c-4fa0-a97c-bdb837a4932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Asssing it to a variable and save\n",
    "df-pyspark=df_pyspark.drop('Experience after 2 years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1afdf1-cd56-49b2-ba84-d238d98956f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see if it was removed\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107dfcff-0f18-4996-9bc1-409224febc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Rename the columns usng the function columnrename\n",
    "# You need to include existing and new name\n",
    "df_pyspark.withColumnRenamed('Name', 'New Name').show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f1313-0ad0-48cf-bd91-5e7b98482912",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### PySpark With Python-PySpark DataFrames-Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d62fe6-c0de-471e-b115-3b746f62f951",
   "metadata": {},
   "source": [
    "#####  Pyspark Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad2b1b-35b8-40a7-bf65-eb6bca8dc164",
   "metadata": {},
   "source": [
    "+ Dropping Columns\n",
    "\n",
    "+ Dropping Rows\n",
    "\n",
    "+ Various Parameter in Dropping Functionalities\n",
    "\n",
    "+ Handling missing values by Mean. Median and Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ed50dba-9abc-4c4f-bce8-4d24608618b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "918e851a-26be-4e49-bcfd-5d3295aabc37",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Builder' object has no attribute 'getOrcreate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1052/4024056441.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msparks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Practice'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Builder' object has no attribute 'getOrcreate'"
     ]
    }
   ],
   "source": [
    "sparks=SparkSession.builder.appName('Practice').getOrcreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3337e6f7-b676-441d-9285-8e282fd52457",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "492e09b6-16b4-47d6-b8d0-85611692ca6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1052/2972812918.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# To read our dataset ,to get the dataset properly, use schema=True,header-True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\user\\Desktop\\test2.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# To read our dataset ,to get the dataset properly, use schema=True,header-True\n",
    "spark.read.csv(r\"C:\\Users\\user\\Desktop\\test2.csv\",header=True,inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f421f104-2cd3-440d-a6df-d72d68ca31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv(r\"C:\\Users\\user\\Desktop\\test2.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052cfc9-b9d9-4e5a-aa19-ce93eab06a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns\n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0534490-058f-485b-a6e0-d831544d24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To view the dataset\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a899be-a2e0-46fb-9adb-4135ea1a88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to drop a specific row based on the NaN value\n",
    "# Using drop, fill and replace\n",
    "# If we dont put a specific row where ever there is \n",
    "# a nan value ,all rows will be get deleted.\n",
    "df_pyspark.na.drop().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d72bfc-a793-4668-9813-d98e2cd82d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we search in the drop, we have three main feature( how='any',thresh=None,subset=None)Docstring:\n",
    "Returns a new : class :'DataFrame' omitting rows with null values.\n",
    "# : func: DataFrameNameFunctions.drop are aliases of each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ad81f-eb0d-482c-99fb-5d16203f2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any=='how', the how value can have 2 values\n",
    "# the null by default will get dropped\n",
    "# any=all, drop a row if it contains any null, it can be 1,2 or entire null\n",
    "# null by default will be dropped\n",
    "# how =='any'when do we use all, suppose in your feature you have all the values in the\n",
    "# row as null execept one value,if you use how='any'. non of the null valuses will drop \n",
    "# cos you have a value in the row\n",
    "df_pyspark.na.drop(how='all')how() \n",
    "# this will give you error ,it will not work cos \n",
    "# we have 1 value in the row\n",
    "        # but if you use how=any ie whether is 1,2 or more null drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae5c1c1-2ee9-4959-a92f-e76fdabe279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how=any\n",
    "df_pyspark.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb67c5d-6bf1-4e15-854e-24e0a4c0c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Threshhold \n",
    "# If we say threshhold =2, the last column will be deleted\n",
    "# When you say threshold =2, it means atleast 2 null value\n",
    "# must be present in a row if it is less than 2 it will be deleted\n",
    "df_pyspark.na.drop(how='any',thresh=2).show()\n",
    "# you can also use thresh=1,3 etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580dd487-bd42-4d20-83f9-7321bd45fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset\n",
    "# use to drop null value from a specific column e.g 'Experience column\n",
    "# do nulll values will be deleted only on the experience column\n",
    "pf_pyspark.na.drop(how='any',subset=['Experience']).show()\n",
    "# you can also apply it in 'Age'\n",
    "# this is very handy when working with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b347c9-6e34-4bd1-bc56-2587c4291b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the missing value using .na.fill\n",
    "# We wil take two parameters, values and subset\n",
    "# if we we use.na.fill(['Missing']) where evre \n",
    "# there is missing value you will see the words missing value replacing it.\n",
    "f_pyspark.na.fill('Missing Values').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939a7a09-6da9-4e22-8aee-531fec251cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to perform missing value for  specific column you need to writet the column name\n",
    "f_pyspark.na.fill('Missing Values',;['Experience','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374be49-52dd-407b-aad2-a4e621648b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a specific column ,handle the missing values,with the help of mean or median of the specific column\n",
    "df_pyspark.show()\n",
    "# we will take the experience value and replace it with the mean of the expereince\n",
    "# we will do this using the inputer function with the help of a scalar\n",
    "# pyspark also have an imputer function so we will just import imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52430ab-1e64-4929-9789-7fc72b594d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=('age', 'Experience', 'Salary'],\n",
    "    outputCols=[\"{}_imputed\",format(c) for c in ['age', 'Experience', 'Salary'\n",
    "   ).setstrategy(\"mean\")    # you can also change to median or mode                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c869143-eeec-4f5d-974b-e3efd6f390c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transfor(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f480e07-4485-42c5-b90e-e6b8b30b193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the result, notice that where you have age,experiwence and salary is repaced with the mean and the null values are also replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01643433-7b59-4d49-a102-498d27f4fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparks=SparkSession.builder.appName('Dataframe').getOrcreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec322e-9415-411b-a3d6-4392a6cf7601",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### PySpark With Python-PySpark DataFrames- Filter Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5d1c15-40d7-44cb-a511-d236dcfdd83a",
   "metadata": {},
   "source": [
    "+ Filter operation is very important for datapreprocessing techniques. For example if you want to retreive some kind of conditions , or boolean conditions you can do that with the help of filter operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a78a508f-4d95-4e50-8e63-f8250a36ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8902d4ec-a8d5-424b-8110-f0d5f6542982",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv((r\"C:\\Users\\user\\Desktop\\test1.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2695f8-6e78-4db7-89bb-a27010daa81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the dataset\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c231862-0da5-4095-88b7-4002a3f85671",
   "metadata": {},
   "source": [
    "#### Filter Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7727d8f9-c148-48a1-945f-73b5f9774e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we want to find out salary of people lesst than 20000 or equal to 20000\n",
    "# There are two ways we can achiee this\n",
    "# first specify the conditon you want ,20000, remember the salary should be the \n",
    "# same name with the column\n",
    "by_pyspark.filter(\"Salary<=20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f3254-b069-4866-905a-c384367e9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will also give you same output as above\n",
    "df_pyspark.filter(df_pysaprk['Salary']<=20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad232e6-4892-4298-a0ab-1ec550737266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To specify two columns and get a specific information\n",
    "df.pyspark.filter(\"Salary<=20000\").select(['Name','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863cee2-2623-44fd-8246-9937210574f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to put 2 diff. conditions or write multiple conditions\n",
    "# you can also use  and ,or operations etc\n",
    "df_pyspark.filter(df_pyspark['Salary']<=20000) &\n",
    "                 (df_pyspark['Salary']>=15000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7345517-7747-4de6-9c23-16a4adef6537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be handy when you are retrieving information from your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f15043-93df-4d62-80b3-859fb00f315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A not operation ~\n",
    "# Anything greater than 20,000 will be given over here\n",
    "df_pyspark.filter(~(df_pyspark[\"Salary\"]<=20000\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b94130-fef9-43b0-b07f-760514675919",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### PySpark With Python-GroupBy And Aggregate Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15e67e-742d-4ccb-9ff8-9808e1214a02",
   "metadata": {},
   "source": [
    "+ We use groupby and aggregate functions for data preprocessing to retrieve result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47e0e2d-736b-4ae9-b5d2-f15274692168",
   "metadata": {},
   "outputs": [],
   "source": [
    "(r\"C:\\Users\\user\\Desktop\\test3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfae9788-54c8-4a88-bb3e-766f1ea03b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7132d1-42ae-40d3-9fe0-633989f324dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session\n",
    "sparks=SparkSession.builder.appName('Agg').getOrcreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b45579-1ae4-4a2d-b3bc-451e1d728c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fddaaed-75ea-43ed-b3d1-a9e67a681d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv(r\"C:\\Users\\user\\Desktop\\test3.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3714bfec-7d41-4179-b667-83964baa5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b896f11-3d1a-4639-a0a1-9ad615382d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to se where the schema belongs to\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e327d-c3e9-4b86-962c-1b05df4876ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby operations\n",
    "# Group by name to find the person with the max .salary\n",
    "# GroupBy and agg.func works together,you work on agg.after groupby\n",
    "# To see aggregate func. input . then tab\n",
    "df_pyspark.groupBy('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74dc48c-fc23-4cab-8b69-23f998cfa063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add the agg.func.\n",
    "# Note,we cant apply sum on the string\n",
    "df_pyspark.groupBy('Name').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde17c4e-f316-45e2-a6da-1d128ec082f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the result by group ,the max salary\n",
    "df_pyspark.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f66ed2-4ff3-45ff-8f6d-62a5327b7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby department\n",
    "# To find out the dept that gives max. salary\n",
    "df_pyspark.groupBy('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a6a143-d700-4fcf-a62c-189338a7060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using groupby to find the mean salary\n",
    "# mean is based on the number of people working in the dept.\n",
    "df_pyspark.groupBy('Departments').mean().show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c15788-919a-43d3-b9c7-ca41cc749f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many numbers of employees are working based on the dept using .count\n",
    "df_pyspark.groupBy('Departments').counts().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2cc740-27f2-4477-abb0-dbc830871b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to find the sum of the salaries using direct agg.func.\n",
    "df_pyspark.agg({'Salary': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ba5a6-f8a2-45d8-bac3-79cf08be252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find out the max.salary\n",
    "df_pyspark.groupBy('Name').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512c967-9491-440e-b4f9-04bbc49cca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find out the min.salary\n",
    "df_pyspark.groupBy('Name').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1a7ad-8ad6-458b-a556-9ef6608cb070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find out the mean/avg.salary\n",
    "df_pyspark.groupBy('Name').ave().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd5c817-efb3-44a4-b5c1-bcbdbc4d578b",
   "metadata": {},
   "source": [
    "+ you can do alot of functionaility. You need to do alot of data preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa646a19-4ded-4f89-89ea-887ef2dacae9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### PySpark With Python-Introduction To Pyspark Mlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eeb601-5aee-496a-b029-d8243171c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "You can check out alot of examples on spark Mlib. It has an amazing documentation with respect to various examples.\n",
    "Just click on examples to see the different types:\n",
    "We have two different techniques.\n",
    "    i. Rdd API \n",
    "    \n",
    "    ii.Apache spark "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682298a9-10de-4bcb-9ab1-242c73461bc5",
   "metadata": {},
   "source": [
    "Dataframe API is the most resent and mostly used.\n",
    "We willl be using it in this study. We will learn through ia and see how we can solve some use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d58ac72-0c48-413c-b6db-3141aa9534d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "+ Problem statement \n",
    "\n",
    "Based on age and experience we need to predict the salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ba877-04ee-4339-9802-ff363194bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparks=SparkSession.builder.appName('Missing').getOrcreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed9c4e-0af2-44da-a028-05a759d1b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "training = spark.read_csv(r\"C:\\Users\\user\\Desktop\\test1.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b82fd5-2f4c-45c0-8951-a27a4adb82a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da4e1aa-f57d-44f6-a3c0-b7cd39f803d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training.print(shemama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebc1c81-fcbe-4c09-9c7d-912f08314977",
   "metadata": {},
   "outputs": [],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbd877-69e7-439a-bec6-1e5dda289da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In pyspark we use a diffent kind of data preprocessing\n",
    "# We basically create a way where we can group all the independent features\n",
    "# We will create a vectorAssembler, this will make sure we have all the \n",
    "# feature together grouped in the form of age and experience\n",
    "# we will treat each as a different feature [Age.Experiene]....> new feature....>indepent feature\n",
    "# We will not take name cos is fixed ,it is a string\n",
    "# If categorical features are there ww will convert it into numerical representation\n",
    "# We will take [Age ,Experience] in the form of a list and try to group this and create\n",
    "# a new column called independent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb7fae-227a-4366-846f-d98af2ff4166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureamsembler=VectorAssembler(inputCols=[\"age\", \"Experience\"],outputCol=\"Independent Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f596c717-8dec-4a3c-842b-65996ec60e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=featureassembler.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa2c47-4001-4783-9240-5a49332b5e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23073197-426c-4e0e-be5c-a67a4e008e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have created an independent feature, indepent ariable will be our input feature and salary will be our output feature(y variable) and we will now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b38608-cfa2-4877-a815-0761b0f03c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will select independent features and salary\n",
    "finalized_data=output.select(\"Independent Feature\",\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f439da-a663-435a-9521-5fee41a51926",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf5f687-7eb0-4cad-88a6-26868aa73fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train out model we will use a function called randomsplit\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "#train test split\n",
    "train_data,test_data=finalized_data.randomSplit([0.75,0.25])\n",
    "regressor= LinearRegression(featuresCol='Independent Features' labelCol='Salary')\n",
    "regresor=regressor.fit(train_data)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af632bd-856c-4e11-9fc7-fb1ac606dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficients\n",
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6321fb-6c9b-4a8e-b635-2b6c18f177b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercepts\n",
    "regressos.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3142fed-7a38-4187-b220-0ff1e45f8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "# use the evaluate function to see the output\n",
    "pred_results=regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d2fb73-fc93-4e42-921c-23d095480578",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ea08e-c679-4ed9-9ee9-52acf36da074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to chech how the model have performed. \n",
    "pred_results.meanAbsoluteError,pred_results.meabSquareError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5304efd2-7254-41aa-ad9e-2a5725a98f9a",
   "metadata": {},
   "source": [
    "### pyspark (PySpark): Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a79df59-7f21-41f3-8922-38163b4d5692",
   "metadata": {},
   "source": [
    "1. The primary Machine Learning API for Spark is now the DataFrame_____ based API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eed53a-d582-422c-99b9-66c370ffaec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Which of the following is a module for Structured data processing?\n",
    "\n",
    "Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab0050-52ea-4947-a0d3-df8b86e05b96",
   "metadata": {},
   "source": [
    "Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine. It enables unmodified Hadoop Hive queries to run up to 100x faster on existing deployments and data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f33f589-9879-4cae-85c0-c22aeede4781",
   "metadata": {},
   "source": [
    "3. SparkSQL translates commands into codes. These codes are processed by executors codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd8e761-e218-4dd3-97ba-e5ea57c1f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Spark SQL translates commands into codes that are processed by executors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b9efa-ebc0-4740-a205-195143991c66",
   "metadata": {},
   "source": [
    "4. Spark SQL plays the main role in the optimization of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c26150-1dd9-4d0e-a2f4-d83faad85c8c",
   "metadata": {},
   "source": [
    "Spark SQL plays a great role in the optimization of queries. The Spark SQL makes use of in-memory columnar storage while caching data. The in-memory columnar is a feature that allows storing the data in a columnar format, rather than row format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46f6e2-d044-495f-aae7-0258482c0016",
   "metadata": {},
   "source": [
    "5. Which of the following is not a Spark SQL query execution phases?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce3f25c-0b91-4259-865c-9a970a91da5d",
   "metadata": {},
   "source": [
    "Analysis\n",
    "Logical Optimization\n",
    "Execution\n",
    "Physical planning\n",
    "\n",
    "\n",
    "Execution  is not part of it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c2618-262a-48c7-89ea-9d1436913ccc",
   "metadata": {},
   "source": [
    "6. DataFrame in Apache Spark prevails over RDD and does not contain any feature of RDD.\n",
    "\n",
    "False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2adf57-7e40-481f-8057-3b6173d56983",
   "metadata": {},
   "source": [
    "7. Which of the following is not true for DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a9f73-5bc1-44f3-a170-b490cbe958b9",
   "metadata": {},
   "source": [
    "7. Which of the following is not true for DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14770c1-788b-441f-a3c2-9eda5eda486f",
   "metadata": {},
   "source": [
    "We can build DataFrame from different data sources. structured data file, tables in Hive\n",
    "\n",
    "The Application Programming Interface (APIs) of DataFrame is available in various languages\n",
    "\n",
    "Both in Scala and Java, we represent DataFrame as Dataset of rows.\n",
    "\n",
    "DataFrame in Apache Spark is behind RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ec60e9-2e97-439b-ae27-ef73d6c5ffc2",
   "metadata": {},
   "source": [
    "DataFrame in Apache Spark is behind RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53423e-66ff-40c3-8a0f-73286775b33f",
   "metadata": {},
   "source": [
    "8. We can create DataFrame using _________________.\n",
    "\n",
    "External databases\n",
    "Tables in Hive\n",
    "Structured data files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4916e5-afff-420b-b470-e6f056d3e71d",
   "metadata": {},
   "source": [
    "9. Which of the following is the fundamental data structure of Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be0253c-3ac5-40b6-8eb9-cd89343d6e1d",
   "metadata": {},
   "source": [
    "Resilient Distributed Dataset (RDD) is the fundamental data structure of Spark. They are immutable Distributed collections of objects of any type. As the name suggests is a Resilient (Fault-tolerant) records of data that resides on multiple nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac1098-2c6f-4a82-9e89-37fd1669a8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
